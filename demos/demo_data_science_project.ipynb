{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e772a91",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will complete a small end-to-end data science tutorial that employs lakeFS-spec for data versioning. We will use weather data to train a random forest classifier to predict whether it is raining a day from now given the current weather.\n",
    "\n",
    "We will do the following:\n",
    "* Environment Setup\n",
    "* lakeFS Setup\n",
    "* Data Ingestion\n",
    "    * Event Hooks\n",
    "    * PUT a File\n",
    "* Model Training\n",
    "* Updating Data and Retraining Model\n",
    "* Accessing Data Version and Reproducing Experiment\n",
    "* Using a Tag instead of a commit SHA for semantic versioning\n",
    "\n",
    "To execute the code in this tutorial as a jupyter notebook, download this `.ipynb` file to a convenient location on your machine. You can also clone the whole `lakefs-spec` repository. During the execution of this tutorial, in the same directory, a folder, 'data', will be created. We will also download a file `.lakectl.yaml`.\n",
    "\n",
    "Prerequisites before we start:\n",
    "* Python 3.9 or higher\n",
    "* Docker desktop installed - [see guidance](https://www.docker.com/get-started/)\n",
    "* git installed\n",
    "\n",
    "# Environment Setup\n",
    "To set up the environment, run the following commands in your console:\n",
    "\n",
    "Create a virtual environment:\n",
    "\n",
    "    `python -m venv .venv`\n",
    "\n",
    "Activate environment \n",
    "- macOS and Linux:\n",
    "\n",
    "        `source .venv/bin/activate`\n",
    "\n",
    "- activate environment - Windows:\n",
    "\n",
    "        `.venv\\Scripts\\activate`\n",
    "\n",
    "Install relevant libriaries on the environment you have just created:\n",
    "\n",
    "    `pip install -r https://raw.githubusercontent.com/appliedAI-Initiative/lakefs-spec/main/demos/requirements.txt`\n",
    "\n",
    "From a terminal activate this jupyter notebook (if not running already).\n",
    "\n",
    "In the notebook \"Kernel\" menu select the environment you just created as a Jupyter kernel.\n",
    "\n",
    "# lakeFS Setup\n",
    "\n",
    "Ensure you have docker desktop is running.\n",
    "\n",
    "Set up LakeFS. You can do this by executing the docker run command given here lakeFS quickstart in your console:\n",
    "\n",
    "`docker run --name lakefs --pull always --rm --publish 8000:8000 treeverse/lakefs:latest run --quickstart`\n",
    "\n",
    "Open a browser and navigate to the lakeFS instance - by default: http://localhost:8000/. \n",
    "\n",
    "Authenticate with the credentials provided https://docs.lakefs.io/quickstart/launch.html :\n",
    "\n",
    "    Access Key ID    : AKIAIOSFOLQUICKSTART\n",
    "    Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n",
    "\n",
    "As an email, you can enter anything, we won't need it in this example.\n",
    "\n",
    "We begin by instantiating the filesystem connector `LakeFSFilesystem` and fixing the name of the repository we are about to create. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f94db5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:06.447321Z",
     "iopub.status.busy": "2023-11-07T15:54:06.447044Z",
     "iopub.status.idle": "2023-11-07T15:54:06.701732Z",
     "shell.execute_reply": "2023-11-07T15:54:06.701356Z"
    }
   },
   "outputs": [],
   "source": [
    "from lakefs_spec import LakeFSFileSystem\n",
    "\n",
    "fs = LakeFSFileSystem()\n",
    "\n",
    "REPO_NAME = 'weather'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af0ffb4",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We will create a repository using a helper function provided by `lakefs-spec`. If you created one in the UI, make sure to set the `REPO_NAME` variable in the cell above accordingly. You can re-execute if necessary.\n",
    "Otherwise, execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55829859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:06.703371Z",
     "iopub.status.busy": "2023-11-07T15:54:06.703259Z",
     "iopub.status.idle": "2023-11-07T15:54:06.727739Z",
     "shell.execute_reply": "2023-11-07T15:54:06.727469Z"
    }
   },
   "outputs": [],
   "source": [
    "from lakefs_spec.client_helpers import create_repository\n",
    "\n",
    "create_repository(client=fs.client,repository_name=REPO_NAME, storage_namespace='local://weather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1822950",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "There many ways to authenticate to lakeFS while executing Python code - in this tutorial we choose convinient yaml file configuration. Execute the code below to dowload yaml file including lakeFS quickstart credentials and server url. This file will be downloaded to the same location as your notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9e7ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:06.729009Z",
     "iopub.status.busy": "2023-11-07T15:54:06.728942Z",
     "iopub.status.idle": "2023-11-07T15:54:07.053773Z",
     "shell.execute_reply": "2023-11-07T15:54:07.052984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100   265  100   265    0     0   1436      0 --:--:-- --:--:-- --:--:--  1480\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Data Ingestion\\n\\nNow it's time to get some data. We will use the [Open Meteo api](https://open-meteo.com/), where we can pull weather data from an API for free (as long as we are non-commercial) and without an API-token.\\n\\nFirst create folder 'data' inside a directory when your notebook is located:\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "!curl -o .lakectl.yaml \"https://raw.githubusercontent.com/appliedAI-Initiative/lakefs-spec/main/demos/.lakectl.yaml\"\n",
    "\n",
    "## %% [markdown]\n",
    "\"\"\"\n",
    "# Data Ingestion\n",
    "\n",
    "Now it's time to get some data. We will use the [Open Meteo api](https://open-meteo.com/), where we can pull weather data from an API for free (as long as we are non-commercial) and without an API-token.\n",
    "\n",
    "First create folder 'data' inside a directory when your notebook is located:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2531f03e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:07.056843Z",
     "iopub.status.busy": "2023-11-07T15:54:07.056625Z",
     "iopub.status.idle": "2023-11-07T15:54:07.187056Z",
     "shell.execute_reply": "2023-11-07T15:54:07.185185Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94bb6a1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Then, for the purpose of training, get the full data of the 2010s from Munich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef21d372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:07.192037Z",
     "iopub.status.busy": "2023-11-07T15:54:07.191684Z",
     "iopub.status.idle": "2023-11-07T15:54:54.568982Z",
     "shell.execute_reply": "2023-11-07T15:54:54.568275Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 32538    0 32538    0     0  37923      0 --:--:-- --:--:-- --:--:-- 38100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100  303k    0  303k    0    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0   159k      0 --:--:--  0:00:01 --:--:--  159k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100  655k    0  655k    0     0   224k      0 --:--:--  0:00:02 --:--:--  225k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100  831k    0  831k    0     0   214k      0 --:--:--  0:00:03 --:--:--  214k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 1007k    0 1007k    0     0   207k      0 --:--:--  0:00:04 --:--:--  207k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 1167k    0 1167k    0     0   198k      0 --:--:--  0:00:05 --:--:--  226k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 1359k    0 1359k    0     0   195k      0 --:--:--  0:00:06 --:--:--  209k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 1487k    0 1487k    0     0   189k      0 --:--:--  0:00:07 --:--:--  168k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 1599k    0 1599k    0     0   180k      0 --:--:--  0:00:08 --:--:--  153k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 1695k    0 1695k    0     0   172k      0 --:--:--  0:00:09 --:--:--  138k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 1823k    0 1823k    0     0   166k      0 --:--:--  0:00:10 --:--:--  128k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 1919k    0 1919k    0     0   161k      0 --:--:--  0:00:11 --:--:--  114k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2047k    0 2047k    0     0   158k      0 --:--:--  0:00:12 --:--:--  110k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2159k    0 2159k    0     0   155k      0 --:--:--  0:00:13 --:--:--  111k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2287k    0 2287k    0     0   153k      0 --:--:--  0:00:14 --:--:--  117k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2399k    0 2399k    0     0   151k      0 --:--:--  0:00:15 --:--:--  116k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2511k    0 2511k    0     0   149k      0 --:--:--  0:00:16 --:--:--  119k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2607k    0 2607k    0     0   146k      0 --:--:--  0:00:17 --:--:--  113k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2735k    0 2735k    0     0   145k      0 --:--:--  0:00:18 --:--:--  116k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2879k    0 2879k    0     0   143k      0 --:--:--  0:00:20 --:--:--  112k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2959k    0 2959k    0     0   142k      0 --:--:--  0:00:20 --:--:--  113k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 3119k    0 3119k    0     0   142k      0 --:--:--  0:00:21 --:--:--  122k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 3247k    0 3247k    0     0   141k      0 --:--:--  0:00:22 --:--:--  126k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 3391k    0 3391k    0     0   142k      0 --:--:--  0:00:23 --:--:--  130k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 3471k    0 3471k    0     0   139k      0 --:--:--  0:00:24 --:--:--  125k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 3599k    0 3599k    0     0   138k      0 --:--:--  0:00:25 --:--:--  125k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 3726k    0 3726k    0     0   138k      0 --:--:--  0:00:26 --:--:--  120k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 3790k    0 3790k    0     0   136k      0 --:--:--  0:00:27 --:--:--  109k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 3886k    0 3886k    0     0   134k      0 --:--:--  0:00:28 --:--:--  100k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 4014k    0 4014k    0     0   134k      0 --:--:--  0:00:29 --:--:--  108k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 4126k    0 4126k    0     0   133k      0 --:--:--  0:00:30 --:--:--  106k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 4270k    0 4270k    0     0   134k      0 --:--:--  0:00:31 --:--:--  109k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 4430k    0 4430k    0     0   134k      0 --:--:--  0:00:32 --:--:--  126k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 4606k    0 4606k    0     0   135k      0 --:--:--  0:00:33 --:--:--  140k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 4750k    0 4750k    0     0   136k      0 --:--:--  0:00:34 --:--:--  145k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 4846k    0 4846k    0     0   135k      0 --:--:--  0:00:35 --:--:--  144k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 5006k    0 5006k    0     0   135k      0 --:--:--  0:00:36 --:--:--  144k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 5182k    0 5182k    0     0   136k      0 --:--:--  0:00:37 --:--:--  150k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 5294k    0 5294k    0     0   136k      0 --:--:--  0:00:38 --:--:--  139k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 5438k    0 5438k    0     0   136k      0 --:--:--  0:00:39 --:--:--  139k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 5566k    0 5566k    0     0   136k      0 --:--:--  0:00:40 --:--:--  143k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 5678k    0 5678k    0     0   135k      0 --:--:--  0:00:41 --:--:--  135k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 5822k    0 5822k    0     0   135k      0 --:--:--  0:00:42 --:--:--  129k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 5934k    0 5934k    0     0   135k      0 --:--:--  0:00:43 --:--:--  128k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 6062k    0 6062k  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0     0   135k      0 --:--:--  0:00:44 --:--:--  124k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 6174k    0 6174k    0     0   134k      0 --:--:--  0:00:45 --:--:--  120k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 6254k    0 6254k    0     0   133k      0 --:--:--  0:00:46 --:--:--  116k"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 6298k    0 6298k    0     0   133k      0 --:--:--  0:00:47 --:--:--  108k\r\n"
     ]
    }
   ],
   "source": [
    "!curl -o data/weather-2010s.json \"https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2010-01-01&end_date=2019-12-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534806e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# PUT a file\n",
    "\n",
    "The data is in JSON format. Therefore, we need to wrangle the data a bit to make it usable. But first we will save it into our lakeFS instance.\n",
    "\n",
    "lakeFS works similar to `git` as a versioning system. You can create `commits` that encapsulate specific changes to the data. You can also work with `branches` to fork of your own copy of the data such that you don't interfere with your colleagues. Every commit (on any branch) is identified by a `commit-SHA`. This can be used to programmatically interact with specific states of your data and enables logging of the specific versions used to create a certain model. We will cover all this later in this demo.\n",
    "\n",
    "For now, we will `put` the file we have. Therefore, we will create a new branch, `transform-raw-data` for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ac23e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:54.572276Z",
     "iopub.status.busy": "2023-11-07T15:54:54.572030Z",
     "iopub.status.idle": "2023-11-07T15:54:54.755420Z",
     "shell.execute_reply": "2023-11-07T15:54:54.755017Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "NEW_BRANCH_NAME = 'transform-raw-data'\n",
    "fs.put('./data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094d531",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Now, on LakeFS in your browser, can change the branch to transform-raw-data and see the saved file. However, the change is not yet committed. While you can do that manually via the uncommitted changes tab in the lakeFS UI, we will commit the file in a different way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3ddf8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Event Hooks\n",
    "\n",
    "To commit changes programmatically, we can register a hook. This hook needs to have the signature `(client, context) -> None`, where the `client` is the file system's LakeFS client. The context object contains information about the requested resource. Within this hook, we can automatically create a commit. We will register the hook for the `PUT_FILE` and `FILEUPLOAD` events. Pandas uses the latter in 'DataFrame.to_csv()' and hence we commit when using 'DataFrame.to_csv()' as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc40642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:54.757765Z",
     "iopub.status.busy": "2023-11-07T15:54:54.757598Z",
     "iopub.status.idle": "2023-11-07T15:54:54.761007Z",
     "shell.execute_reply": "2023-11-07T15:54:54.760663Z"
    }
   },
   "outputs": [],
   "source": [
    "from lakefs_sdk.client import LakeFSClient\n",
    "\n",
    "from lakefs_spec.client_helpers import commit\n",
    "from lakefs_spec.hooks import FSEvent, HookContext\n",
    "\n",
    "\n",
    "# Define the commit hook\n",
    "def commit_on_put(client: LakeFSClient, ctx:HookContext) -> None:\n",
    "    commit_message = f\"Add file {ctx.resource}\"\n",
    "    print(f\"Attempting Commit: {commit_message}\")\n",
    "    commit(client, repository=ctx.repository, branch=ctx.ref, message=commit_message)\n",
    "    \n",
    "\n",
    "# Register the commit hook to be executed after the PUT_FILE and FILEUPLOAD events\n",
    "fs.register_hook(FSEvent.PUT_FILE, commit_on_put)\n",
    "fs.register_hook(FSEvent.FILEUPLOAD, commit_on_put)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774686a1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now uploading the file will create a commit. Since we already uploaded the file, lakeFS will skip the upload as the checksums of the local and remote file match. The hook will be executed regardless.\n",
    "\n",
    "If we want to execute the upload even for an unchanged file, we can do so by passing `precheck=False` to the `fs.put()` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6544e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:54.762692Z",
     "iopub.status.busy": "2023-11-07T15:54:54.762582Z",
     "iopub.status.idle": "2023-11-07T15:54:54.844443Z",
     "shell.execute_reply": "2023-11-07T15:54:54.844085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting Commit: Add file weather-2010.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fs.put('data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json', precheck=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c68c61",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Data Transformation\n",
    "Now let's transform the data for our use case. We put the transformation into a function such that we can reuse it later\n",
    "\n",
    "In this notebook, we follow a simple toy example to predict whether it is raining at the same time tomorrow given weather data from right now.\n",
    "\n",
    "We will skip a lot of possible feature engineering etc. in order to focus on the application of lakeFS and the `LakeFSFileSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1b17d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:54.846253Z",
     "iopub.status.busy": "2023-11-07T15:54:54.846126Z",
     "iopub.status.idle": "2023-11-07T15:54:55.453487Z",
     "shell.execute_reply": "2023-11-07T15:54:55.453216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relativehumidity_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>pressure_msl</th>\n",
       "      <th>surface_pressure</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>cloudcover_low</th>\n",
       "      <th>cloudcover_mid</th>\n",
       "      <th>cloudcover_high</th>\n",
       "      <th>windspeed_10m</th>\n",
       "      <th>windspeed_100m</th>\n",
       "      <th>winddirection_10m</th>\n",
       "      <th>winddirection_100m</th>\n",
       "      <th>is_raining</th>\n",
       "      <th>is_raining_in_1_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010-01-02 00:00:00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>999.2</td>\n",
       "      <td>100</td>\n",
       "      <td>54</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>8.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2010-01-02 01:00:00</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.5</td>\n",
       "      <td>999.7</td>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>9.1</td>\n",
       "      <td>18.6</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2010-01-02 02:00:00</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.2</td>\n",
       "      <td>1000.4</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>98</td>\n",
       "      <td>77</td>\n",
       "      <td>10.1</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2010-01-02 03:00:00</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>1000.8</td>\n",
       "      <td>93</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>89</td>\n",
       "      <td>11.2</td>\n",
       "      <td>21.7</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-01-02 04:00:00</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.2</td>\n",
       "      <td>1001.4</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>95</td>\n",
       "      <td>11.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>358</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  temperature_2m  relativehumidity_2m  rain  \\\n",
       "24 2010-01-02 00:00:00            -3.0                   88   0.0   \n",
       "25 2010-01-02 01:00:00            -3.2                   89   0.0   \n",
       "26 2010-01-02 02:00:00            -3.4                   89   0.0   \n",
       "27 2010-01-02 03:00:00            -3.5                   89   0.0   \n",
       "28 2010-01-02 04:00:00            -3.7                   90   0.0   \n",
       "\n",
       "    pressure_msl  surface_pressure  cloudcover  cloudcover_low  \\\n",
       "24        1004.0             999.2         100              54   \n",
       "25        1004.5             999.7         100              37   \n",
       "26        1005.2            1000.4         100              24   \n",
       "27        1005.6            1000.8          93               8   \n",
       "28        1006.2            1001.4          94               6   \n",
       "\n",
       "    cloudcover_mid  cloudcover_high  windspeed_10m  windspeed_100m  \\\n",
       "24             100               70            8.3            17.3   \n",
       "25              98               92            9.1            18.6   \n",
       "26              98               77           10.1            20.3   \n",
       "27              98               89           11.2            21.7   \n",
       "28             100               95           11.2            21.8   \n",
       "\n",
       "    winddirection_10m  winddirection_100m  is_raining is_raining_in_1_day  \n",
       "24                 18                  31       False               False  \n",
       "25                  9                  22       False               False  \n",
       "26                  2                  13       False               False  \n",
       "27                  4                  12       False               False  \n",
       "28                358                   9       False               False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def transform_json_weather_data(filepath):\n",
    "    with open(filepath,\"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data[\"hourly\"])\n",
    "    df.time = pd.to_datetime(df.time)\n",
    "    df['is_raining'] = df.rain > 0\n",
    "    df['is_raining_in_1_day'] = df.is_raining.shift(24)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "    \n",
    "df = transform_json_weather_data('data/weather-2010s.json')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3aedd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we save this data as a CSV file into the main branch. The `DataFrame.to_csv` method calls an `open` operation behind the scenes, our commit hook is called and the file is committed. You can verify the saving worked in the LakeFS UI in your browser by switching to the commits tab of the main branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985aecae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:55.455039Z",
     "iopub.status.busy": "2023-11-07T15:54:55.454910Z",
     "iopub.status.idle": "2023-11-07T15:54:55.926160Z",
     "shell.execute_reply": "2023-11-07T15:54:55.925867Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxmynter/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/lakefs_spec/spec.py:634: UserWarning: Calling `LakeFSFile.open()` in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Uploading large files unbuffered can have performance implications.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to commit on branch 'main', aborting commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting Commit: Add file weather_2010s.csv\n",
      "Attempting Commit: Add file weather_2010s.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(f'lakefs://{REPO_NAME}/main/weather_2010s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f73ab9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Model Training\n",
    "First we will do a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d2af7be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:55.927895Z",
     "iopub.status.busy": "2023-11-07T15:54:55.927770Z",
     "iopub.status.idle": "2023-11-07T15:54:56.496173Z",
     "shell.execute_reply": "2023-11-07T15:54:56.495863Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "model_data=df.drop('time', axis=1)\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(model_data, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23e84c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We save these train and test datasets into a new `training` branch. If the branch does not yet exist, as in this case, it is implicitly created by default. You can control this behaviour with the `create_branch_ok` flag when initializing the 'LakeFSFileSystem'. By default `create_branch_ok=True`, so that we needed only `fs = LakeFSFileSystem()` to enable implicit branch creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6243daf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:56.497975Z",
     "iopub.status.busy": "2023-11-07T15:54:56.497797Z",
     "iopub.status.idle": "2023-11-07T15:54:57.136271Z",
     "shell.execute_reply": "2023-11-07T15:54:57.135969Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to commit on branch 'training', aborting commit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to commit on branch 'training', aborting commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting Commit: Add file train_weather.csv\n",
      "Attempting Commit: Add file train_weather.csv\n",
      "Attempting Commit: Add file test_weather.csv\n",
      "Attempting Commit: Add file test_weather.csv\n"
     ]
    }
   ],
   "source": [
    "TRAINING_BRANCH = 'training'\n",
    "train.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "test.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af2951",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Implicit branch creation is a convenient way to create new branches programmatically. However, one drawback is that typos in your code might result in new accidental branch creations. If you want to avoid this implicit behavior and raise errors instead, you can disable implicit branch creation by setting `fs.create_branch_ok=False`.\n",
    "\n",
    "We can now read train and test files directly from the remote LakeFS instance. (You can verify that neither the train nor the test file are saved in the `/data` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bba85bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:57.137875Z",
     "iopub.status.busy": "2023-11-07T15:54:57.137747Z",
     "iopub.status.idle": "2023-11-07T15:54:57.248060Z",
     "shell.execute_reply": "2023-11-07T15:54:57.247822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relativehumidity_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>pressure_msl</th>\n",
       "      <th>surface_pressure</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>cloudcover_low</th>\n",
       "      <th>cloudcover_mid</th>\n",
       "      <th>cloudcover_high</th>\n",
       "      <th>windspeed_10m</th>\n",
       "      <th>windspeed_100m</th>\n",
       "      <th>winddirection_10m</th>\n",
       "      <th>winddirection_100m</th>\n",
       "      <th>is_raining</th>\n",
       "      <th>is_raining_in_1_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59272</th>\n",
       "      <td>10.4</td>\n",
       "      <td>79</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1026.8</td>\n",
       "      <td>1022.1</td>\n",
       "      <td>100</td>\n",
       "      <td>58</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>21.7</td>\n",
       "      <td>35.9</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73375</th>\n",
       "      <td>16.5</td>\n",
       "      <td>79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>1007.4</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>12.4</td>\n",
       "      <td>16.7</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26941</th>\n",
       "      <td>0.2</td>\n",
       "      <td>82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>1001.1</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>14.4</td>\n",
       "      <td>24.9</td>\n",
       "      <td>176</td>\n",
       "      <td>183</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65930</th>\n",
       "      <td>16.2</td>\n",
       "      <td>78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1010.9</td>\n",
       "      <td>1006.4</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>13.4</td>\n",
       "      <td>25.8</td>\n",
       "      <td>54</td>\n",
       "      <td>63</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55318</th>\n",
       "      <td>4.3</td>\n",
       "      <td>68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1003.3</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21.8</td>\n",
       "      <td>302</td>\n",
       "      <td>308</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature_2m  relativehumidity_2m  rain  pressure_msl  \\\n",
       "59272            10.4                   79   0.3        1026.8   \n",
       "73375            16.5                   79   0.0        1011.9   \n",
       "26941             0.2                   82   0.0        1005.9   \n",
       "65930            16.2                   78   0.0        1010.9   \n",
       "55318             4.3                   68   0.0        1008.0   \n",
       "\n",
       "       surface_pressure  cloudcover  cloudcover_low  cloudcover_mid  \\\n",
       "59272            1022.1         100              58              93   \n",
       "73375            1007.4          28               0               3   \n",
       "26941            1001.1          87               2              93   \n",
       "65930            1006.4          42              10              37   \n",
       "55318            1003.3          46              12              58   \n",
       "\n",
       "       cloudcover_high  windspeed_10m  windspeed_100m  winddirection_10m  \\\n",
       "59272               98           21.7            35.9                 11   \n",
       "73375               88           12.4            16.7                 17   \n",
       "26941               98           14.4            24.9                176   \n",
       "65930               36           13.4            25.8                 54   \n",
       "55318                0           11.0            21.8                302   \n",
       "\n",
       "       winddirection_100m  is_raining  is_raining_in_1_day  \n",
       "59272                  13        True                False  \n",
       "73375                  19       False                False  \n",
       "26941                 183       False                False  \n",
       "65930                  63       False                False  \n",
       "55318                 308       False                False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd1fd3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's check the shape of train and test data. Later on we will train to get back to this data version and reproduce the results of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8261b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:57.249433Z",
     "iopub.status.busy": "2023-11-07T15:54:57.249352Z",
     "iopub.status.idle": "2023-11-07T15:54:57.251324Z",
     "shell.execute_reply": "2023-11-07T15:54:57.251078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train initial data shape: (65718, 15)\n",
      "Test initial data shape: (21906, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Train initial data shape: {train.shape}')\n",
    "print(f'Test initial data shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cabc199",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We now proceed to train a random forest classifier and evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1e17686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:54:57.252684Z",
     "iopub.status.busy": "2023-11-07T15:54:57.252614Z",
     "iopub.status.idle": "2023-11-07T15:55:05.416421Z",
     "shell.execute_reply": "2023-11-07T15:55:05.415886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 87.99 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dependent_variable = 'is_raining_in_1_day'\n",
    "\n",
    "model = RandomForestClassifier(random_state=7)\n",
    "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
    "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04787e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Updating Data and Model\n",
    "Until now, we only have used data from the 2010s. Let's download additional 2020s data, transform it, and save it to LakeFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9016702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:05.419446Z",
     "iopub.status.busy": "2023-11-07T15:55:05.419309Z",
     "iopub.status.idle": "2023-11-07T15:55:08.449719Z",
     "shell.execute_reply": "2023-11-07T15:55:08.449402Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0     0    0     0    0 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0      0      0 --:--:--  0:00:01 --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100 2312k    0 2312k    0     0   899k      0 --:--:--  0:00:02 --:--:--  904k\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to commit on branch 'main', aborting commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting Commit: Add file weather_2020s.csv\n",
      "Attempting Commit: Add file weather_2020s.csv\n"
     ]
    }
   ],
   "source": [
    "!curl -o data/weather-2020s.json \"https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2020-01-01&end_date=2023-08-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m\"\n",
    "\n",
    "new_data = transform_json_weather_data('./data/weather-2020s.json')\n",
    "new_data.to_csv(f'lakefs://{REPO_NAME}/main/weather_2020s.csv')\n",
    "new_data = new_data.drop('time', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6147a62",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's concatenate the old data and the new data, create a new train-test split, and overwrite the files on lakeFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df9e01e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:08.451418Z",
     "iopub.status.busy": "2023-11-07T15:55:08.451295Z",
     "iopub.status.idle": "2023-11-07T15:55:09.097261Z",
     "shell.execute_reply": "2023-11-07T15:55:09.097008Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to commit on branch 'training', aborting commit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No changes to commit on branch 'training', aborting commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting Commit: Add file train_weather.csv\n",
      "Attempting Commit: Add file train_weather.csv\n",
      "Attempting Commit: Add file test_weather.csv\n",
      "Attempting Commit: Add file test_weather.csv\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "df_test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "\n",
    "full_data = pd.concat([new_data, df_train, df_test])\n",
    "\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(full_data, random_state=7)\n",
    "\n",
    "train_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "test_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f30543",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We may now read the updated data directly from lakeFS and check their shape to insure that initial files `train_weather.csv` and `test_weather.csv` have been overwritten successfully (number of rows should be significantly higher as 2020 data were added):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cc07f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:09.098663Z",
     "iopub.status.busy": "2023-11-07T15:55:09.098585Z",
     "iopub.status.idle": "2023-11-07T15:55:09.209177Z",
     "shell.execute_reply": "2023-11-07T15:55:09.208906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train updated data shape: (89802, 15)\n",
      "test updated data shape: (29934, 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "print(f'train updated data shape: {train.shape}')\n",
    "print(f'test updated data shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a5345",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we may train the model based on the new train data and validate based on the new test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83dee0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:09.210638Z",
     "iopub.status.busy": "2023-11-07T15:55:09.210556Z",
     "iopub.status.idle": "2023-11-07T15:55:20.423141Z",
     "shell.execute_reply": "2023-11-07T15:55:20.422856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 87.27000000000001 %\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
    "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b00a6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Accessing Data Version and Reproducing Experiment\n",
    "\n",
    "Let's assume we need to go to our initial data and reproduce initial experiment (initial model with its initial accuracy). This might be tricky as we have overwritten initial train and test data on lakeFS.\n",
    "\n",
    "To enable data versioning we should save the `ref` of the specific datasets. `ref` can be a branch we are pulling a file from LakeFS. `ref` can be also a commit id - then you can access different data versions within the same branch and not only the version from the latest commit. Therefore, we will use explicit versioning and get the actual commit SHA. We have multiple ways to do this. Manually, we could go into the lakeFS UI, select the training branch, and navigate to the \"Commits\" tab. There, we could see the latest two commits, titled `Add file test_weather.csv` and `Add file train_weather.csv`, and copy their IDs.\n",
    "\n",
    "However, we want to automate as much as possible and therefore use a helper function. You find pre-written helper functions in the `lakefs_spec.client_helpers` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c494c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:20.424565Z",
     "iopub.status.busy": "2023-11-07T15:55:20.424480Z",
     "iopub.status.idle": "2023-11-07T15:55:20.437320Z",
     "shell.execute_reply": "2023-11-07T15:55:20.437082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07b25e9f5d12d46baf118a78328bc632a1b732d97ba9faba250a4bd882cea0f0\n"
     ]
    }
   ],
   "source": [
    "from lakefs_spec.client_helpers import rev_parse\n",
    "\n",
    "fixed_commit_id  = rev_parse(fs.client, REPO_NAME, TRAINING_BRANCH, parent=2) # parent is a relative number of a commit when 0 is the latest\n",
    "print(fixed_commit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9118f9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "With our commit hook setup, both DataFrame.to_csv() operations create an individual commit. lakeFS saves the state of every file at every commit. To get other commits with the rev_parse function, you can change the repo, branch parameters. To go back in the chosen branch's commit history, you can increase the `parent` parameter. In our case the initial data was commited 3 commits ago - we count the latest commit on a branch as 0, thus `parent` = 2.\n",
    "Let's check whether we manage to get the initial train and test data with this commit SHA - let's compare the shape to the initial data shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2571fd8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:20.438584Z",
     "iopub.status.busy": "2023-11-07T15:55:20.438509Z",
     "iopub.status.idle": "2023-11-07T15:55:20.517596Z",
     "shell.execute_reply": "2023-11-07T15:55:20.517337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (65718, 15)\n",
      "test data shape: (21906, 15)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(f\"lakefs://{REPO_NAME}/{fixed_commit_id}/train_weather.csv\", index_col=0)\n",
    "test = pd.read_csv(f\"lakefs://{REPO_NAME}/{fixed_commit_id}/test_weather.csv\", index_col=0)\n",
    "\n",
    "print(f'train data shape: {train.shape}')\n",
    "print(f'test data shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c2f53d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's train and validate the model based on re-fetched data and see whether we manage to reproduce the initial accuracy ratio:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fc6d28c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:20.519094Z",
     "iopub.status.busy": "2023-11-07T15:55:20.519006Z",
     "iopub.status.idle": "2023-11-07T15:55:28.434721Z",
     "shell.execute_reply": "2023-11-07T15:55:28.434412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 87.99 %\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
    "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b67e5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Using a Tag instead of a commit SHA for semantic versioning\n",
    "The above method for data versioning works great when you have experiment tracking tools to store and retrieve the commit SHA in automated pipelines. But it is hard to remember and tedious to retrieve in manual prototyping. We can make selected versions of the dataset more accessible with semantic versioning. We attach a human-interpretable tag that points to a specific commit SHA.\n",
    "\n",
    "The `client_helpers` module of the `lakefs-spec` library provides the helper function `create_tag` to achieve this. We make a semantic tag point to the `fixed_commit_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78e7c407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:28.436130Z",
     "iopub.status.busy": "2023-11-07T15:55:28.436046Z",
     "iopub.status.idle": "2023-11-07T15:55:28.458216Z",
     "shell.execute_reply": "2023-11-07T15:55:28.457999Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "from lakefs_spec.client_helpers import create_tag\n",
    "\n",
    "TAG='train-test-split'\n",
    "\n",
    "create_tag(client=fs.client, repository=REPO_NAME,ref=fixed_commit_id, tag=TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7854c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we can access the specific files with the semantic tag. Now the `fixed_commit_id` and `TAG` reference the same version `ref` in lakeFS whereas specifying a branch points to the latest version on the respective branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "158ec4aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:28.459507Z",
     "iopub.status.busy": "2023-11-07T15:55:28.459439Z",
     "iopub.status.idle": "2023-11-07T15:55:28.653668Z",
     "shell.execute_reply": "2023-11-07T15:55:28.653398Z"
    }
   },
   "outputs": [],
   "source": [
    "train_from_branch_head = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "train_from_commit_sha = pd.read_csv(f'lakefs://{REPO_NAME}/{fixed_commit_id}/train_weather.csv', index_col=0)\n",
    "train_from_semantic_tag = pd.read_csv(f'lakefs://{REPO_NAME}/{TAG}/train_weather.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d4639a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We can verify this by looking at the lengths of the `DataFrame`s. We see that the `train_from_commit_sha` and `train_from_semantic_tag` are equal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "298ebd2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T15:55:28.655155Z",
     "iopub.status.busy": "2023-11-07T15:55:28.655075Z",
     "iopub.status.idle": "2023-11-07T15:55:28.656981Z",
     "shell.execute_reply": "2023-11-07T15:55:28.656741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89802\n",
      "65718\n",
      "65718\n"
     ]
    }
   ],
   "source": [
    "print(len(train_from_branch_head))\n",
    "print(len(train_from_commit_sha))\n",
    "print(len(train_from_semantic_tag))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
