{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will complete a small end to end data science tutorial that employs LakeFS-spec for data versioning.\n",
    "We will use weather data to train a random forest classifier and aim to predict whether it is raining a day from now given the current weather. \n",
    "\n",
    "Lets set the environment up. You can create it with `python -m venv .demo-venv` in the command line. Then activate it (`source .demo-venv/bin/activate`) and execute `pip install -r demo-requirements.txt` to install the dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us set-up LakeFS. You can do this by executing the `docker run` command given here [lakeFS quickstart](https://docs.lakefs.io/quickstart/launch.html) in a terminal of your choice. Open a browser and navigate to the lakeFS instance (per default`localhost:8000`). Authenticate with the credentials given in the terminal where you executed the docker container. As an email, you can enter anything, we won't need it in this example. Proceed to create an empty repository. You may call it `weather`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also install the CLI of lakeFS, `lakectl`. Then lakeFS-spec can automatically handle authentication. To do so, open a terminal of your choice and `brew install lakefs`. Then use `lakectl config`. You find the authentication information in the terminal window where you started the LakeFS Docker container. \n",
    "\n",
    "Note: for this to work, you need the `pyyaml` package which is not a default dependency of LakeFS-spec. We already installed via the `demo-requirements.txt`. In you own projects you need to add the dependency manually, if you want to use the LakeCTL authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = 'weather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to get some data. We will use the [Open-Meteo api](https://open-meteo.com/), where we can pull weather data from an API for free (as long as we are non-commercial) and without an API-token.  \n",
    "\n",
    "For trainig we get the full data of the 2010s from Munich (where I am writing this right now ;) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 6297k    0 6297k    0     0  1195k      0 --:--:--  0:00:05 --:--:-- 1312k7k      0 --:--:--  0:00:01 --:--:-- 1108k\n"
     ]
    }
   ],
   "source": [
    "!curl -o './data/weather-2010s.json' 'https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2010-01-01&end_date=2019-12-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in JSON format so we need to wrangle the data a bit to make it usable. But first we will save it into our lakeFS instance. We will create a new branch, `transform-raw-data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new branch 'transform-raw-data' from branch 'main'.\n"
     ]
    }
   ],
   "source": [
    "from lakefs_spec import LakeFSFileSystem\n",
    "\n",
    "NEW_BRANCH_NAME = 'transform-raw-data'\n",
    "\n",
    "\n",
    "fs = LakeFSFileSystem()\n",
    "fs.put('./data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on LakeFS in your browser, can change the branch to `transform-raw-data` and see the saved file. However, the change is not yet committed. \n",
    "While you can do that manually via the uncommitted changes tab in Lakefs, we will take another route to commit. \n",
    "\n",
    "We can also do commit changes programmatically. To achieve this, we register a hook. The hook needs to have the signature `(client, context) -> None`. Where the `client` is the respective LakeFS client. The context object contains information about the corresponding resource. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.client import LakeFSClient\n",
    "from lakefs_spec.client_helpers import commit\n",
    "from lakefs_spec.hooks import FSEvent, HookContext\n",
    "\n",
    "#Define the commit hook\n",
    "def commit_on_put(client: LakeFSClient, ctx:HookContext) -> None:\n",
    "    commit_message = f\"Add file {ctx.resource}\"\n",
    "    print(f\"Attempting Commit: {commit_message}\")\n",
    "    commit(client, repository=ctx.repository, branch = ctx.ref, message=commit_message)\n",
    "    \n",
    "\n",
    "#Register the commit hook to be executed after a PUT_FILE event\n",
    "fs.register_hook(FSEvent.PUT_FILE, commit_on_put)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the next cell however, you will see a message indicating that the upload of the resource has been skipped because the file is uploaded to LakeFS already (checksums match). This is useful when we work with large files to reduce the amount of network traffic. Nonetheless, in this specific situation the `PUT` is not executed and neither is our commit hook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping upload of resource '/Users/maxmynter/Desktop/appliedAI/lakefs/spec/demos/data/weather-2010s.json' to remote path 'weather/transform-raw-data/weather-2010.json': Resource 'weather/transform-raw-data/weather-2010.json' exists and checksums match.\n"
     ]
    }
   ],
   "source": [
    "fs.put('./data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can circumvent this by disabling checking the checksums on a specific put operation. We do this by passing `precheck=False` to the `PUT` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting Commit: Add file weather-2010.json\n"
     ]
    }
   ],
   "source": [
    "fs.put('./data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json', precheck=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform the data for our use case. We put the transformation into a function such that we can reuse it later\n",
    "\n",
    "In this notebook, we follow a simple toy example to predict whether it is raining at the same time tomorrow given weather data from right now. \n",
    "\n",
    "However, we will skip a lot of possible feature engineering etc. in order to focus on the application of LakeFS and LakeFS-spec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relativehumidity_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>pressure_msl</th>\n",
       "      <th>surface_pressure</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>cloudcover_low</th>\n",
       "      <th>cloudcover_mid</th>\n",
       "      <th>cloudcover_high</th>\n",
       "      <th>windspeed_10m</th>\n",
       "      <th>windspeed_100m</th>\n",
       "      <th>winddirection_10m</th>\n",
       "      <th>winddirection_100m</th>\n",
       "      <th>is_raining</th>\n",
       "      <th>is_raining_in_1_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010-01-02 00:00:00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>999.2</td>\n",
       "      <td>100</td>\n",
       "      <td>54</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>8.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2010-01-02 01:00:00</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.5</td>\n",
       "      <td>999.7</td>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>9.1</td>\n",
       "      <td>18.6</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2010-01-02 02:00:00</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.2</td>\n",
       "      <td>1000.4</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>98</td>\n",
       "      <td>77</td>\n",
       "      <td>10.1</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2010-01-02 03:00:00</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>1000.8</td>\n",
       "      <td>93</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>89</td>\n",
       "      <td>11.2</td>\n",
       "      <td>21.7</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-01-02 04:00:00</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.2</td>\n",
       "      <td>1001.4</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>95</td>\n",
       "      <td>11.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>358</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  temperature_2m  relativehumidity_2m  rain  \\\n",
       "24 2010-01-02 00:00:00            -3.0                   88   0.0   \n",
       "25 2010-01-02 01:00:00            -3.2                   89   0.0   \n",
       "26 2010-01-02 02:00:00            -3.4                   89   0.0   \n",
       "27 2010-01-02 03:00:00            -3.5                   89   0.0   \n",
       "28 2010-01-02 04:00:00            -3.7                   90   0.0   \n",
       "\n",
       "    pressure_msl  surface_pressure  cloudcover  cloudcover_low  \\\n",
       "24        1004.0             999.2         100              54   \n",
       "25        1004.5             999.7         100              37   \n",
       "26        1005.2            1000.4         100              24   \n",
       "27        1005.6            1000.8          93               8   \n",
       "28        1006.2            1001.4          94               6   \n",
       "\n",
       "    cloudcover_mid  cloudcover_high  windspeed_10m  windspeed_100m  \\\n",
       "24             100               70            8.3            17.3   \n",
       "25              98               92            9.1            18.6   \n",
       "26              98               77           10.1            20.3   \n",
       "27              98               89           11.2            21.7   \n",
       "28             100               95           11.2            21.8   \n",
       "\n",
       "    winddirection_10m  winddirection_100m  is_raining is_raining_in_1_day  \n",
       "24                 18                  31       False               False  \n",
       "25                  9                  22       False               False  \n",
       "26                  2                  13       False               False  \n",
       "27                  4                  12       False               False  \n",
       "28                358                   9       False               False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "\n",
    "def transform_json_weather_data(filepath):\n",
    "    f = open(filepath)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data[\"hourly\"])\n",
    "    df.time = pd.to_datetime(df.time)\n",
    "    df['is_raining'] = df.rain > 0\n",
    "    df['is_raining_in_1_day'] = df.is_raining.shift(24)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "    \n",
    "df = transform_json_weather_data('./data/weather-2010s.json')\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now save this data as a csv into the main branch. The `.to_csv` method calls a `put` operation behind the scenes, our commit hook is called and the file committed. You can verify the saving worked in your LakeFS GUI in the browser when looking at the Comits and uncommitted changes tabs of the main branch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file weather_2010s.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(f'lakefs://{REPO_NAME}/main/weather_2010s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do a train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "model_data=df.drop('time', axis=1)\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save these train and test datasets into a new `training` branch. If, as in this case, the branch does not yet exist, it is implicitly created. You can steer this behaviour via the `create_branch_ok` flag when initializing the `LakeFSFileSystem`. The flag defaults to `True`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Created new branch 'training' from branch 'main'.\n",
      "Attempting Commit: Add file train_weather.csv\n",
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file test_weather.csv\n"
     ]
    }
   ],
   "source": [
    "TRAINING_BRANCH = 'training'\n",
    "train.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "test.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implicit branch creation is a convenient way to create new branches programmatically. However, one drawback is that typos might result in new branch creation. If you want these to throw errors instead, disable the implicit branch creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read these files directly from the remote LakeFS instance. (You can verify that neither the train nor the test file are saved in the `/data` directory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relativehumidity_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>pressure_msl</th>\n",
       "      <th>surface_pressure</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>cloudcover_low</th>\n",
       "      <th>cloudcover_mid</th>\n",
       "      <th>cloudcover_high</th>\n",
       "      <th>windspeed_10m</th>\n",
       "      <th>windspeed_100m</th>\n",
       "      <th>winddirection_10m</th>\n",
       "      <th>winddirection_100m</th>\n",
       "      <th>is_raining</th>\n",
       "      <th>is_raining_in_1_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40255</th>\n",
       "      <td>20.5</td>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015.7</td>\n",
       "      <td>1011.2</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>31</td>\n",
       "      <td>8.2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>322</td>\n",
       "      <td>323</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26345</th>\n",
       "      <td>4.9</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1021.7</td>\n",
       "      <td>1016.9</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>17.7</td>\n",
       "      <td>31.7</td>\n",
       "      <td>257</td>\n",
       "      <td>262</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8914</th>\n",
       "      <td>1.7</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012.1</td>\n",
       "      <td>1007.3</td>\n",
       "      <td>100</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>5.7</td>\n",
       "      <td>9.4</td>\n",
       "      <td>252</td>\n",
       "      <td>274</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68183</th>\n",
       "      <td>13.9</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.9</td>\n",
       "      <td>1009.3</td>\n",
       "      <td>93</td>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>224</td>\n",
       "      <td>229</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86307</th>\n",
       "      <td>6.8</td>\n",
       "      <td>96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>997.1</td>\n",
       "      <td>992.5</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>97</td>\n",
       "      <td>96</td>\n",
       "      <td>13.7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>243</td>\n",
       "      <td>248</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature_2m  relativehumidity_2m  rain  pressure_msl  \\\n",
       "40255            20.5                   83   0.0        1015.7   \n",
       "26345             4.9                   89   0.0        1021.7   \n",
       "8914              1.7                   97   0.0        1012.1   \n",
       "68183            13.9                   74   0.0        1013.9   \n",
       "86307             6.8                   96   0.0         997.1   \n",
       "\n",
       "       surface_pressure  cloudcover  cloudcover_low  cloudcover_mid  \\\n",
       "40255            1011.2          44               4              51   \n",
       "26345            1016.9          86              85              12   \n",
       "8914             1007.3         100              95               0   \n",
       "68183            1009.3          93              97               4   \n",
       "86307             992.5         100             100              97   \n",
       "\n",
       "       cloudcover_high  windspeed_10m  windspeed_100m  winddirection_10m  \\\n",
       "40255               31            8.2            11.3                322   \n",
       "26345                6           17.7            31.7                257   \n",
       "8914                99            5.7             9.4                252   \n",
       "68183               10           16.0            28.6                224   \n",
       "86307               96           13.7            24.4                243   \n",
       "\n",
       "       winddirection_100m  is_raining  is_raining_in_1_day  \n",
       "40255                 323       False                False  \n",
       "26345                 262       False                 True  \n",
       "8914                  274       False                 True  \n",
       "68183                 229       False                False  \n",
       "86307                 248       False                False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to train a random forest classifier and evaluate it on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.92 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dependent_variable = 'is_raining_in_1_day'\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
    "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we only have used data from the 2010s. Lets get additional 2020s data, transform it and save it to LakeFS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2308k    0 2308k    0     0  1092k      0 --:--:--  0:00:02 --:--:-- 1094k\n",
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file weather_2020s.csv\n"
     ]
    }
   ],
   "source": [
    "!curl -o './data/weather-2020s.json' 'https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2020-01-01&end_date=2023-08-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m'\n",
    "\n",
    "new_data = transform_json_weather_data('./data/weather-2020s.json')\n",
    "new_data.to_csv(f'lakefs://{REPO_NAME}/main/weather_2020s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test how well our model performs on 2020s data.\n",
    "\n",
    "First, we drop the `time` column such that we have the same variables as during the fit in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.drop('time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 84.54 %\n"
     ]
    }
   ],
   "source": [
    "acc_2020s = model.score(new_data.drop(dependent_variable, axis=1), new_data[dependent_variable].astype(bool))\n",
    "\n",
    "print(f\"Test accuracy: {round(acc_2020s, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an accuracy similar to the one we had on the 2020s data. Yet, it makes sense to utilize this data for training as well. We will create a concatenated dataframe and perform a new train test split. \n",
    "\n",
    "However, this gives rise to the problem, that we now have multiple models which will perfom differently on different datasets. For example, if someone takes the model we are about to train and evaluates it on the data from the 2020s the accuracy will probably be higher. But it will be higher because of data leakage. We are going to use some of the data points in the 2020s data to train. \n",
    "\n",
    "To circumvent this issue (or at least enable the traceability and reproducibility) we should save the `ref` of the specific datasets. `ref` can be the branch we are pulling the file from LakeFS from.  \n",
    "\n",
    "\n",
    "We are going to implement versioning with the commit ids now.\n",
    "\n",
    "First we create the new train test split and save it in the training branch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file train_weather.csv\n",
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file test_weather.csv\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "df_test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "\n",
    "full_data = pd.concat([new_data, df_train, df_test])\n",
    "\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(full_data, test_size=0.9)\n",
    "\n",
    "train_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "test_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now concatenated the old data, created a new train test split and overwrote the files. This presents problems with respect to strict versioning. When we get the data using only the branch name and filename we get the latest commit. \n",
    "\n",
    "Lets use explicit versioning and get the explicit commit. Therefore go into the LakeFS GUI, select the training branch and choose the \"Commits\" tab. \n",
    "\n",
    "You should see the latest two Commits `Add file test_weather.csv` and `Add file train_weather.csv`.\n",
    "\n",
    "Copy the ID to your clipboard and paste it below. (They should look something like this `be9e4c17be128bd86a082e9c5eb63135160699edd135b8b6eb78180d070b31a1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_commit_id  = ''\n",
    "train_commit_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the specific dataset versions irrespective of subsequent changes to the files on the branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"lakefs://{REPO_NAME}/{train_commit_id}/train_weather.csv\", index_col=0)\n",
    "test = pd.read_csv(f\"lakefs://{REPO_NAME}/{test_commit_id}/test_weather.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.82 %\n"
     ]
    }
   ],
   "source": [
    "model.fit(train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool))\n",
    "\n",
    "test_acc = model.score(test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool))\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done. We have a model trained on the new data. \n",
    "\n",
    "Now we can save the commit ids as well as the model and accuracy metrics into an experiment tracking tool of our choice. With this, we achieve reproducible experiments and have a clear trail on what data input resulted in which model weights with the resulting hyperparameter results.  \n",
    "\n",
    "If in the future we decide to train another model or switch the data up or invest more time in feature engineering, we can always come back to the current state to study the model performance and draw insights that might help us down the road. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
