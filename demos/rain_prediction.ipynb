{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will complete a small end to end data science tutorial that employs LakeFS-spec for data versioning.\n",
    "We will use weather data to train a random forest classifier and aim to predict whether it is raining a day from now given the current weather. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set the environment up. You can use the commands below. We first create an environment  (`python -m venv .venv`), activate it (`source .venv/bin/activate`) and install the dependencies (`pip install -r requirements.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lakefs-spec@ git+https://github.com/appliedAI-Initiative/lakefs-spec.git@e2d188c937310b6053759e7db56804235c64e352\n",
      "  Using cached lakefs_spec-0.1.5.dev3+ge2d188c-py3-none-any.whl\n",
      "Requirement already satisfied: appnope==0.1.3 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.1.3)\n",
      "Requirement already satisfied: asttokens==2.4.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: comm==0.1.4 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.1.4)\n",
      "Requirement already satisfied: debugpy==1.7.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.7.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (5.1.1)\n",
      "Requirement already satisfied: executing==1.2.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: fsspec==2023.9.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (2023.9.0)\n",
      "Requirement already satisfied: ipykernel==6.25.2 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (6.25.2)\n",
      "Requirement already satisfied: ipython==8.15.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (8.15.0)\n",
      "Requirement already satisfied: jedi==0.19.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.19.0)\n",
      "Requirement already satisfied: joblib==1.3.2 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: jupyter_client==8.3.1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (8.3.1)\n",
      "Requirement already satisfied: jupyter_core==5.3.1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (5.3.1)\n",
      "Requirement already satisfied: lakefs-client==0.109.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (0.109.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio==1.5.7 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (1.5.7)\n",
      "Requirement already satisfied: numpy==1.25.2 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (1.25.2)\n",
      "Requirement already satisfied: packaging==23.1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (23.1)\n",
      "Requirement already satisfied: pandas==2.1.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (2.1.0)\n",
      "Requirement already satisfied: parso==0.8.3 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (0.8.3)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (0.7.5)\n",
      "Requirement already satisfied: platformdirs==3.10.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (3.10.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.39 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (3.0.39)\n",
      "Requirement already satisfied: psutil==5.9.5 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (5.9.5)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (0.2.2)\n",
      "Requirement already satisfied: Pygments==2.16.1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (2.16.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2023.3.post1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (2023.3.post1)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (6.0.1)\n",
      "Requirement already satisfied: pyzmq==25.1.1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (25.1.1)\n",
      "Requirement already satisfied: scikit-learn==1.3.1 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (1.3.1)\n",
      "Requirement already satisfied: scipy==1.11.2 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 36)) (1.11.2)\n",
      "Requirement already satisfied: six==1.16.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 37)) (1.16.0)\n",
      "Requirement already satisfied: stack-data==0.6.2 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (0.6.2)\n",
      "Requirement already satisfied: threadpoolctl==3.2.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 39)) (3.2.0)\n",
      "Requirement already satisfied: tornado==6.3.3 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 40)) (6.3.3)\n",
      "Requirement already satisfied: traitlets==5.9.0 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (5.9.0)\n",
      "Requirement already satisfied: tzdata==2023.3 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (2023.3)\n",
      "Requirement already satisfied: urllib3==2.0.4 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (2.0.4)\n",
      "Requirement already satisfied: wcwidth==0.2.6 in /Users/maxmynter/Desktop/appliedAI/lakefs/spec/.demo-venv/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (0.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!python -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us set up LakeFS. You can do this by executing the `docker run` command given here [lakeFS quickstart](https://docs.lakefs.io/quickstart/launch.html) in a terminal of your choice. Open a browser and navigate to the lakeFS instance (by default`localhost:8000`). Authenticate with the credentials given in the terminal where you executed the docker container. As an email, you can enter anything, we won't need it in this example. Proceed to create an empty repository. You may call it `weather`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also install the CLI of lakeFS, `lakectl`. Then lakeFS-spec can automatically handle authentication. To do so, open a terminal of your choice and `brew install lakefs`. Then use `lakectl config`. You find the authentication information in the terminal window where you started the LakeFS Docker container. \n",
    "\n",
    "Note: for this to work, you need the `pyyaml` package which is not a default dependency of LakeFS-spec. We already installed via the `demo-requirements.txt`. In you own projects you need to add the dependency manually, for example by running `pip install --upgrade pyyaml`, if you want to use automatic authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = 'weather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to get some data. We will use the [Open-Meteo api](https://open-meteo.com/), where we can pull weather data from an API for free (as long as we are non-commercial) and without an API-token.  \n",
    "\n",
    "For training, we get the full data of the 2010s from Munich (where I am writing this right now ;) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o 'data/weather-2010s.json' 'https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2010-01-01&end_date=2019-12-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in JSON format so we need to wrangle the data a bit to make it usable. But first we will save it into our lakeFS instance. We will create a new branch, `transform-raw-data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new branch 'transform-raw-data' from branch 'main'.\n"
     ]
    }
   ],
   "source": [
    "from lakefs_spec import LakeFSFileSystem\n",
    "\n",
    "NEW_BRANCH_NAME = 'transform-raw-data'\n",
    "\n",
    "\n",
    "fs = LakeFSFileSystem()\n",
    "fs.put('./data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on LakeFS in your browser, can change the branch to `transform-raw-data` and see the saved file. However, the change is not yet committed. \n",
    "While you can do that manually via the uncommitted changes tab in the lakeFS UI, we will commit the file in a different way. \n",
    "\n",
    "To commit changes programmatically, we can register a hook. This hook needs to have the signature `(client, context) -> None`, where the `client` is the file system's LakeFS client. The context object contains information about the requested resource. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.client import LakeFSClient\n",
    "from lakefs_spec.client_helpers import commit\n",
    "from lakefs_spec.hooks import FSEvent, HookContext\n",
    "\n",
    "#Define the commit hook\n",
    "def commit_on_put(client: LakeFSClient, ctx:HookContext) -> None:\n",
    "    commit_message = f\"Add file {ctx.resource}\"\n",
    "    print(f\"Attempting Commit: {commit_message}\")\n",
    "    commit(client, repository=ctx.repository, branch=ctx.ref, message=commit_message)\n",
    "    \n",
    "\n",
    "#Register the commit hook to be executed after a PUT_FILE event\n",
    "fs.register_hook(FSEvent.PUT_FILE, commit_on_put)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when you execute the next cell, you will see a message indicating that the upload of the resource has been skipped because the file is uploaded to lakeFS already, and the checksums match. This is useful when we work with large files to reduce the amount of network traffic. Nonetheless, in this specific situation the `PUT` is not executed and neither is our commit hook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping upload of resource '/Users/maxmynter/Desktop/appliedAI/lakefs/spec/demos/data/weather-2010s.json' to remote path 'weather/transform-raw-data/weather-2010.json': Resource 'weather/transform-raw-data/weather-2010.json' exists and checksums match.\n"
     ]
    }
   ],
   "source": [
    "fs.put('data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can circumvent this by disabling pre-checking file checksums on a specific put operation. We do this by passing `precheck=False` to the `PUT` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting Commit: Add file weather-2010.json\n"
     ]
    }
   ],
   "source": [
    "fs.put('data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json', precheck=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform the data for our use case. We put the transformation into a function such that we can reuse it later\n",
    "\n",
    "In this notebook, we follow a simple toy example to predict whether it is raining at the same time tomorrow given weather data from right now. \n",
    "\n",
    "We will skip a lot of possible feature engineering etc. in order to focus on the application of lakeFS and the `LakeFSFileSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relativehumidity_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>pressure_msl</th>\n",
       "      <th>surface_pressure</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>cloudcover_low</th>\n",
       "      <th>cloudcover_mid</th>\n",
       "      <th>cloudcover_high</th>\n",
       "      <th>windspeed_10m</th>\n",
       "      <th>windspeed_100m</th>\n",
       "      <th>winddirection_10m</th>\n",
       "      <th>winddirection_100m</th>\n",
       "      <th>is_raining</th>\n",
       "      <th>is_raining_in_1_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010-01-02 00:00:00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>999.2</td>\n",
       "      <td>100</td>\n",
       "      <td>54</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>8.3</td>\n",
       "      <td>17.3</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2010-01-02 01:00:00</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.5</td>\n",
       "      <td>999.7</td>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>9.1</td>\n",
       "      <td>18.6</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2010-01-02 02:00:00</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.2</td>\n",
       "      <td>1000.4</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>98</td>\n",
       "      <td>77</td>\n",
       "      <td>10.1</td>\n",
       "      <td>20.3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2010-01-02 03:00:00</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>1000.8</td>\n",
       "      <td>93</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>89</td>\n",
       "      <td>11.2</td>\n",
       "      <td>21.7</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-01-02 04:00:00</td>\n",
       "      <td>-3.7</td>\n",
       "      <td>90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.2</td>\n",
       "      <td>1001.4</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>95</td>\n",
       "      <td>11.2</td>\n",
       "      <td>21.8</td>\n",
       "      <td>358</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  temperature_2m  relativehumidity_2m  rain  \\\n",
       "24 2010-01-02 00:00:00            -3.0                   88   0.0   \n",
       "25 2010-01-02 01:00:00            -3.2                   89   0.0   \n",
       "26 2010-01-02 02:00:00            -3.4                   89   0.0   \n",
       "27 2010-01-02 03:00:00            -3.5                   89   0.0   \n",
       "28 2010-01-02 04:00:00            -3.7                   90   0.0   \n",
       "\n",
       "    pressure_msl  surface_pressure  cloudcover  cloudcover_low  \\\n",
       "24        1004.0             999.2         100              54   \n",
       "25        1004.5             999.7         100              37   \n",
       "26        1005.2            1000.4         100              24   \n",
       "27        1005.6            1000.8          93               8   \n",
       "28        1006.2            1001.4          94               6   \n",
       "\n",
       "    cloudcover_mid  cloudcover_high  windspeed_10m  windspeed_100m  \\\n",
       "24             100               70            8.3            17.3   \n",
       "25              98               92            9.1            18.6   \n",
       "26              98               77           10.1            20.3   \n",
       "27              98               89           11.2            21.7   \n",
       "28             100               95           11.2            21.8   \n",
       "\n",
       "    winddirection_10m  winddirection_100m  is_raining is_raining_in_1_day  \n",
       "24                 18                  31       False               False  \n",
       "25                  9                  22       False               False  \n",
       "26                  2                  13       False               False  \n",
       "27                  4                  12       False               False  \n",
       "28                358                   9       False               False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "\n",
    "def transform_json_weather_data(filepath):\n",
    "    f = open(filepath)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data[\"hourly\"])\n",
    "    df.time = pd.to_datetime(df.time)\n",
    "    df['is_raining'] = df.rain > 0\n",
    "    df['is_raining_in_1_day'] = df.is_raining.shift(24)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "    \n",
    "df = transform_json_weather_data('data/weather-2010s.json')\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now save this data as a csv into the main branch. The `.to_csv` method calls a `put` operation behind the scenes, our commit hook is called and the file committed. You can verify the saving worked in your LakeFS GUI in the browser when looking at the Comits and uncommitted changes tabs of the main branch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file weather_2010s.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(f'lakefs://{REPO_NAME}/main/weather_2010s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "model_data=df.drop('time', axis=1)\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save these train and test datasets into a new `training` branch. If the branch does not yet exist, as in this case, it is implicitly created by default. You can control this behaviour with the `create_branch_ok` flag when initializing the `LakeFSFileSystem`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Created new branch 'training' from branch 'main'.\n",
      "Attempting Commit: Add file train_weather.csv\n",
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file test_weather.csv\n"
     ]
    }
   ],
   "source": [
    "TRAINING_BRANCH = 'training'\n",
    "train.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "test.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit branch creation is a convenient way to create new branches programmatically. However, one drawback is that typos in your code might result in new accidental branch creations. If you want to avoid this implicit behavior and raise errors instead, you can disable implicit branch creation by setting `fs.create_branch_ok=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read these files directly from the remote LakeFS instance. (You can verify that neither the train nor the test file are saved in the `/data` directory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relativehumidity_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>pressure_msl</th>\n",
       "      <th>surface_pressure</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>cloudcover_low</th>\n",
       "      <th>cloudcover_mid</th>\n",
       "      <th>cloudcover_high</th>\n",
       "      <th>windspeed_10m</th>\n",
       "      <th>windspeed_100m</th>\n",
       "      <th>winddirection_10m</th>\n",
       "      <th>winddirection_100m</th>\n",
       "      <th>is_raining</th>\n",
       "      <th>is_raining_in_1_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84769</th>\n",
       "      <td>13.3</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1023.3</td>\n",
       "      <td>1018.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>255</td>\n",
       "      <td>270</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65384</th>\n",
       "      <td>16.2</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1019.2</td>\n",
       "      <td>1014.6</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>301</td>\n",
       "      <td>302</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56122</th>\n",
       "      <td>18.6</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.3</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>4.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>318</td>\n",
       "      <td>317</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34991</th>\n",
       "      <td>7.2</td>\n",
       "      <td>86</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1004.9</td>\n",
       "      <td>1000.3</td>\n",
       "      <td>100</td>\n",
       "      <td>91</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>5.4</td>\n",
       "      <td>12.1</td>\n",
       "      <td>176</td>\n",
       "      <td>203</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70540</th>\n",
       "      <td>0.1</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001.7</td>\n",
       "      <td>997.0</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>32.1</td>\n",
       "      <td>228</td>\n",
       "      <td>233</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature_2m  relativehumidity_2m  rain  pressure_msl  \\\n",
       "84769            13.3                   72   0.0        1023.3   \n",
       "65384            16.2                   60   0.0        1019.2   \n",
       "56122            18.6                   65   0.0        1017.3   \n",
       "34991             7.2                   86   0.3        1004.9   \n",
       "70540             0.1                   88   0.0        1001.7   \n",
       "\n",
       "       surface_pressure  cloudcover  cloudcover_low  cloudcover_mid  \\\n",
       "84769            1018.7           1               0               1   \n",
       "65384            1014.6          90             100               0   \n",
       "56122            1012.8          54              15              17   \n",
       "34991            1000.3         100              91              99   \n",
       "70540             997.0          34              38               0   \n",
       "\n",
       "       cloudcover_high  windspeed_10m  windspeed_100m  winddirection_10m  \\\n",
       "84769                0            5.6            12.2                255   \n",
       "65384                1           27.0            38.7                301   \n",
       "56122              100            4.8             5.9                318   \n",
       "34991               95            5.4            12.1                176   \n",
       "70540                0           17.3            32.1                228   \n",
       "\n",
       "       winddirection_100m  is_raining  is_raining_in_1_day  \n",
       "84769                 270       False                False  \n",
       "65384                 302       False                 True  \n",
       "56122                 317       False                 True  \n",
       "34991                 203        True                False  \n",
       "70540                 233       False                False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to train a random forest classifier and evaluate it on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.55 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dependent_variable = 'is_raining_in_1_day'\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
    "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we only have used data from the 2010s. Let's download additional 2020s data, transform it, and save it to LakeFS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2308k    0 2308k    0     0  1128k      0 --:--:--  0:00:02 --:--:-- 1130k\n",
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file weather_2020s.csv\n"
     ]
    }
   ],
   "source": [
    "!curl -o './data/weather-2020s.json' 'https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2020-01-01&end_date=2023-08-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m'\n",
    "\n",
    "new_data = transform_json_weather_data('./data/weather-2020s.json')\n",
    "new_data.to_csv(f'lakefs://{REPO_NAME}/main/weather_2020s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test how well our model performs on 2020s data.\n",
    "\n",
    "First, we drop the `time` column such that we have the same variables as during the fit in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.drop('time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 84.6 %\n"
     ]
    }
   ],
   "source": [
    "acc_2020s = model.score(new_data.drop(dependent_variable, axis=1), new_data[dependent_variable].astype(bool))\n",
    "\n",
    "print(f\"Test accuracy: {round(acc_2020s, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an accuracy similar to the one we had on the 2020s data. Yet, it makes sense to utilize this data for training as well. We will create a concatenated dataframe and perform a new train test split. \n",
    "\n",
    "However, this means that we now have multiple models which will perform differently on different datasets. For example, if someone takes the model we are about to train and evaluates it on the 2020s data, the accuracy will probably be higher, because of data leakage. We are going to use some of the data points in the 2020s data to train. \n",
    "\n",
    "To circumvent this issue (or at least enable the traceability and reproducibility) we should save the `ref` of the specific datasets. `ref` can be the branch we are pulling the file from LakeFS from.  \n",
    "\n",
    "\n",
    "We are going to implement versioning with the commit ids now.\n",
    "\n",
    "First we create the new train test split and save it in the training branch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file train_weather.csv\n",
      "Calling open() in write mode results in unbuffered file uploads, because the lakeFS Python client does not support multipart uploads. Note that uploading large files unbuffered can have performance implications.\n",
      "Attempting Commit: Add file test_weather.csv\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "df_test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "\n",
    "full_data = pd.concat([new_data, df_train, df_test])\n",
    "\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(full_data, test_size=0.9)\n",
    "\n",
    "train_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "test_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concatenates the old data, creates a new train-test split, and overwrites the files. Of course, this presents problems with respect to strict versioning. When we get the data directly from a branch, we only get the version from the latest commit. \n",
    "\n",
    "Let's use explicit versioning instead and get the actual commit SHA. For that, go into the lakeFS UI, select the training branch, and navigate to the \"Commits\" tab. \n",
    "\n",
    "You should see the latest two commits `Add file test_weather.csv` and `Add file train_weather.csv`.\n",
    "\n",
    "Copy the ID to your clipboard and paste it below. (They should look something like this `be9e4c17be128bd86a082e9c5eb63135160699edd135b8b6eb78180d070b31a1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_commit_id  = ''\n",
    "train_commit_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the specific dataset versions irrespective of subsequent changes to the files on the branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"lakefs://{REPO_NAME}/{train_commit_id}/train_weather.csv\", index_col=0)\n",
    "test = pd.read_csv(f\"lakefs://{REPO_NAME}/{test_commit_id}/test_weather.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool))\n",
    "\n",
    "test_acc = model.score(test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool))\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done. We have a model trained on the new data. \n",
    "\n",
    "Now, we can save the commit SHAs as well as the model and accuracy metrics into an experiment tracking tool of our choice. With this, we achieve reproducible experiments and have a clear trail on what input data and hyperparameters resulted in which set of model weights.  \n",
    "\n",
    "If in the future we decide to train another model, use different data, or invest more time in feature engineering, we can always come back to the current state to study the model performance and draw insights that might help us down the road. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
