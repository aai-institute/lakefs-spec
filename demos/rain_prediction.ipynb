{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will complete a small end to end data science tutorial that employs LakeFS-spec for data versioning.\n",
    "We will use weather data to train a random forest classifier and aim to predict whether it is raining a day from now given the current weather. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the following:\n",
    "- Environment and lakeFS Setup\n",
    "- Data Ingestion\n",
    "    - Event Hooks\n",
    "    - PUT a File\n",
    "- Model Training\n",
    "- Updating Data\n",
    "    - Accessing specific Data Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment and lakeFS Setup \n",
    "Lets set the environment up. First create an environment (`python -m venv .venv`) and activate it (`source .venv/bin/activate`). Execute the comands in the parenthesis in your console. Select the environment you just created as a Jupyter kernel. Then install the dependencies by executing the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us set up LakeFS. You can do this by executing the `docker run` command given here [lakeFS quickstart](https://docs.lakefs.io/quickstart/launch.html) in a terminal of your choice. Open a browser and navigate to the lakeFS instance (by default`localhost:8000`). Authenticate with the credentials given in the terminal where you executed the docker container. As an email, you can enter anything, we won't need it in this example. Proceed to create an empty repository. You may call it `weather`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = 'weather'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also set-up automatic authentication. We can do this via two approaches. \n",
    "\n",
    "Either we install `lakectl`. Then lakeFS-spec can automatically handle authentication. To do so, open a terminal of your choice and `brew install lakefs`. Then use `lakectl config`. You find the authentication information in the terminal window where you started the LakeFS Docker container. \n",
    "\n",
    "As a second option, if we don't want to install `lakectl`, you can manually create the `.lakectl.yaml` in the default location `~/.lakectl.yaml`. It needs to contain the following information (replace the values if applicable): \n",
    "\n",
    "```yaml\n",
    "credentials:\n",
    "    access_key_id: AKIAIOSFOLQUICKSTART\n",
    "    secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n",
    "server:\n",
    "    endpoint_url: http://127.0.0.1:8000\n",
    "```\n",
    "\n",
    "Note: for the automatic authentication to work, you need the `pyyaml` package which is not a default dependency of LakeFS-spec. We already installed via the `requirements.txt`. In you own projects you need to add the dependency manually, for example by running `pip install --upgrade pyyaml`, if you want to use automatic authentication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "Now it's time to get some data. We will use the [Open-Meteo api](https://open-meteo.com/), where we can pull weather data from an API for free (as long as we are non-commercial) and without an API-token.  \n",
    "\n",
    "For training, we get the full data of the 2010s from Munich (where I am writing this right now ;) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o 'data/weather-2010s.json' 'https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2010-01-01&end_date=2019-12-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUT a file \n",
    "The data is in JSON format so we need to wrangle the data a bit to make it usable. But first we will save it into our lakeFS instance. \n",
    "\n",
    "lakeFS works similar to `git` as a versioning system. You can create `commits` that encapsulate specific changes to the data. You can also work with `branches` to fork of your own copy of the data such that you don't interfere with your colleagues. Every commit (on any branch) is identified by a `commit-SHA`. This can be used to programmatically interact with specific states of your data and enables logging of the specific versions used to create a certain model. We will cover all this later in this demo. \n",
    "\n",
    "\n",
    "For now, we will `put` the file we have. Therefore, we will create a new branch, `transform-raw-data` for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_spec import LakeFSFileSystem\n",
    "\n",
    "NEW_BRANCH_NAME = 'transform-raw-data'\n",
    "\n",
    "\n",
    "fs = LakeFSFileSystem()\n",
    "fs.put('./data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on LakeFS in your browser, can change the branch to `transform-raw-data` and see the saved file. However, the change is not yet committed. \n",
    "While you can do that manually via the uncommitted changes tab in the lakeFS UI, we will commit the file in a different way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Hooks\n",
    "To commit changes programmatically, we can register a hook. This hook needs to have the signature `(client, context) -> None`, where the `client` is the file system's LakeFS client. The context object contains information about the requested resource. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.client import LakeFSClient\n",
    "from lakefs_spec.client_helpers import commit\n",
    "from lakefs_spec.hooks import FSEvent, HookContext\n",
    "\n",
    "#Define the commit hook\n",
    "def commit_on_put(client: LakeFSClient, ctx:HookContext) -> None:\n",
    "    commit_message = f\"Add file {ctx.resource}\"\n",
    "    print(f\"Attempting Commit: {commit_message}\")\n",
    "    commit(client, repository=ctx.repository, branch=ctx.ref, message=commit_message)\n",
    "    \n",
    "\n",
    "#Register the commit hook to be executed after a PUT_FILE event\n",
    "fs.register_hook(FSEvent.PUT_FILE, commit_on_put)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when you execute the next cell, you will see a message indicating that the upload of the resource has been skipped because the file is uploaded to lakeFS already, and the checksums match. This is useful when we work with large files to reduce the amount of network traffic. Nonetheless, in this specific situation the `PUT` is not executed and neither is our commit hook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.put('data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can circumvent this by disabling pre-checking file checksums on a specific put operation. We do this by passing `precheck=False` to the `PUT` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.put('data/weather-2010s.json',  f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json', precheck=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform the data for our use case. We put the transformation into a function such that we can reuse it later\n",
    "\n",
    "In this notebook, we follow a simple toy example to predict whether it is raining at the same time tomorrow given weather data from right now. \n",
    "\n",
    "We will skip a lot of possible feature engineering etc. in order to focus on the application of lakeFS and the `LakeFSFileSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "\n",
    "def transform_json_weather_data(filepath):\n",
    "    with open(filepath,\"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data[\"hourly\"])\n",
    "    df.time = pd.to_datetime(df.time)\n",
    "    df['is_raining'] = df.rain > 0\n",
    "    df['is_raining_in_1_day'] = df.is_raining.shift(24)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "    \n",
    "df = transform_json_weather_data('data/weather-2010s.json')\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save this data as a CSV file into the main branch. The `DataFrame.to_csv` method calls an `open` operation behind the scenes, our commit hook is called and the file is committed. You can verify the saving worked in the LakeFS UI in your browser by switching to the commits tab of the main branch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'lakefs://{REPO_NAME}/main/weather_2010s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "First we will do a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "model_data=df.drop('time', axis=1)\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save these train and test datasets into a new `training` branch. If the branch does not yet exist, as in this case, it is implicitly created by default. You can control this behaviour with the `create_branch_ok` flag when initializing the `LakeFSFileSystem`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_BRANCH = 'training'\n",
    "train.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "test.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicit branch creation is a convenient way to create new branches programmatically. However, one drawback is that typos in your code might result in new accidental branch creations. If you want to avoid this implicit behavior and raise errors instead, you can disable implicit branch creation by setting `fs.create_branch_ok=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read these files directly from the remote LakeFS instance. (You can verify that neither the train nor the test file are saved in the `/data` directory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to train a random forest classifier and evaluate it on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dependent_variable = 'is_raining_in_1_day'\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
    "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Data\n",
    "Until now, we only have used data from the 2010s. Let's download additional 2020s data, transform it, and save it to LakeFS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o './data/weather-2020s.json' 'https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2020-01-01&end_date=2023-08-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m'\n",
    "\n",
    "new_data = transform_json_weather_data('./data/weather-2020s.json')\n",
    "new_data.to_csv(f'lakefs://{REPO_NAME}/main/weather_2020s.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test how well our model performs on 2020s data.\n",
    "\n",
    "First, we drop the `time` column such that we have the same variables as during the fit in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.drop('time', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_2020s = model.score(new_data.drop(dependent_variable, axis=1), new_data[dependent_variable].astype(bool))\n",
    "\n",
    "print(f\"Test accuracy: {round(acc_2020s, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing specific Data Versions\n",
    "We have an accuracy similar to the one we had on the 2020s data. Yet, it makes sense to utilize this data for training as well. We will create a concatenated dataframe and perform a new train test split. \n",
    "\n",
    "However, this means that we now have multiple models which will perform differently on different datasets. For example, if someone takes the model we are about to train and evaluates it on the 2020s data, the accuracy will probably be higher, because of data leakage. We are going to use some of the data points in the 2020s data to train. \n",
    "\n",
    "To circumvent this issue (or at least enable the traceability and reproducibility) we should save the `ref` of the specific datasets. `ref` can be the branch we are pulling the file from LakeFS from.  \n",
    "\n",
    "\n",
    "We are going to implement versioning with the commit ids now.\n",
    "\n",
    "First we create the new train test split and save it in the training branch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
    "df_test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
    "\n",
    "full_data = pd.concat([new_data, df_train, df_test])\n",
    "\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(full_data, test_size=0.9)\n",
    "\n",
    "train_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "test_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concatenates the old data, creates a new train-test split, and overwrites the files. Of course, this presents problems with respect to strict versioning. When we get the data directly from a branch, we only get the version from the latest commit. \n",
    "\n",
    "Let's use explicit versioning instead and get the actual commit SHA. For that, go into the lakeFS UI, select the training branch, and navigate to the \"Commits\" tab. \n",
    "\n",
    "You should see the latest two commits `Add file test_weather.csv` and `Add file train_weather.csv`.\n",
    "\n",
    "Copy the ID to your clipboard and paste it below. (They should look something like this `be9e4c17be128bd86a082e9c5eb63135160699edd135b8b6eb78180d070b31a1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_commit_id  = ''\n",
    "train_commit_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the specific dataset versions irrespective of subsequent changes to the files on the branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"lakefs://{REPO_NAME}/{train_commit_id}/train_weather.csv\", index_col=0)\n",
    "test = pd.read_csv(f\"lakefs://{REPO_NAME}/{test_commit_id}/test_weather.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool))\n",
    "\n",
    "test_acc = model.score(test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool))\n",
    "\n",
    "print(f\"Test accuracy: {round(test_acc, 4) * 100 } %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done. We have a model trained on the new data. \n",
    "\n",
    "Now, we can save the commit SHAs as well as the model and accuracy metrics into an experiment tracking tool of our choice. With this, we achieve reproducible experiments and have a clear trail on what input data and hyperparameters resulted in which set of model weights.  \n",
    "\n",
    "If in the future we decide to train another model, use different data, or invest more time in feature engineering, we can always come back to the current state to study the model performance and draw insights that might help us down the road. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
