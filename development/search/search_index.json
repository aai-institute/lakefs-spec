{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"lakefs-spec: An fsspec backend for lakeFS","text":"<p>Welcome to <code>lakefs-spec</code>, a filesystem-spec backend implementation for the lakeFS data lake. Our primary goal is to streamline versioned data operations in lakeFS, enabling seamless integration with popular data science tools such as Pandas, Polars, and DuckDB directly from Python.</p> <p>Highlights:</p> <ul> <li>High-level abstraction over basic lakeFS repository operations</li> <li>Seamless integration into the <code>fsspec</code> ecosystem</li> <li>Transaction support</li> <li>Zero-config option through config autodiscovery</li> <li>Automatic up-/download management to avoid unnecessary transfers for unchanged files</li> </ul> <p>Quickstart</p><p>Step-by-step installation and first operations</p> <p>Use Cases</p><p>Reasons to use lakefs-spec in your project</p> <p>Tutorials</p><p>In-depth tutorials on using lakefs-spec</p> <p>API Reference</p><p>Full documentation of the Python API</p> <p>User Guide</p><p>Solving specific tasks with lakefs-spec</p> <p>Contributing</p><p>How to contribute to the project</p>"},{"location":"CONTRIBUTING/","title":"Developing on <code>lakefs-spec</code>","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#quickstart","title":"Quickstart","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/aai-institute/lakefs-spec.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment:</p> <pre><code>cd lakefs-spec\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython -m pip install -r requirements.txt -r requirements-dev.txt\npython -m pip install -e . --no-deps\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests against an ephemeral lakeFS instance, you just run <code>pytest</code>:     <pre><code>pytest\n</code></pre></p> <p>To spin up a local lakeFS instance quickly for testing, you can use the Docker Compose file bundled with this repository:</p> <pre><code>docker-compose -f hack/docker-compose.yml up\n</code></pre> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.optional-dependencies</code> table instead.</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.optional-dependencies</code>.</li> </ol> <p>After adding the dependency in either of these sections, run the helper script <code>hack/lock-deps.sh</code> (which in turn uses <code>pip-compile</code>) to pin all dependencies again:</p> <pre><code>python -m pip install --upgrade pip-tools\nhack/lock-deps.sh\n</code></pre> <p>In addition to these manual steps, we also provide <code>pre-commit</code> hooks that automatically lock the dependencies whenever <code>pyproject.toml</code> is changed.</p> <p>Selective package upgrade for existing dependencies are also handled by the helper script above. If you want to update the <code>lakefs-sdk</code> dependency, for example, simply run:</p> <pre><code>hack/lock-deps.sh lakefs-sdk\n</code></pre> <p>\u26a0\ufe0f Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"CONTRIBUTING/#working-on-documentation","title":"Working on Documentation","text":"<p>Improvements or additions to the project's documentation are highly appreciated.</p> <p>The documentation is based on the <code>mkdocs</code> and <code>mkdocs-material</code> projects, see their homepages for in-depth guides on their features and usage. We use the Numpy documentation style for Python docstrings.</p> <p>To build the documentation locally, you need to first install the optional <code>docs</code> dependencies from <code>requirements-docs.txt</code>, e.g., with <code>pip install -r requirements-docs.txt</code>. You can then start a local documentation server with <code>mkdocs serve</code>, or build the documentation into its output folder in <code>public/</code>.</p> <p>In order to maintain documentation for multiple versions of this library, we use the <code>mike</code> tool, which automatically maintains individual documentation builds per version and publishes them to the <code>gh-pages</code> branch.</p> <p>The GitHub CI pipeline automatically invokes <code>mike</code> as part of the release process with the correct version and updates the GitHub pages branch for the project.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome! This quickstart guide will get you up and running with <code>lakefs-spec</code> by showing you how to</p> <ol> <li>Install the <code>lakefs-spec</code> package,</li> <li>spin up a local lakeFS server,</li> <li>create a lakeFS repository for experimentation, and</li> <li>perform basic file system operations in a lakeFS repository using <code>lakefs-spec</code>.</li> </ol> Prerequisites <p>To follow along with this guide, you will need a few prerequisites ready on your machine:</p> <ul> <li><code>lakefs-spec</code> supports Windows, macOS, or Linux</li> <li>Docker, with Docker Compose (Podman should work as well, but is untested)</li> <li>Python 3.9 or later</li> <li>optionally, <code>lakectl</code>, the lakeFS command line tool</li> </ul> <p>Please take a moment to make sure you have these tools available before proceeding with the next steps.</p>"},{"location":"quickstart/#installing-lakefs-spec","title":"Installing <code>lakefs-spec</code>","text":"A note on virtual environments <p>We generally recommend installing the library in a virtual environment to ensure proper isolation, especially when following this quickstart guide.</p> <p>If you are using Poetry, virtual environments can automatically be created by the tool.</p> <p>If you prefer the <code>venv</code> functionality built into Python, see the official docs (tl;dr: <code>python -m venv venv; source venv/bin/activate</code>).</p> <p>To install the package directly from PyPI, run:</p> pippoetry <pre><code>pip install lakefs-spec\n</code></pre> <pre><code>poetry add lakefs-spec\n</code></pre> <p>Or, if you want to try the latest pre-release version directly from GitHub:</p> pippoetry <pre><code>pip install git+https://github.com/aai-institute/lakefs-spec.git\n</code></pre> <pre><code>poetry add git+https://github.com/aai-institute/lakefs-spec.git\n</code></pre>"},{"location":"quickstart/#first-steps","title":"First Steps","text":""},{"location":"quickstart/#spinning-up-a-local-lakefs-instance","title":"Spinning up a local lakeFS instance","text":"<p>Warning</p> <p>This setup is not recommended for production uses, since it does not store the data persistently.</p> <p>Please check out the lakeFS docs for production-ready deployment options.</p> <p>If you don't already have access to a lakeFS server, you can quickly start a local instance using Docker Compose. Before continuing, please make sure Docker is installed and running on your machine.</p> <p>The lakeFS quickstart deployment can be launched directly with a configuration file provided in the <code>lakefs-spec</code> repository:</p> <pre><code>$ curl https://raw.githubusercontent.com/aai-institute/lakefs-spec/main/hack/docker-compose.yml | docker-compose -f - up\n</code></pre> <p>If you do not have <code>curl</code> installed on your machine or would like to examine and/or customize the container configuration, you can also create a <code>docker-compose.yml</code> file locally and use it with <code>docker-compose up</code>:</p> docker-compose.yml<pre><code>version: \"3\"\n\nservices:\n  lakefs:\n    image: treeverse/lakefs:1.0.0\n    ports:\n      - 8000:8000\n    environment:\n      LAKEFS_INSTALLATION_USER_NAME: \"quickstart\"\n      LAKEFS_INSTALLATION_ACCESS_KEY_ID: \"AKIAIOSFOLQUICKSTART\"\n      LAKEFS_INSTALLATION_SECRET_ACCESS_KEY: \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n      LAKEFS_DATABASE_TYPE: \"local\"\n      LAKEFS_AUTH_ENCRYPT_SECRET_KEY: \"THIS_MUST_BE_CHANGED_IN_PRODUCTION\"\n      LAKEFS_BLOCKSTORE_TYPE: \"local\"\n</code></pre> <p>In order to allow <code>lakefs-spec</code> to automatically discover credentials to access this lakeFS instance, create a <code>.lakectl.yaml</code> in your home directory containing the credentials for the quickstart environment (you can also use <code>lakectl config</code> to create this file interactively if you have the <code>lakectl</code> tool installed on your machine):</p> ~/.lakectl.yaml<pre><code>credentials: # (1)!\n  access_key_id: AKIAIOSFOLQUICKSTART\n  secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nserver:\n  endpoint_url: http://127.0.0.1:8000\n</code></pre> <ol> <li>These must match the credentials set in the <code>environment</code> section of the Docker Compose file above</li> </ol> <p>After the container has finished initializing, you can access the web UI of your local lakeFS deployment in your browser. Fill out the setup form, where you can optionally share your email address with the developers of lakeFS to receive updates on their product. Next, you can log into your fresh lakeFS instance with the credentials listed above.</p> <p>Success</p> <p>Your fresh local lakeFS instance is a playground for you to explore lakeFS functionality. </p> <p>In the next step, we will create your first repository on this server.</p>"},{"location":"quickstart/#create-a-lakefs-repository","title":"Create a lakeFS repository","text":"<p>Once you have logged into the web UI of the lakeFS server for the first time, you can create an empty repository on the next page. Click the small Click here link at the bottom of the page to proceed and create a repository named <code>repo</code> (we don't want to add the sample data for this guide):</p> <p></p> Tip: Creating a repository later <p>If you have inadvertently skipped over the quickstart repository creation page, you can always create a new repository on the Repositories tab in the lakeFS web UI (and optionally choose to add the sample data):</p> <p></p> <p>Success</p> <p>You have successfully created a lakeFS repository named <code>repo</code>, ready to be used with <code>lakefs-spec</code>.</p>"},{"location":"quickstart/#using-the-lakefs-fsspec-file-system","title":"Using the lakeFS <code>fsspec</code> file system","text":"<p>We will now use the <code>lakefs-spec</code> file system interface to perform some basic operations on the repository created in the previous step:</p> <ul> <li>Upload a local file to the repository</li> <li>Read data from a file in the repository</li> <li>Make a commit</li> <li>Fetch metadata about repository contents</li> <li>Delete a file from the repository</li> </ul> <p>To get started, create a file called <code>quickstart.py</code> with the following contents:</p> quickstart.py<pre><code>from pathlib import Path\n\nfrom lakefs_spec import LakeFSFileSystem\n\nREPO, BRANCH = \"repo\", \"main\"\n\n# Prepare example local data\nlocal_path = Path(\"demo.txt\")\nlocal_path.write_text(\"Hello, lakeFS!\")\n</code></pre> <p>Tip</p> <p>We will keep adding more code to this file as we progress through the next steps. Feel free to execute the script after each step and observe the effects as noted in the guide.</p> <p>This code snippet prepares a file <code>demo.txt</code> on your machine, ready to be added to the lakeFS repository, so let's do just that:</p> <pre><code>fs = LakeFSFileSystem()  # will auto-discover credentials from ~/.lakectl.yaml\nrepo_path = f\"{REPO}/{BRANCH}/{local_path.name}\"\n\nwith fs.transaction as tx:\n    fs.put(str(local_path), repo_path)\n    tx.commit(REPO, BRANCH, \"Add demo data\")\n</code></pre> <p>If you execute the <code>quickstart.py</code> script at this point, you can already see the committed file in the lakeFS web UI:</p> <p></p> <p>While examining the file contents in the browser is nice, we want to access the committed file programmatically. Add the following lines at the end of your script and observe the output:</p> <pre><code>f = fs.open(repo_path, \"rt\")\nprint(f.readline())  # prints \"Hello, lakeFS!\"\n</code></pre> <p>Note that executing the same code multiple times will only result in a single commit in the repository since the contents of the file on disk and in the repository are identical.</p> <p>In addition to simple read and write operations, the <code>fsspec</code> file system interface also allows us to list the files in a repository folder using <code>ls</code>, and query the metadata of objects in the repository through <code>info</code> (akin to the POSIX <code>stat</code> system call). Let's add the following code to our script and observe the output:</p> <pre><code># Compare the sizes of local file and repo\nfile_info = fs.info(repo_path)\nprint(\n    f\"{local_path.name}: local size: {file_info['size']}, remote size: {local_path.stat().st_size}\"\n)\n\n# Get information about all files in the repo root\nprint(fs.ls(f\"{REPO}/{BRANCH}/\"))\n</code></pre> <p>As the last order of business, let's clean up the repository to its original state by removing the file using the <code>rm</code> operation and creating another commit (also, the local file is deleted, since we don't need it anymore):</p> <pre><code>with fs.transaction as tx:\n    fs.rm(repo_path)\n    tx.commit(REPO, BRANCH, \"Delete demo data\")\nlocal_path.unlink()\n</code></pre> <p>Success</p> <p>You now have all the basic tools available to version data from your Python code using the file system interface provided by <code>lakefs-spec</code>.</p> Full example code quickstart.py<pre><code>from pathlib import Path\n\nfrom lakefs_spec import LakeFSFileSystem\n\nREPO, BRANCH = \"repo\", \"main\"\n\n# Prepare example local data\nlocal_path = Path(\"demo.txt\")\nlocal_path.write_text(\"Hello, lakeFS!\")\n\n# Upload the local file to the repo and commit\nfs = LakeFSFileSystem()  # will auto-discover credentials from ~/.lakectl.yaml\nrepo_path = f\"{REPO}/{BRANCH}/{local_path.name}\"\n\nwith fs.transaction as tx:\n    fs.put(str(local_path), repo_path)\n    tx.commit(REPO, BRANCH, \"Add demo data\")\n\n# Read back the file contents\nf = fs.open(repo_path, \"rt\")\nprint(f.readline())  # prints \"Hello, lakeFS!\"\n\n# Compare the sizes of local file and repo\nfile_info = fs.info(repo_path)\nprint(\n    f\"{local_path.name}: local size: {file_info['size']}, remote size: {local_path.stat().st_size}\"\n)\n\n# Get information about all files in the repo root\nprint(fs.ls(f\"{REPO}/{BRANCH}/\"))\n\n# Delete uploaded file from the repository (and commit)\nwith fs.transaction as tx:\n    fs.rm(repo_path)\n    tx.commit(REPO, BRANCH, \"Delete demo data\")\nlocal_path.unlink()\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Todo</p> <p>These links might refer to pages under development and are subject to change (or being broken)</p> <p>After this walkthrough of the installation and an introduction to basic file system operations using <code>lakefs-spec</code>, you might want to consider more advanced topics:</p> <ul> <li>Use Cases for <code>lakefs-spec</code></li> <li>API Reference</li> <li>TODO: User Guide</li> </ul>"},{"location":"use-cases/","title":"Why should you use <code>lakefs-spec</code>?","text":"<p>There are a few reasons that come to mind:</p>"},{"location":"use-cases/#file-system-operations-client-interactions","title":"File system operations <code>&gt;&gt;</code> client interactions","text":"<p>Before, the only way to interface with a lakeFS instance was to use the Python API client. While it has its merits, and (mostly) works as advertised, it has its issues - the developer experience being the most difficult one, with incomplete typing information, and the woes of reading lots of raw client code, for example when downloading a file to load into a data frame:</p> <pre><code>from lakefs_sdk import Configuration\nfrom lakefs_sdk.client import LakeFSClient\n\n# painful!\nconfiguration = Configuration(host=\"my-host\")\nclient = LakeFSClient(configuration=configuration)\n\nrepository = \"my-repo\"\nref = \"my-ref\"\ndata = \"data.parquet\"\n\nwith open(\"res.parquet\", \"w\") as f:\n    res = client.objects_api.get_object(repository, ref, data)\n    while True:\n        chunk = res.read(2 ** 20)\n        if not chunk:\n            break\n        f.write(chunk)\n</code></pre> <p>When comparing this to <code>lakefs-spec</code>, we see that approaching the problem from a file system point of view makes things much easier:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem(host=\"my-host\")\n\nfs.get(\"my-repo/my-ref/data.parquet\", \"res.parquet\")\n</code></pre> <p>These abstractions make the code needed to interact with files and specific versions of resources much more straightforward.</p>"},{"location":"use-cases/#integration-with-fsspec-powered-python-data-science-libraries","title":"Integration with <code>fsspec</code>-powered Python data science libraries","text":"<p>Some projects, including <code>pandas</code>, <code>polars</code>, and <code>pyspark</code>, use <code>fsspec</code> to abstract I/O into a file-system paradigm. Registering other file systems is as easy as running a hook in <code>setuptools</code> (or <code>poetry</code>, depending on your preference) at build time.</p> <p><code>lakefs-spec</code> itself is registered as the default file system for the <code>lakefs</code> protocol in the official <code>fsspec</code> registry, so all you have to do is to <code>pip install --upgrade lakefs-spec</code> to start using it.</p> <p>With it, reading and writing data in a versioned manner is supported natively, for example in data frame I/O in <code>pandas</code>:</p> <pre><code>import pandas as pd\n\nstorage_options={\n    \"host\": \"localhost:8000\",\n    \"username\": \"username\",\n    \"password\": \"password\",\n}\n\ndf = pd.read_parquet('lakefs://quickstart/main/lakes.parquet', storage_options=storage_options)\n</code></pre> <p>With more advanced methods in <code>lakefs-spec</code> such as config file instantiation and environment variable setting, you can even instantiate the file system without any arguments and rely on automated client configuration and setup.</p>"},{"location":"use-cases/#setting-up-lakefs-automations-and-versioning-workflows-using-transactions","title":"Setting up lakeFS automations and versioning workflows using transactions","text":"<p>As described earlier, dealing with the raw lakeFS API client in interactions with a lakeFS instance can be cumbersome, for reasons like weirdnesses in autogenerated classes and docstrings, API models instead of raw Python types, and unintuitive keyword arguments.</p> <p>Using <code>lakefs-spec</code> you can interact with your lakeFS instance in a file-centric way. Each file operation is mapped to one (or more) client API calls, which are completely abstracted away from the user and hidden behind the standard <code>fsspec</code> APIs, which allow for accurate typing.</p> <p>Since lakeFS is a data versioning tool and not only an object store, you should be able to carry out its versioning operations during file operations. <code>lakefs-spec</code> solves this by defining a <code>Transaction</code> class which works as a context manager and has many data versioning operations available.</p> <p>A concrete example of this is creating lakeFS commits after a successful file upload. Commits provide a snapshot of a lakeFS repository, similarly to version control systems like git.</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nwith fs.transaction as tx:\n    # creates a commit with the message \"Add file my-file.txt\" after the file put.\n    # setting `autocommit=False` defers the actual upload until the transaction has finished.\n    fs.put_file(\"my-file.txt\", \"my-repo/my-branch/my-file.txt\", autocommit=False)\n    tx.commit(\"my-repo\", \"my-branch\", message=\"Add file my-file.txt\")\n</code></pre> <p>The <code>lakefs_spec.Transaction</code> class contains multiple useful versioning helpers like <code>commit</code>, <code>tag</code>, <code>merge</code>, and <code>revert</code>. If you need more extensibility, you can define your own subclass of this transaction class to gain even more control over your versioning workflow.</p>"},{"location":"guides/","title":"User Guide","text":"<p>The <code>lakefs-spec</code> user guide provides documentation for users of the library looking to solve specific tasks. See the Quickstart guide for an introductory tutorial.</p> <ul> <li>How to use the lakeFS file system</li> <li>Passing configuration to the file system</li> <li>Using file system transactions</li> <li>How to use <code>lakefs-spec</code> with third-party data science libraries</li> </ul>"},{"location":"guides/configuration/","title":"Passing configuration to the file system","text":"<p>There are multiple ways to configure the <code>LakeFSFileSystem</code> for use with a deployed lakeFS instance. This guide introduces them in the order of least to most in-Python configuration - the preferred way to use the file system is with as little Python code as possible.</p> <p>Info</p> <p>The configuration methods are introduced in reverse order of precedence - config file arguments have the lowest priority, environment variables have medium priority, and constructor arguments have the highest priority.</p>"},{"location":"guides/configuration/#the-lakectlyaml-configuration-file","title":"The <code>.lakectl.yaml</code> configuration file","text":"<p>The easiest way of configuring the lakeFS file system is with a <code>lakectl</code> YAML configuration file. To address a lakeFS server, the following minimum configuration is required:</p> <pre><code>credentials:\n  access_key_id: &lt;ID&gt;\n  secret_access_key: &lt;KEY&gt;\nserver:\n  endpoint_url: &lt;LAKEFS-HOST&gt;\n</code></pre> <p>For a local instance produced by the quickstart, the following values will work:</p> <pre><code>credentials:\n  access_key_id: AKIAIOSFOLQUICKSTART\n  secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nserver:\n  endpoint_url: http://127.0.0.1:8000\n</code></pre> <p>To work without any more arguments \"out of the box\", the configuration file has to be placed in your home directory with the name <code>.lakectl.yaml</code> (this is where lakeFS expects it). If you set all values correctly, you can instantiate the lakeFS file system without any arguments:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\n# zero config necessary.\nfs = LakeFSFileSystem()\n</code></pre> <p>If you cannot use the default location (<code>$HOME/.lakectl.yaml</code>), you can read a file from any other location by passing the <code>configfile</code> argument:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem(configfile=\"/path/to/my/configfile.yaml\")\n</code></pre>"},{"location":"guides/configuration/#setting-environment-variables","title":"Setting environment variables","text":"<p>It is also possible to specify certain configuration values used for authentication with the lakeFS server with environment variables. For these values, the variable name is exactly the constructor argument name prefaced with <code>LAKEFS_</code>, e.g. the <code>host</code> argument can be set via the <code>LAKEFS_HOST</code> environment variable.</p> <pre><code>import os\nfrom lakefs_spec import LakeFSFileSystem\n\nos.environ[\"LAKEFS_HOST\"] = \"http://my-lakefs.host\"\nos.environ[\"LAKEFS_USERNAME\"] = \"my-username\"\nos.environ[\"LAKEFS_PASSWORD\"] = \"my-password\"\n\n# also zero-config.\nfs = LakeFSFileSystem()\n</code></pre> <p>Info</p> <p>Not all initialization values can be set via environment variables - the <code>proxy</code>, <code>create_branch_ok</code>, <code>source_branch</code>, and <code>storage_options</code> arguments can only be supplied in Python.</p>"},{"location":"guides/configuration/#using-constructor-arguments","title":"Using constructor arguments","text":"<p>The most straightforward way is to simply supply the values to your lakeFS file system in Python:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem(\n    host=\"http://my-lakefs.host\",\n    username=\"my-username\",\n    password=\"my-password\",\n)\n</code></pre> <p>Beware that this method of configuration can leak sensitive information when using fixed, constant values and checking them into version control systems.</p>"},{"location":"guides/configuration/#appendix-mixing-zero-config-methods","title":"Appendix: Mixing zero-config methods","text":"<p>Two of the introduced methods allow for \"zero-config\" (i.e. no arguments given to the constructor) initialization of the file system. However, care must be taken when working with different file systems configured by the same means (for example, file systems configured with separate environment variables).</p> <p>The reason for this is the instance caching mechanism built into <code>fsspec</code>. While this allows for efficient reuse of file systems e.g. by third-party libraries (pandas, DuckDB, ...), it can lead to silent misconfigurations. Consider this example, with an existent <code>.lakectl.yaml</code> file:</p> $HOME/.lakectl.yaml<pre><code>credentials:\n  access_key_id: AKIAIOSFOLQUICKSTART\n  secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nserver:\n  endpoint_url: http://127.0.0.1:8000\n</code></pre> <p>Now, mixing config file and environment variable initializations leads to the wrong result:</p> <pre><code>import os\nfrom lakefs_spec import LakeFSFileSystem\n\n# first file system, initialized from the config file\nconfig_fs = LakeFSFileSystem()\n\nos.environ[\"LAKEFS_HOST\"] = \"http://my-other-lakefs.host\"\nos.environ[\"LAKEFS_USERNAME\"] = \"my-username\"\nos.environ[\"LAKEFS_PASSWORD\"] = \"my-password\"\n\nenvvar_fs = LakeFSFileSystem()\n\nprint(config_fs is envvar_fs) # &lt;- prints True! \n</code></pre> <p>The reason why the above code does not work as desired is that the cached config-file-initialized file system is simply reused on the second assignment. To clear the file system instance cache, you can run the following:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nLakeFSFileSystem.clear_instance_cache()\n</code></pre>"},{"location":"guides/filesystem-usage/","title":"How to use the lakeFS file system","text":"<p>This guide contains instructions and code snippets on how to use the lakeFS file system.</p>"},{"location":"guides/filesystem-usage/#the-lakefs-uri-structure","title":"The lakeFS URI structure","text":"<p>In the following subsections, we frequently make use of lakeFS URIs in the example code. lakeFS URIs identify resources in a lakeFS deployment through a unique path consisting of repository name, lakeFS revision/ref name, and file name relative to the repository root.</p> <p>As an example, a URI like <code>repo/main/file.txt</code> addresses the <code>file.txt</code> file on the <code>main</code> branch in the repository named <code>repo</code>.</p> <p>In some lakeFS file system operations, directories are also allowed as resource names. For example, the URI <code>repo/main/data/</code> (note the trailing slash) refers to the <code>data</code> directory on the <code>main</code> branch in the <code>repo</code> repository.</p>"},{"location":"guides/filesystem-usage/#on-staged-versus-committed-changes","title":"On staged versus committed changes","text":"<p>When uploading, copying, or removing files or directories from a branch, those removal operations will result in staged changes in the repository until a commit is created. <code>lakefs-spec</code> does not create these commits automatically, since it separates file operations from versioning operations rigorously. If you want to conduct versioning operations, like creating commits, between file transfers, the best way to do so is by using filesystem transactions.</p>"},{"location":"guides/filesystem-usage/#how-to-use-lakefs-file-system-apis","title":"How to use lakeFS file system APIs","text":"<p>The following section explains more in-depth how to use the <code>LakeFSFileSystem</code> APIs. This section concerns the explicitly implemented operations. In addition, there are a number of file system APIs inherited from the <code>AbstractFileSystem</code> interface in fsspec.</p> <p>More information on file system usage can be found in the <code>fsspec</code> documentation.</p>"},{"location":"guides/filesystem-usage/#uploading-and-downloading-files","title":"Uploading and downloading files","text":"<p>The arguably most important feature of the file system is file transfers.</p>"},{"location":"guides/filesystem-usage/#file-uploads","title":"File uploads","text":"<p>To upload a file, you can use the <code>fs.put()</code> and <code>fs.put_file()</code> methods.  While <code>fs.put_file()</code> operates on single files only, the <code>fs.put()</code> API can be used for directory uploads.</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# remote path, then local target path.\nfs.put_file(\"file.txt\", \"my-repo/my-ref/file.txt\")\n</code></pre> <p>If you want to upload an entire directory to lakeFS, you can use the <code>fs.put()</code> API together with the <code>recursive=True</code> switch:</p> <pre><code># structure:\n#   dir/a.txt\n#      /b.yaml\n#      /c.csv\n#      /...\n\nfs.put(\"dir\", \"my-repo/my-ref/dir\", recursive=True)\n</code></pre> <p>Info</p> <p>The above method of file uploading results in two transfers: Once from the client to the lakeFS server, and once from the lakeFS server to the object storage. This can impact performance if the uploaded files are very large. To avoid this performance issue, you can also decide to write the file directly to the underlying object storage:</p> <pre><code>fs = LakeFSFileSystem()\n\nfs.put_file(\"my-repo/my-ref/file.txt\", \"file.txt\", use_blockstore=True)\n</code></pre> <p>Direct lakeFS blockstore uploads require the installation of the corresponding <code>fsspec</code> file system implementation through <code>pip</code>. For an S3-based lakeFS deployment, install the <code>s3fs</code> package. For Google Cloud Storage (GCS), install the <code>gcsfs</code> package. For Azure blob storage, install the <code>adlfs</code> package.</p>"},{"location":"guides/filesystem-usage/#file-downloads","title":"File downloads","text":"<p>To download a file, you can use the <code>fs.get()</code> or <code>fs.get_file()</code> methods. While <code>fs.get_file()</code> downloads single files only, the <code>fs.get()</code> API can be used for recursive directory downloads.</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# remote path, then local target path.\nfs.get_file(\"my-repo/my-ref/file.txt\", \"file.txt\")\n</code></pre> <p>In the case of a directory in lakeFS, use the <code>fs.get()</code> API together with the <code>recursive=True</code> switch:</p> <pre><code># structure:\n#   dir/a.txt\n#      /b.yaml\n#      /c.csv\n#      /...\n\n# downloads the entire `dir` directory (and subdirectories) into the current directory.\nfs.get(\"my-repo/my-ref/dir\", \"dir\", recursive=True)\n</code></pre>"},{"location":"guides/filesystem-usage/#checking-the-existence-of-lakefs-objects","title":"Checking the existence of lakeFS objects","text":"<p>To check the existence of a file in a given revision of a repository, you can use the <code>fs.exists()</code> API:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nmy_file_exists = fs.exists(\"my-repo/my-ref/my-file.txt\")\n</code></pre> <p>This function returns <code>True</code> if the file exists on that revision, and <code>False</code> if it does not. Errors (e.g. permission errors) will be raised, since in that case, object existence cannot be decided.</p> <p>Warning</p> <p><code>fs.exists()</code> only works on file objects, and will return <code>False</code> if called on directories.</p>"},{"location":"guides/filesystem-usage/#obtaining-info-on-stored-objects","title":"Obtaining info on stored objects","text":"<p>To query the metadata of a single object in a lakeFS repository, use the <code>fs.info()</code> API:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nmy_file_info = fs.info(\"my-repo/my-ref/my-file.txt\")\n</code></pre> <p>The resulting <code>my_file_info</code> object is a dictionary containing useful information such as storage location of the file, creation timestamp, and size (in bytes).</p> <p>You can also call <code>fs.info()</code> on directories:</p> <pre><code>dir_info = fs.info(\"my-repo/my-ref/dir/\")\n</code></pre> <p>In this case, the resulting <code>dir_info</code> object only contains the directory name, and the cumulated size of the files it contains.</p>"},{"location":"guides/filesystem-usage/#listing-directories-in-lakefs","title":"Listing directories in lakeFS","text":"<p>To list the files in a directory in lakeFS, use the <code>fs.ls()</code> method:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nmy_dir_listing = fs.ls(\"my-repo/my-ref/my-dir/\")\n</code></pre> <p>This returns a list of Python dictionaries containing information on the objects contained in the requested directory. The returned objects have the same fields set as those returned by a normal <code>fs.info()</code> call on a file object.</p>"},{"location":"guides/filesystem-usage/#deleting-objects-from-a-lakefs-branch","title":"Deleting objects from a lakeFS branch","text":"<p>To delete objects from a lakeFS branch, use the <code>fs.rm_file()</code> or <code>fs.rm()</code> APIs. As before, while the former works only for single files, the latter can be used to remove entire directories with the <code>recursive=True</code> option.</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nfs.rm_file(\"my-repo/my-branch/my-file.txt\")\n\n# removes the entire `my-dir` directory.\nfs.rm(\"my-repo/my-branch/my-dir/\", recursive=True)\n</code></pre>"},{"location":"guides/filesystem-usage/#copying-files-in-a-repository","title":"Copying files in a repository","text":"<p>To copy files on a branch or from one branch to another, use the <code>fs.cp_file()</code> or <code>fs.copy()</code> methods:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# copies a single file on the same branch to a new location.\nfs.cp_file(\"my-repo/branch-a/file.txt\", \"my-repo/branch-a/file.txt.bak\")\n\n# copies a single file from branch A to branch B.\nfs.cp_file(\"my-repo/branch-a/file.txt\", \"my-repo/branch-b/file.txt\")\n\n# copies the entire `my-dir` directory from branch A to branch B (which must exist).\nfs.copy(\"my-repo/branch-a/my-dir/\", \"my-repo/branch-b/my-dir/\", recursive=True)\n</code></pre> <p>Info</p> <p>Files and directories can only be copied between branches in the same repository, not between different repositories.</p> <p>Trying to copy to a non-existent branch will not create the branch.</p>"},{"location":"guides/integrations/","title":"How to use <code>lakefs-spec</code> with third-party data science libraries","text":"<p><code>lakefs-spec</code> is built on top of the <code>fsspec</code> library, which allows third-party libraries to make use of its file system abstraction to offer high-level features. The <code>fsspec</code> documentation lists examples of its users, mostly data science libraries.</p> <p>This user guide page adds more detail on how <code>lakefs-spec</code> can be used with four prominent data science libraries.</p> <p>Code Examples</p> <p>The code examples assume access to an existing lakeFS server with a <code>quickstart</code> containing the sample data set repository set up.</p> <p>Please see the Quickstart guide if you need guidance in getting started.</p> <p>The relevant lines for the <code>lakefs-spec</code> integration in these examples are highlighted.</p>"},{"location":"guides/integrations/#pandas","title":"Pandas","text":"<p>Pandas can read and write data from remote locations, and uses <code>fsspec</code> for all URLs that are not local or HTTP(S).</p> <p>This means that (almost) all <code>pd.read_*</code> and <code>pd.DataFrame.to_*</code> operations can benefit from the lakeFS integration offered by our library without any additional configuration. See the Pandas documentation on reading/writing remote files for additional details.</p> <p>The following code snippet illustrates how to read and write Pandas data frames in various formats from/to a lakeFS repository in the context of a transaction:</p> <pre><code>import pandas as pd\n\nfrom lakefs_spec.transaction import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nwith fs.transaction as tx:\n    tx.create_branch(\"quickstart\", \"german-lakes\", \"main\")\n\n    lakes = pd.read_parquet(\"lakefs://quickstart/main/lakes.parquet\")\n    german_lakes = lakes.query('Country == \"Germany\"')\n    german_lakes.to_csv(\"lakefs://quickstart/german-lakes/german_lakes.csv\")\n\n    tx.commit(\"quickstart\", \"german-lakes\", \"Add German lakes\")\n</code></pre>"},{"location":"guides/integrations/#duckdb","title":"DuckDB","text":"<p>The DuckDB in-memory database management system includes support for <code>fsspec</code> file systems as part of its Python API (see the official documentation on using fsspec filesystems for details). This allows DuckDB to transparently query and store data located in lakeFS repositories through <code>lakefs-spec</code>.</p> <p>Similar to the example above, the following code snippet illustrates how to read and write data from/to a lakeFS repository in the context of a transaction through the DuckDB Python API:</p> <pre><code>import duckdb\n\nfrom lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\nduckdb.register_filesystem(fs)  # (1)! \n\nwith fs.transaction as tx:\n    tx.create_branch(\"quickstart\", \"german-lakes\", \"main\")\n\n    lakes = duckdb.read_parquet(\"lakefs://quickstart/main/lakes.parquet\")\n    german_lakes = duckdb.sql(\"SELECT * FROM lakes where Country='Germany'\")\n    german_lakes.to_csv(\"lakefs://quickstart/german-lakes/german_lakes.csv\")\n\n    tx.commit(\"quickstart\", \"german-lakes\", \"Add German lakes\")\n</code></pre> <ol> <li>Makes the <code>lakefs-spec</code> file system known to DuckDB (<code>duckdb.register_filesystem(fsspec.filesystem(\"lakefs\"))</code> can also be used to avoid the direct import of <code>LakeFSFileSystem</code>)</li> </ol>"},{"location":"guides/integrations/#polars","title":"Polars","text":"<p>Warning</p> <p>There is an ongoing discussion in the Polars development team whether to remove support for <code>fsspec</code> file systems, with no clear outcome as of the time this page was written. Please refer to the discussion on the relevant GitHub issue in case you encounter any problems.</p> <p>The Python API wrapper for the Rust-based Polars DataFrame library can access remote storage through <code>fsspec</code>, similar to Pandas (see the official documentation on cloud storage).</p> <p>Again, the following code example demonstrates how to read a Parquet file and save a modified version back in CSV format to a lakeFS repository from Polars in the context of a  transaction:</p> <pre><code>import polars as pl\n\nfrom lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nwith fs.transaction as tx:\n    tx.create_branch(\"quickstart\", \"german-lakes\", \"main\")\n\n    lakes = pl.read_parquet(\"lakefs://quickstart/main/lakes.parquet\")\n    german_lakes = lakes.filter(pl.col(\"Country\") == \"Germany\")\n\n    with fs.open(\"lakefs://quickstart/german-lakes/german_lakes.csv\", \"wb\") as f: # (1)!\n        german_lakes.write_csv(f)\n\n    tx.commit(\"quickstart\", \"german-lakes\", \"Add German lakes\")\n</code></pre> <ol> <li>Polars does not support directly writing to remote storage through the <code>pl.DataFrame.write_*</code> API (see docs)</li> </ol>"},{"location":"guides/integrations/#pyarrow","title":"PyArrow","text":"<p>Apache Arrow and its Python API, PyArrow, can also use <code>fsspec</code> file systems to perform I/O operations on data objects. The documentation has additional details on using fsspec-compatible file systems with Arrow.</p> <p>PyArrow <code>read_*</code> and <code>write_*</code> functions take an explicit <code>filesystem</code> parameter, which accepts any <code>fsspec</code> file system, such as the <code>LakeFSFileSystem</code> provided by this library. </p> <p>The following example code illustrates the use of <code>lakefs-spec</code> with PyArrow, reading a Parquet file and writing it back to a lakeFS repository as a partitioned CSV dataset in the context of a transaction:</p> <pre><code>import pyarrow as pa\nimport pyarrow.dataset as ds\nimport pyarrow.parquet as pq\n\nfrom lakefs_spec.spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nwith fs.transaction as tx:\n    tx.create_branch(\"quickstart\", \"partitioned-data\", \"main\")\n\n    lakes_table = pq.read_table(\"quickstart/main/lakes.parquet\", filesystem=fs)\n\n    ds.write_dataset(\n        lakes_table,\n        \"quickstart/partitioned-data/lakes\",\n        filesystem=fs,\n        format=\"csv\",\n        partitioning=ds.partitioning(pa.schema([lakes_table.schema.field(\"Country\")])),\n    )\n\n    tx.commit(\"quickstart\", \"partitioned-data\", \"Add German lakes\")\n</code></pre>"},{"location":"guides/transactions/","title":"Using transactions on the lakeFS file system","text":"<p>A transaction defers file transfers and versioning operations to a queue, which is unwound sequentially on completion. Transactions are thread-safe and atomic, meaning that a single failure during any transaction function causes the entire transaction to abort.</p> <p>TODO: Add content</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>lakefs_spec<ul> <li>client_helpers</li> <li>config</li> <li>errors</li> <li>spec</li> <li>transaction</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/lakefs_spec/","title":"lakefs_spec","text":"<p>lakefs-spec is an fsspec file system integration for the lakeFS data lake.</p>"},{"location":"reference/lakefs_spec/client_helpers/","title":"client_helpers","text":""},{"location":"reference/lakefs_spec/client_helpers/#lakefs_spec.client_helpers.create_branch","title":"create_branch","text":"<pre><code>create_branch(client, repository, name, source_branch, exist_ok=True)\n</code></pre> <p>Create a branch in a lakeFS repository.</p> PARAMETER  DESCRIPTION <code>client</code> <p>The lakeFS client configured for (and authenticated with) the target instance.</p> <p> TYPE: <code>LakeFSClient</code> </p> <code>repository</code> <p>Repository name.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of the newly created (or existing) branch.</p> <p> TYPE: <code>str</code> </p> <code>source_branch</code> <p>Name of the source branch the new branch is created from.</p> <p> TYPE: <code>str</code> </p> <code>exist_ok</code> <p>Ignore errors if the branch already exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>The requested branch name.</code> Source code in <code>src/lakefs_spec/client_helpers.py</code> <pre><code>def create_branch(\n    client: LakeFSClient, repository: str, name: str, source_branch: str, exist_ok: bool = True\n) -&gt; str:\n    \"\"\"\n    Create a branch in a lakeFS repository.\n\n    Parameters\n    ----------\n    client: LakeFSClient\n        The lakeFS client configured for (and authenticated with) the target instance.\n    repository: str\n        Repository name.\n    name: str\n        Name of the newly created (or existing) branch.\n    source_branch: str\n        Name of the source branch the new branch is created from.\n    exist_ok: bool\n        Ignore errors if the branch already exists.\n\n    Returns\n    -------\n    The requested branch name.\n    \"\"\"\n\n    try:\n        new_branch = BranchCreation(name=name, source=source_branch)\n        # client.branches_api.create_branch throws ApiException if branch exists\n        client.branches_api.create_branch(repository=repository, branch_creation=new_branch)\n        logger.debug(f\"Created new branch {name!r} from branch {source_branch!r}.\")\n        return name\n    except ApiException as e:\n        if e.status == 409 and exist_ok:\n            return name\n        raise e\n</code></pre>"},{"location":"reference/lakefs_spec/client_helpers/#lakefs_spec.client_helpers.create_repository","title":"create_repository","text":"<pre><code>create_repository(client, name, storage_namespace, exist_ok=True)\n</code></pre> <p>Create a repository with the given name and storage namespace.</p> <p>Important: Due to cleanup issues in the lakeFS backend, creating a repository again after prior deletion under the same name and storage namespace will almost certainly not work.</p> <p>The <code>exist_ok</code> flag only asserts idempotence in case the repository is not deleted in between <code>create_repository</code> calls.</p> Source code in <code>src/lakefs_spec/client_helpers.py</code> <pre><code>def create_repository(\n    client: LakeFSClient, name: str, storage_namespace: str, exist_ok: bool = True\n) -&gt; Repository:\n    \"\"\"\n    Create a repository with the given name and storage namespace.\n\n    Important: Due to cleanup issues in the lakeFS backend, creating a repository again after prior\n    deletion under the same name and storage namespace will almost certainly not work.\n\n    The `exist_ok` flag only asserts idempotence in case the repository is not deleted in between\n    `create_repository` calls.\n    \"\"\"\n    try:\n        repository_creation = RepositoryCreation(name=name, storage_namespace=storage_namespace)\n        return client.repositories_api.create_repository(repository_creation=repository_creation)\n    except ApiException as e:\n        if e.status == 400 and \"namespace already in use\" in e.body and exist_ok:\n            return client.repositories_api.get_repository(name)\n        raise e\n</code></pre>"},{"location":"reference/lakefs_spec/client_helpers/#lakefs_spec.client_helpers.revert","title":"revert","text":"<pre><code>revert(client, repository, branch, parent_number=1)\n</code></pre> <p>Reverts the commit on the specified branch to the parent specified by parent_number.</p> PARAMETER  DESCRIPTION <code>client</code> <p>The client to interact with.</p> <p> TYPE: <code>LakeFSClient</code> </p> <code>repository</code> <p>Repository in which the specified branch is located.</p> <p> TYPE: <code>str</code> </p> <code>branch</code> <p>Branch on which the commit should be reverted.</p> <p> TYPE: <code>str</code> </p> <code>parent_number</code> <p>If there are multiple parents to a commit, specify to which parent the commit should be reverted. Index starts at 1. Defaults to 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/lakefs_spec/client_helpers.py</code> <pre><code>def revert(client: LakeFSClient, repository: str, branch: str, parent_number: int = 1) -&gt; None:\n    \"\"\"Reverts the commit on the specified branch to the parent specified by parent_number.\n\n    Parameters\n    ----------\n    client: LakeFSClient\n        The client to interact with.\n    repository: str\n        Repository in which the specified branch is located.\n    branch: str\n        Branch on which the commit should be reverted.\n    parent_number: int, optional\n        If there are multiple parents to a commit, specify to which parent the commit should be reverted. Index starts at 1. Defaults to 1.\n    \"\"\"\n    revert_creation = RevertCreation(ref=branch, parent_number=parent_number)\n    client.branches_api.revert_branch(\n        repository=repository, branch=branch, revert_creation=revert_creation\n    )\n</code></pre>"},{"location":"reference/lakefs_spec/config/","title":"config","text":""},{"location":"reference/lakefs_spec/errors/","title":"errors","text":""},{"location":"reference/lakefs_spec/errors/#lakefs_spec.errors.translate_lakefs_error","title":"translate_lakefs_error","text":"<pre><code>translate_lakefs_error(error, message=None, set_cause=True, *args)\n</code></pre> <p>Convert a lakeFS client ApiException into a Python exception.</p> PARAMETER  DESCRIPTION <code>error</code> <p>The exception returned by the lakeFS API.</p> <p> TYPE: <code>ApiException</code> </p> <code>message</code> <p>An error message to use for the returned exception. If not given, the error message returned by the lakeFS server is used instead.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>set_cause</code> <p>Whether to set the <code>__cause__</code> attribute to the previous exception if the exception is translated.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>*args</code> <p>Additional arguments to pass to the exception constructor, after the error message. Useful for passing the filename arguments to <code>IOError</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>OSError</code> <p>An instantiated exception ready to be thrown. If the error code isn't recognized, an <code>IOError</code> with the original error message is returned.</p> Source code in <code>src/lakefs_spec/errors.py</code> <pre><code>def translate_lakefs_error(\n    error: ApiException,\n    message: str | None = None,\n    set_cause: bool = True,\n    *args: Any,\n) -&gt; OSError:\n    \"\"\"Convert a lakeFS client ApiException into a Python exception.\n\n    Parameters\n    ----------\n\n    error : lakefs_client.ApiException\n        The exception returned by the lakeFS API.\n    message : str\n        An error message to use for the returned exception. If not given, the\n        error message returned by the lakeFS server is used instead.\n    set_cause : bool\n        Whether to set the ``__cause__`` attribute to the previous exception if the\n        exception is translated.\n    *args:\n        Additional arguments to pass to the exception constructor, after the\n        error message. Useful for passing the filename arguments to ``IOError``.\n\n    Returns\n    -------\n    OSError\n        An instantiated exception ready to be thrown. If the error code isn't\n        recognized, an ``IOError`` with the original error message is returned.\n    \"\"\"\n    status, reason, body = error.status, error.reason, error.body\n\n    emsg = f\"HTTP{status} ({reason})\"\n    try:\n        lakefs_msg = json.loads(body)[\"message\"]\n        emsg += f\": {lakefs_msg}\"\n    except json.JSONDecodeError:\n        pass\n\n    constructor = HTTP_CODE_TO_ERROR.get(status, functools.partial(IOError, errno.EIO))\n    custom_exc = constructor(message or emsg, *args)\n    if set_cause:\n        custom_exc.__cause__ = error\n    return custom_exc\n</code></pre>"},{"location":"reference/lakefs_spec/spec/","title":"spec","text":""},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFileSystem","title":"LakeFSFileSystem","text":"<p>             Bases: <code>AbstractFileSystem</code></p> <p>lakeFS file system implementation.</p> <p>The client is immutable in this implementation, so different users need different file systems.</p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>class LakeFSFileSystem(AbstractFileSystem):\n    \"\"\"\n    lakeFS file system implementation.\n\n    The client is immutable in this implementation, so different users need different\n    file systems.\n    \"\"\"\n\n    protocol = \"lakefs\"\n\n    def __init__(\n        self,\n        host: str | None = None,\n        username: str | None = None,\n        password: str | None = None,\n        api_key: str | None = None,\n        api_key_prefix: str | None = None,\n        access_token: str | None = None,\n        verify_ssl: bool = True,\n        ssl_ca_cert: str | None = None,\n        proxy: str | None = None,\n        configfile: str = \"~/.lakectl.yaml\",\n        create_branch_ok: bool = True,\n        source_branch: str = \"main\",\n        **storage_options: Any,\n    ):\n        \"\"\"\n        The LakeFS file system constructor.\n\n        Parameters\n        ----------\n        host: str or None\n            The address of your lakeFS instance.\n        username: str or None\n            The access key name to use in case of access key authentication.\n        password: str or None\n            The access key secret to use in case of access key authentication.\n        api_key: str or None\n            The API key to use in case of authentication with an API key.\n        api_key_prefix: str or None\n            A string prefix to use for the API key in authentication.\n        access_token: str or None\n            An access token to use in case of access token authentication.\n        verify_ssl: bool\n            Whether to verify SSL certificates in API interactions. Do not disable in production.\n        ssl_ca_cert: str or None\n            A custom certificate PEM file to use to verify the peer in SSL connections.\n        proxy: str or None\n            Proxy address to use when connecting to a lakeFS server.\n        create_branch_ok: bool\n            Whether to create branches implicitly when not-existing branches are referenced on file uploads.\n        source_branch: str\n            Source branch set as origin when a new branch is implicitly created.\n        storage_options: Any\n            Configuration options to pass to the file system's directory cache.\n        \"\"\"\n        super().__init__(**storage_options)\n\n        if (p := Path(configfile).expanduser()).exists():\n            lakectl_config = LakectlConfig.read(p)\n        else:\n            # empty config.\n            lakectl_config = LakectlConfig()\n\n        configuration = Configuration(\n            host=host or os.getenv(\"LAKEFS_HOST\") or lakectl_config.host,\n            api_key=api_key or os.getenv(\"LAKEFS_API_KEY\"),\n            api_key_prefix=api_key_prefix or os.getenv(\"LAKEFS_API_KEY_PREFIX\"),\n            access_token=access_token or os.getenv(\"LAKEFS_ACCESS_TOKEN\"),\n            username=username or os.getenv(\"LAKEFS_USERNAME\") or lakectl_config.username,\n            password=password or os.getenv(\"LAKEFS_PASSWORD\") or lakectl_config.password,\n            ssl_ca_cert=ssl_ca_cert or os.getenv(\"LAKEFS_SSL_CA_CERT\"),\n        )\n        # proxy address, not part of the constructor\n        configuration.proxy = proxy\n        # whether to verify SSL certs, not part of the constructor\n        configuration.verify_ssl = verify_ssl\n\n        self.client = LakeFSClient(configuration=configuration)\n        self.create_branch_ok = create_branch_ok\n        self.source_branch = source_branch\n\n    @classmethod\n    @overload\n    def _strip_protocol(cls, path: str | os.PathLike[str] | Path) -&gt; str:\n        ...\n\n    @classmethod\n    @overload\n    def _strip_protocol(cls, path: list[str | os.PathLike[str] | Path]) -&gt; list[str]:\n        ...\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        \"\"\"Copied verbatim from the base class, save for the slash rstrip.\"\"\"\n        if isinstance(path, list):\n            return [cls._strip_protocol(p) for p in path]\n        spath = super()._strip_protocol(path)\n        if stringify_path(path).endswith(\"/\"):\n            return spath + \"/\"\n        return spath\n\n    @property\n    def transaction(self):\n        \"\"\"A context within which files are committed together upon exit\n\n        Requires the file class to implement `.commit()` and `.discard()`\n        for the normal and exception cases.\n        \"\"\"\n        self._transaction: LakeFSTransaction | None\n        if self._transaction is None:\n            self._transaction = LakeFSTransaction(self)\n        return self._transaction\n\n    def start_transaction(self):\n        \"\"\"Begin write transaction for deferring files, non-context version\"\"\"\n        self._intrans = True\n        self._transaction = LakeFSTransaction(self)\n        return self.transaction\n\n    @contextmanager\n    def wrapped_api_call(self, message: str | None = None, set_cause: bool = True) -&gt; EmptyYield:\n        try:\n            yield\n        except ApiException as e:\n            raise translate_lakefs_error(e, message=message, set_cause=set_cause)\n\n    @contextmanager\n    def scope(\n        self,\n        create_branch_ok: bool | None = None,\n        source_branch: str | None = None,\n    ) -&gt; EmptyYield:\n        \"\"\"\n        A context manager yielding scope in which the lakeFS file system behavior\n        is changed from defaults.\n        \"\"\"\n        curr_create_branch_ok, curr_source_branch = (\n            self.create_branch_ok,\n            self.source_branch,\n        )\n        try:\n            if create_branch_ok is not None:\n                self.create_branch_ok = create_branch_ok\n            if source_branch is not None:\n                self.source_branch = source_branch\n            yield\n        finally:\n            self.create_branch_ok = curr_create_branch_ok\n            self.source_branch = curr_source_branch\n\n    def checksum(self, path: str) -&gt; str | None:\n        try:\n            return self.info(path).get(\"checksum\")\n        except FileNotFoundError:\n            return None\n\n    def exists(self, path: str, **kwargs: Any) -&gt; bool:\n        repository, ref, resource = parse(path)\n\n        try:\n            self.client.objects_api.head_object(repository, ref, resource, **kwargs)\n            return True\n        except NotFoundException:\n            return False\n        except ApiException as e:\n            # in case of an error other than \"not found\", existence cannot be\n            # decided, so raise the translated error.\n            raise translate_lakefs_error(e)\n\n    def cp_file(self, path1: str, path2: str, **kwargs: Any) -&gt; None:\n        if path1 == path2:\n            return\n\n        orig_repo, orig_ref, orig_path = parse(path1)\n        dest_repo, dest_ref, dest_path = parse(path2)\n\n        if orig_repo != dest_repo:\n            raise ValueError(\n                \"can only copy objects within a repository, but got source \"\n                f\"repository {orig_repo!r} and destination repository {dest_repo!r}\"\n            )\n\n        with self.wrapped_api_call():\n            object_copy_creation = ObjectCopyCreation(src_path=orig_path, src_ref=orig_ref)\n            self.client.objects_api.copy_object(\n                repository=dest_repo,\n                branch=dest_ref,\n                dest_path=dest_path,\n                object_copy_creation=object_copy_creation,\n                **kwargs,\n            )\n\n    def get_file(\n        self,\n        rpath: str,\n        lpath: str,\n        callback: Callback = _DEFAULT_CALLBACK,\n        outfile: Any = None,\n        precheck: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        lp = Path(lpath)\n        if precheck and lp.exists() and lp.is_file():\n            local_checksum = md5_checksum(lpath, blocksize=self.blocksize)\n            remote_checksum = self.info(rpath).get(\"checksum\")\n            if local_checksum == remote_checksum:\n                logger.info(\n                    f\"Skipping download of resource {rpath!r} to local path {lpath!r}: \"\n                    f\"Resource {lpath!r} exists and checksums match.\"\n                )\n                return\n\n        super().get_file(rpath=rpath, lpath=lpath, callback=callback, outfile=outfile, **kwargs)\n\n    def info(self, path: str, **kwargs: Any) -&gt; dict[str, Any]:\n        path = self._strip_protocol(path)\n\n        repository, ref, resource = parse(path)\n        # first, try with `stat_object` in case of a file.\n        # the condition below checks edge cases of resources that cannot be files.\n        if resource and not resource.endswith(\"/\"):\n            try:\n                # the set of keyword arguments allowed in `list_objects` is a\n                # superset of the keyword arguments for `stat_object`.\n                # Ensure that only admissible keyword arguments are actually\n                # passed to `stat_object`.\n                stat_keywords = [\"presign\", \"user_metadata\"]\n                stat_kwargs = {k: v for k, v in kwargs.items() if k in stat_keywords}\n\n                res = self.client.objects_api.stat_object(\n                    repository=repository, ref=ref, path=resource, **stat_kwargs\n                )\n                return {\n                    \"checksum\": res.checksum,\n                    \"content-type\": res.content_type,\n                    \"mtime\": res.mtime,\n                    \"name\": f\"{repository}/{ref}/{res.path}\",\n                    \"size\": res.size_bytes,\n                    \"type\": \"file\",\n                }\n            except NotFoundException:\n                # fall through, retry with `ls` if it's a directory.\n                pass\n            except ApiException as e:\n                raise translate_lakefs_error(e)\n\n        out = self.ls(path, detail=True, **kwargs)\n        if not out:\n            raise FileNotFoundError(path)\n\n        return {\n            \"name\": path.rstrip(\"/\"),\n            \"size\": sum(o.get(\"size\", 0) for o in out),\n            \"type\": \"directory\",\n        }\n\n    def ls(self, path: str, detail: bool = True, **kwargs: Any) -&gt; list:\n        path = self._strip_protocol(path)\n        repository, ref, prefix = parse(path)\n\n        # Try lookup in dircache unless explicitly disabled by `refresh=True` kwarg\n        use_dircache = True\n        if \"refresh\" in kwargs:\n            use_dircache = not kwargs[\"refresh\"]\n            del kwargs[\"refresh\"]  # cannot be forwarded to the API\n\n        if use_dircache:\n            cache_entry: list[Any] | None = None\n            try:\n                cache_entry = self._ls_from_cache(path)\n            except FileNotFoundError:\n                # we patch files missing from an ls call in the cache entry below,\n                # so this should not be an error.\n                pass\n\n            if cache_entry is not None:\n                if not detail:\n                    return [e[\"name\"] for e in cache_entry]\n                return cache_entry\n\n        has_more, after = True, \"\"\n        # stat infos are either the path only (`detail=False`) or a dict full of metadata\n        info: list[Any] = []\n\n        with self.wrapped_api_call():\n            while has_more:\n                res: ObjectStatsList = self.client.objects_api.list_objects(\n                    repository, ref, after=after, prefix=prefix, **kwargs\n                )\n                has_more, after = res.pagination.has_more, res.pagination.next_offset\n                for obj in res.results:\n                    info.append(\n                        {\n                            \"checksum\": obj.checksum,\n                            \"content-type\": obj.content_type,\n                            \"mtime\": obj.mtime,\n                            \"name\": f\"{repository}/{ref}/{obj.path}\",\n                            \"size\": obj.size_bytes,\n                            \"type\": \"file\",\n                        }\n                    )\n\n        # cache the info if not empty.\n        if info:\n            # assumes that the returned info is name-sorted.\n            pp = self._parent(info[0][\"name\"])\n            if pp in self.dircache:\n                # ls info has files not in cache, so we update them in the cache entry.\n                cache_entry = self.dircache[pp]\n                # extend the entry by the new ls results\n                cache_entry.extend(info)\n                self.dircache[pp] = sorted(cache_entry, key=operator.itemgetter(\"name\"))\n            else:\n                self.dircache[pp] = info\n\n        if not detail:\n            info = [o[\"name\"] for o in info]\n\n        return info\n\n    def _open(\n        self,\n        path: str,\n        mode: Literal[\"rb\", \"wb\"] = \"rb\",\n        block_size: int | None = None,\n        autocommit: bool = True,\n        cache_options: dict[str, str] | None = None,\n        **kwargs: Any,\n    ) -&gt; LakeFSFile:\n        if mode not in {\"rb\", \"wb\"}:\n            raise NotImplementedError(f\"unsupported mode {mode!r}\")\n\n        return LakeFSFile(\n            self,\n            path=path,\n            mode=mode,\n            block_size=block_size or self.blocksize,\n            autocommit=autocommit,\n            cache_options=cache_options,\n            **kwargs,\n        )\n\n    def put_file_to_blockstore(\n        self,\n        lpath: str,\n        rpath: str,\n        callback: Callback = _DEFAULT_CALLBACK,\n        presign: bool = False,\n        storage_options: dict[str, Any] | None = None,\n    ) -&gt; None:\n        repository, branch, resource = parse(rpath)\n\n        staging_location = self.client.staging_api.get_physical_address(\n            repository, branch, resource, presign=presign\n        )\n\n        if presign:\n            remote_url = staging_location.presigned_url\n            content_type, _ = mimetypes.guess_type(lpath)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n            with open(lpath, \"rb\") as f:\n                headers = {\n                    \"Content-Type\": content_type,\n                }\n                request = urllib.request.Request(\n                    url=remote_url, data=f, headers=headers, method=\"PUT\"\n                )\n                try:\n                    if not remote_url.lower().startswith(\"http\"):\n                        raise ValueError(\"Wrong protocol for remote connection\")\n                    else:\n                        logger.info(f\"Begin upload of {lpath}\")\n                        with urllib.request.urlopen(request):  # nosec [B310:blacklist] # We catch faulty protocols above.\n                            logger.info(f\"Successfully uploaded {lpath}\")\n                except urllib.error.HTTPError as e:\n                    urllib_http_error_as_lakefs_api_exception = ApiException(\n                        status=e.code, reason=e.reason\n                    )\n                    raise translate_lakefs_error(error=urllib_http_error_as_lakefs_api_exception)\n        else:\n            blockstore_type = self.client.config_api.get_config().storage_config.blockstore_type\n            # lakeFS blockstore name is \"azure\", but Azure's fsspec registry entry is \"az\".\n            if blockstore_type == \"azure\":\n                blockstore_type = \"az\"\n\n            if blockstore_type not in [\"s3\", \"gs\", \"az\"]:\n                raise ValueError(\n                    f\"Blockstore writes are not implemented for blockstore type {blockstore_type!r}\"\n                )\n\n            remote_url = staging_location.physical_address\n            remote = filesystem(blockstore_type, **(storage_options or {}))\n            remote.put_file(lpath, remote_url, callback=callback)\n\n        staging_metadata = StagingMetadata(\n            staging=staging_location,\n            checksum=md5_checksum(lpath, blocksize=self.blocksize),\n            size_bytes=os.path.getsize(lpath),\n        )\n        self.client.staging_api.link_physical_address(\n            repository, branch, resource, staging_metadata\n        )\n\n    def put_file(\n        self,\n        lpath: str,\n        rpath: str,\n        callback: Callback = _DEFAULT_CALLBACK,\n        precheck: bool = True,\n        use_blockstore: bool = False,\n        presign: bool = False,\n        storage_options: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        if precheck and Path(lpath).is_file():\n            remote_checksum = self.checksum(rpath)\n            local_checksum = md5_checksum(lpath, blocksize=self.blocksize)\n            if local_checksum == remote_checksum:\n                logger.info(\n                    f\"Skipping upload of resource {lpath!r} to remote path {rpath!r}: \"\n                    f\"Resource {rpath!r} exists and checksums match.\"\n                )\n                return\n\n        if use_blockstore:\n            self.put_file_to_blockstore(\n                lpath,\n                rpath,\n                presign=presign,\n                callback=callback,\n                storage_options=storage_options,\n            )\n        else:\n            super().put_file(lpath=lpath, rpath=rpath, callback=callback, **kwargs)\n\n    def rm_file(self, path: str) -&gt; None:\n        repository, branch, resource = parse(path)\n\n        with self.wrapped_api_call():\n            self.client.objects_api.delete_object(\n                repository=repository, branch=branch, path=resource\n            )\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFileSystem.transaction","title":"transaction  <code>property</code>","text":"<pre><code>transaction\n</code></pre> <p>A context within which files are committed together upon exit</p> <p>Requires the file class to implement <code>.commit()</code> and <code>.discard()</code> for the normal and exception cases.</p>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFileSystem.__init__","title":"__init__","text":"<pre><code>__init__(\n    host=None,\n    username=None,\n    password=None,\n    api_key=None,\n    api_key_prefix=None,\n    access_token=None,\n    verify_ssl=True,\n    ssl_ca_cert=None,\n    proxy=None,\n    configfile=\"~/.lakectl.yaml\",\n    create_branch_ok=True,\n    source_branch=\"main\",\n    **storage_options\n)\n</code></pre> <p>The LakeFS file system constructor.</p> PARAMETER  DESCRIPTION <code>host</code> <p>The address of your lakeFS instance.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>username</code> <p>The access key name to use in case of access key authentication.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>password</code> <p>The access key secret to use in case of access key authentication.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>api_key</code> <p>The API key to use in case of authentication with an API key.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>api_key_prefix</code> <p>A string prefix to use for the API key in authentication.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>access_token</code> <p>An access token to use in case of access token authentication.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>verify_ssl</code> <p>Whether to verify SSL certificates in API interactions. Do not disable in production.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ssl_ca_cert</code> <p>A custom certificate PEM file to use to verify the peer in SSL connections.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>proxy</code> <p>Proxy address to use when connecting to a lakeFS server.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>create_branch_ok</code> <p>Whether to create branches implicitly when not-existing branches are referenced on file uploads.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>source_branch</code> <p>Source branch set as origin when a new branch is implicitly created.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'main'</code> </p> <code>storage_options</code> <p>Configuration options to pass to the file system's directory cache.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>def __init__(\n    self,\n    host: str | None = None,\n    username: str | None = None,\n    password: str | None = None,\n    api_key: str | None = None,\n    api_key_prefix: str | None = None,\n    access_token: str | None = None,\n    verify_ssl: bool = True,\n    ssl_ca_cert: str | None = None,\n    proxy: str | None = None,\n    configfile: str = \"~/.lakectl.yaml\",\n    create_branch_ok: bool = True,\n    source_branch: str = \"main\",\n    **storage_options: Any,\n):\n    \"\"\"\n    The LakeFS file system constructor.\n\n    Parameters\n    ----------\n    host: str or None\n        The address of your lakeFS instance.\n    username: str or None\n        The access key name to use in case of access key authentication.\n    password: str or None\n        The access key secret to use in case of access key authentication.\n    api_key: str or None\n        The API key to use in case of authentication with an API key.\n    api_key_prefix: str or None\n        A string prefix to use for the API key in authentication.\n    access_token: str or None\n        An access token to use in case of access token authentication.\n    verify_ssl: bool\n        Whether to verify SSL certificates in API interactions. Do not disable in production.\n    ssl_ca_cert: str or None\n        A custom certificate PEM file to use to verify the peer in SSL connections.\n    proxy: str or None\n        Proxy address to use when connecting to a lakeFS server.\n    create_branch_ok: bool\n        Whether to create branches implicitly when not-existing branches are referenced on file uploads.\n    source_branch: str\n        Source branch set as origin when a new branch is implicitly created.\n    storage_options: Any\n        Configuration options to pass to the file system's directory cache.\n    \"\"\"\n    super().__init__(**storage_options)\n\n    if (p := Path(configfile).expanduser()).exists():\n        lakectl_config = LakectlConfig.read(p)\n    else:\n        # empty config.\n        lakectl_config = LakectlConfig()\n\n    configuration = Configuration(\n        host=host or os.getenv(\"LAKEFS_HOST\") or lakectl_config.host,\n        api_key=api_key or os.getenv(\"LAKEFS_API_KEY\"),\n        api_key_prefix=api_key_prefix or os.getenv(\"LAKEFS_API_KEY_PREFIX\"),\n        access_token=access_token or os.getenv(\"LAKEFS_ACCESS_TOKEN\"),\n        username=username or os.getenv(\"LAKEFS_USERNAME\") or lakectl_config.username,\n        password=password or os.getenv(\"LAKEFS_PASSWORD\") or lakectl_config.password,\n        ssl_ca_cert=ssl_ca_cert or os.getenv(\"LAKEFS_SSL_CA_CERT\"),\n    )\n    # proxy address, not part of the constructor\n    configuration.proxy = proxy\n    # whether to verify SSL certs, not part of the constructor\n    configuration.verify_ssl = verify_ssl\n\n    self.client = LakeFSClient(configuration=configuration)\n    self.create_branch_ok = create_branch_ok\n    self.source_branch = source_branch\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFileSystem.start_transaction","title":"start_transaction","text":"<pre><code>start_transaction()\n</code></pre> <p>Begin write transaction for deferring files, non-context version</p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>def start_transaction(self):\n    \"\"\"Begin write transaction for deferring files, non-context version\"\"\"\n    self._intrans = True\n    self._transaction = LakeFSTransaction(self)\n    return self.transaction\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFileSystem.scope","title":"scope","text":"<pre><code>scope(create_branch_ok=None, source_branch=None)\n</code></pre> <p>A context manager yielding scope in which the lakeFS file system behavior is changed from defaults.</p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>@contextmanager\ndef scope(\n    self,\n    create_branch_ok: bool | None = None,\n    source_branch: str | None = None,\n) -&gt; EmptyYield:\n    \"\"\"\n    A context manager yielding scope in which the lakeFS file system behavior\n    is changed from defaults.\n    \"\"\"\n    curr_create_branch_ok, curr_source_branch = (\n        self.create_branch_ok,\n        self.source_branch,\n    )\n    try:\n        if create_branch_ok is not None:\n            self.create_branch_ok = create_branch_ok\n        if source_branch is not None:\n            self.source_branch = source_branch\n        yield\n    finally:\n        self.create_branch_ok = curr_create_branch_ok\n        self.source_branch = curr_source_branch\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFile","title":"LakeFSFile","text":"<p>             Bases: <code>AbstractBufferedFile</code></p> <p>lakeFS file implementation. Buffered in reads, unbuffered in writes.</p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>class LakeFSFile(AbstractBufferedFile):\n    \"\"\"lakeFS file implementation. Buffered in reads, unbuffered in writes.\"\"\"\n\n    def __init__(\n        self,\n        fs: LakeFSFileSystem,\n        path: str,\n        mode: Literal[\"rb\", \"wb\"] = \"rb\",\n        block_size: int | str = \"default\",\n        autocommit: bool = True,\n        cache_type: str = \"readahead\",\n        cache_options: dict[str, Any] | None = None,\n        size: int | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(\n            fs,\n            path,\n            mode=mode,\n            block_size=block_size,\n            autocommit=autocommit,\n            cache_type=cache_type,\n            cache_options=cache_options,\n            size=size,\n            **kwargs,\n        )\n\n        self.buffer: io.BytesIO\n        if mode == \"wb\" and self.fs.create_branch_ok:\n            repository, branch, resource = parse(path)\n            create_branch(self.fs.client, repository, branch, self.fs.source_branch)\n\n    def _upload_chunk(self, final: bool = False) -&gt; bool:\n        \"\"\"Commits the file on final chunk via single-shot upload, no-op otherwise.\"\"\"\n        if final and self.autocommit:\n            self.commit()\n        return not final\n\n    def commit(self):\n        \"\"\"Commit the file via single-shot upload.\"\"\"\n        repository, branch, resource = parse(self.path)\n\n        with self.fs.wrapped_api_call():\n            # empty buffer is equivalent to a touch()\n            self.buffer.seek(0)\n            self.fs.client.objects_api.upload_object(\n                repository=repository,\n                branch=branch,\n                path=resource,\n                content=self.buffer.read(),\n            )\n\n        self.buffer = io.BytesIO()\n\n    def discard(self):\n        self.buffer = io.BytesIO()  # discards the data, but in a type-safe way.\n\n    def flush(self, force: bool = False) -&gt; None:\n        \"\"\"\n        Write buffered data to backend store.\n\n        Writes the current buffer, if it is larger than the block-size, or if\n        the file is being closed.\n\n        In contrast to the abstract class, this implementation does NOT unload the buffer\n        if it is larger than the block size, because the lakeFS server does not support\n        multipart uploads.\n\n        Parameters\n        ----------\n        force: bool\n            When closing, write the last block even if it is smaller than\n            blocks are allowed to be. Disallows further writing to this file.\n        \"\"\"\n\n        if self.closed:\n            raise ValueError(\"Flush on closed file\")\n        self.forced: bool\n        if force and self.forced:\n            raise ValueError(\"Force flush cannot be called more than once\")\n        if force:\n            self.forced = True\n\n        if self.mode != \"wb\":\n            # no-op to flush on read-mode\n            return\n\n        if not force and self.buffer.tell() &lt; self.blocksize:\n            # Defer write on small block\n            return\n\n        self.offset: int\n        if self.offset is None:\n            # Initialize an upload\n            self.offset = 0\n\n        if self._upload_chunk(final=force) is not False:\n            self.offset += self.buffer.seek(0, 2)\n\n    def _fetch_range(self, start: int, end: int) -&gt; bytes:\n        repository, ref, resource = parse(self.path)\n        with self.fs.wrapped_api_call():\n            return self.fs.client.objects_api.get_object(\n                repository, ref, resource, range=f\"bytes={start}-{end - 1}\"\n            )\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFile.commit","title":"commit","text":"<pre><code>commit()\n</code></pre> <p>Commit the file via single-shot upload.</p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>def commit(self):\n    \"\"\"Commit the file via single-shot upload.\"\"\"\n    repository, branch, resource = parse(self.path)\n\n    with self.fs.wrapped_api_call():\n        # empty buffer is equivalent to a touch()\n        self.buffer.seek(0)\n        self.fs.client.objects_api.upload_object(\n            repository=repository,\n            branch=branch,\n            path=resource,\n            content=self.buffer.read(),\n        )\n\n    self.buffer = io.BytesIO()\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFile.flush","title":"flush","text":"<pre><code>flush(force=False)\n</code></pre> <p>Write buffered data to backend store.</p> <p>Writes the current buffer, if it is larger than the block-size, or if the file is being closed.</p> <p>In contrast to the abstract class, this implementation does NOT unload the buffer if it is larger than the block size, because the lakeFS server does not support multipart uploads.</p> PARAMETER  DESCRIPTION <code>force</code> <p>When closing, write the last block even if it is smaller than blocks are allowed to be. Disallows further writing to this file.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>def flush(self, force: bool = False) -&gt; None:\n    \"\"\"\n    Write buffered data to backend store.\n\n    Writes the current buffer, if it is larger than the block-size, or if\n    the file is being closed.\n\n    In contrast to the abstract class, this implementation does NOT unload the buffer\n    if it is larger than the block size, because the lakeFS server does not support\n    multipart uploads.\n\n    Parameters\n    ----------\n    force: bool\n        When closing, write the last block even if it is smaller than\n        blocks are allowed to be. Disallows further writing to this file.\n    \"\"\"\n\n    if self.closed:\n        raise ValueError(\"Flush on closed file\")\n    self.forced: bool\n    if force and self.forced:\n        raise ValueError(\"Force flush cannot be called more than once\")\n    if force:\n        self.forced = True\n\n    if self.mode != \"wb\":\n        # no-op to flush on read-mode\n        return\n\n    if not force and self.buffer.tell() &lt; self.blocksize:\n        # Defer write on small block\n        return\n\n    self.offset: int\n    if self.offset is None:\n        # Initialize an upload\n        self.offset = 0\n\n    if self._upload_chunk(final=force) is not False:\n        self.offset += self.buffer.seek(0, 2)\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/","title":"transaction","text":""},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction","title":"LakeFSTransaction","text":"<p>             Bases: <code>Transaction</code></p> <p>A lakeFS transaction model capable of versioning operations in between file uploads.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>class LakeFSTransaction(Transaction):\n    \"\"\"A lakeFS transaction model capable of versioning operations in between file uploads.\"\"\"\n\n    def __init__(self, fs: \"LakeFSFileSystem\"):\n        \"\"\"\n        Initialize a lakeFS transaction. The base class' `file` stack can also contain\n        versioning operations.\n        \"\"\"\n        super().__init__(fs=fs)\n        self.fs: \"LakeFSFileSystem\"\n        self.files: deque[AbstractBufferedFile | VersioningOpTuple] = deque(self.files)\n\n    def __enter__(self):\n        self.fs._intrans = True\n        return self\n\n    def commit(\n        self, repository: str, branch: str, message: str, metadata: dict[str, str] | None = None\n    ) -&gt; Placeholder[Commit]:\n        \"\"\"\n        Create a commit on a branch in a repository with a commit message and attached metadata.\n        \"\"\"\n\n        # bind all arguments to the client helper function, and then add it to the file-/callstack.\n        op = partial(\n            commit, repository=repository, branch=branch, message=message, metadata=metadata\n        )\n        p: Placeholder[Commit] = Placeholder()\n        self.files.append((op, p))\n        # return a placeholder for the commit.\n        return p\n\n    def complete(self, commit: bool = True) -&gt; None:\n        \"\"\"\n        Finish transaction: Unwind file+versioning op stack via\n         1. Committing or discarding in case of a file, and\n         2. Conducting versioning operations using the file system's client.\n\n         No operations happen and all files are discarded if `commit` is False,\n         which is the case e.g. if an exception happens in the context manager.\n        \"\"\"\n        while self.files:\n            # fsspec base class calls `append` on the file, which means we\n            # have to pop from the left to preserve order.\n            f = self.files.popleft()\n            if isinstance(f, AbstractBufferedFile):\n                if commit:\n                    f.commit()\n                else:\n                    f.discard()\n            else:\n                # client helper + return value case.\n                op, retval = f\n                if commit:\n                    result = op(self.fs.client)\n                    # if the transaction member returns a placeholder,\n                    # fill it with the result of the client helper.\n                    if isinstance(retval, Placeholder):\n                        retval.set_value(result)\n\n        self.fs._intrans = False\n\n    def create_branch(\n        self, repository: str, name: str, source_branch: str, exist_ok: bool = True\n    ) -&gt; str:\n        \"\"\"\n        Create a branch with the name `name` in a repository, branching off `source_branch`.\n        \"\"\"\n        op = partial(\n            create_branch,\n            repository=repository,\n            name=name,\n            source_branch=source_branch,\n            exist_ok=exist_ok,\n        )\n        self.files.append((op, name))\n        return name\n\n    def merge(self, repository: str, source_ref: str, into: str) -&gt; None:\n        \"\"\"Merge a branch into another branch in a repository.\"\"\"\n        op = partial(merge, repository=repository, source_ref=source_ref, target_branch=into)\n        self.files.append((op, None))\n        return None\n\n    def revert(self, repository: str, branch: str, parent_number: int = 1) -&gt; None:\n        \"\"\"Revert a previous commit on a branch.\"\"\"\n        op = partial(revert, repository=repository, branch=branch, parent_number=parent_number)\n        self.files.append((op, None))\n        return None\n\n    def rev_parse(\n        self, repository: str, ref: str | Placeholder[Commit], parent: int = 0\n    ) -&gt; Placeholder[Commit]:\n        \"\"\"Parse a given reference or any of its parents in a repository.\"\"\"\n\n        def rev_parse_op(client: LakeFSClient, **kwargs: Any) -&gt; Commit:\n            kwargs = unwrap_placeholders(kwargs)\n            return rev_parse(client, **kwargs)\n\n        p: Placeholder[Commit] = Placeholder()\n        op = partial(rev_parse_op, repository=repository, ref=ref, parent=parent)\n        self.files.append((op, p))\n        return p\n\n    def tag(\n        self, repository: str, ref: str | Placeholder[Commit], tag: str, exist_ok: bool = True\n    ) -&gt; str:\n        \"\"\"Create a tag referencing a commit in a repository.\"\"\"\n\n        def tag_op(client: LakeFSClient, **kwargs: Any) -&gt; Ref:\n            kwargs = unwrap_placeholders(kwargs)\n            return create_tag(client, **kwargs)\n\n        self.files.append(\n            (partial(tag_op, repository=repository, ref=ref, tag=tag, exist_ok=exist_ok), tag)\n        )\n        return tag\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction.__init__","title":"__init__","text":"<pre><code>__init__(fs)\n</code></pre> <p>Initialize a lakeFS transaction. The base class' <code>file</code> stack can also contain versioning operations.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>def __init__(self, fs: \"LakeFSFileSystem\"):\n    \"\"\"\n    Initialize a lakeFS transaction. The base class' `file` stack can also contain\n    versioning operations.\n    \"\"\"\n    super().__init__(fs=fs)\n    self.fs: \"LakeFSFileSystem\"\n    self.files: deque[AbstractBufferedFile | VersioningOpTuple] = deque(self.files)\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction.commit","title":"commit","text":"<pre><code>commit(repository, branch, message, metadata=None)\n</code></pre> <p>Create a commit on a branch in a repository with a commit message and attached metadata.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>def commit(\n    self, repository: str, branch: str, message: str, metadata: dict[str, str] | None = None\n) -&gt; Placeholder[Commit]:\n    \"\"\"\n    Create a commit on a branch in a repository with a commit message and attached metadata.\n    \"\"\"\n\n    # bind all arguments to the client helper function, and then add it to the file-/callstack.\n    op = partial(\n        commit, repository=repository, branch=branch, message=message, metadata=metadata\n    )\n    p: Placeholder[Commit] = Placeholder()\n    self.files.append((op, p))\n    # return a placeholder for the commit.\n    return p\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction.complete","title":"complete","text":"<pre><code>complete(commit=True)\n</code></pre> <p>Finish transaction: Unwind file+versioning op stack via  1. Committing or discarding in case of a file, and  2. Conducting versioning operations using the file system's client.</p> <p>No operations happen and all files are discarded if <code>commit</code> is False,  which is the case e.g. if an exception happens in the context manager.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>def complete(self, commit: bool = True) -&gt; None:\n    \"\"\"\n    Finish transaction: Unwind file+versioning op stack via\n     1. Committing or discarding in case of a file, and\n     2. Conducting versioning operations using the file system's client.\n\n     No operations happen and all files are discarded if `commit` is False,\n     which is the case e.g. if an exception happens in the context manager.\n    \"\"\"\n    while self.files:\n        # fsspec base class calls `append` on the file, which means we\n        # have to pop from the left to preserve order.\n        f = self.files.popleft()\n        if isinstance(f, AbstractBufferedFile):\n            if commit:\n                f.commit()\n            else:\n                f.discard()\n        else:\n            # client helper + return value case.\n            op, retval = f\n            if commit:\n                result = op(self.fs.client)\n                # if the transaction member returns a placeholder,\n                # fill it with the result of the client helper.\n                if isinstance(retval, Placeholder):\n                    retval.set_value(result)\n\n    self.fs._intrans = False\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction.create_branch","title":"create_branch","text":"<pre><code>create_branch(repository, name, source_branch, exist_ok=True)\n</code></pre> <p>Create a branch with the name <code>name</code> in a repository, branching off <code>source_branch</code>.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>def create_branch(\n    self, repository: str, name: str, source_branch: str, exist_ok: bool = True\n) -&gt; str:\n    \"\"\"\n    Create a branch with the name `name` in a repository, branching off `source_branch`.\n    \"\"\"\n    op = partial(\n        create_branch,\n        repository=repository,\n        name=name,\n        source_branch=source_branch,\n        exist_ok=exist_ok,\n    )\n    self.files.append((op, name))\n    return name\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction.merge","title":"merge","text":"<pre><code>merge(repository, source_ref, into)\n</code></pre> <p>Merge a branch into another branch in a repository.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>def merge(self, repository: str, source_ref: str, into: str) -&gt; None:\n    \"\"\"Merge a branch into another branch in a repository.\"\"\"\n    op = partial(merge, repository=repository, source_ref=source_ref, target_branch=into)\n    self.files.append((op, None))\n    return None\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction.revert","title":"revert","text":"<pre><code>revert(repository, branch, parent_number=1)\n</code></pre> <p>Revert a previous commit on a branch.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>def revert(self, repository: str, branch: str, parent_number: int = 1) -&gt; None:\n    \"\"\"Revert a previous commit on a branch.\"\"\"\n    op = partial(revert, repository=repository, branch=branch, parent_number=parent_number)\n    self.files.append((op, None))\n    return None\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction.rev_parse","title":"rev_parse","text":"<pre><code>rev_parse(repository, ref, parent=0)\n</code></pre> <p>Parse a given reference or any of its parents in a repository.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>def rev_parse(\n    self, repository: str, ref: str | Placeholder[Commit], parent: int = 0\n) -&gt; Placeholder[Commit]:\n    \"\"\"Parse a given reference or any of its parents in a repository.\"\"\"\n\n    def rev_parse_op(client: LakeFSClient, **kwargs: Any) -&gt; Commit:\n        kwargs = unwrap_placeholders(kwargs)\n        return rev_parse(client, **kwargs)\n\n    p: Placeholder[Commit] = Placeholder()\n    op = partial(rev_parse_op, repository=repository, ref=ref, parent=parent)\n    self.files.append((op, p))\n    return p\n</code></pre>"},{"location":"reference/lakefs_spec/transaction/#lakefs_spec.transaction.LakeFSTransaction.tag","title":"tag","text":"<pre><code>tag(repository, ref, tag, exist_ok=True)\n</code></pre> <p>Create a tag referencing a commit in a repository.</p> Source code in <code>src/lakefs_spec/transaction.py</code> <pre><code>def tag(\n    self, repository: str, ref: str | Placeholder[Commit], tag: str, exist_ok: bool = True\n) -&gt; str:\n    \"\"\"Create a tag referencing a commit in a repository.\"\"\"\n\n    def tag_op(client: LakeFSClient, **kwargs: Any) -&gt; Ref:\n        kwargs = unwrap_placeholders(kwargs)\n        return create_tag(client, **kwargs)\n\n    self.files.append(\n        (partial(tag_op, repository=repository, ref=ref, tag=tag, exist_ok=exist_ok), tag)\n    )\n    return tag\n</code></pre>"},{"location":"reference/lakefs_spec/util/","title":"util","text":""},{"location":"reference/lakefs_spec/util/#lakefs_spec.util.parse","title":"parse","text":"<pre><code>parse(path)\n</code></pre> <p>Parses a lakeFS URI in the form <code>&lt;repo&gt;/&lt;ref&gt;/&lt;resource&gt;</code>.</p> PARAMETER  DESCRIPTION <code>path</code> <p>String path, needs to conform to the lakeFS URI format described above. The <code>&lt;resource&gt;</code> part can be the empty string.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A 3-tuple of repository name, reference, and resource name.</p> Source code in <code>src/lakefs_spec/util.py</code> <pre><code>def parse(path: str) -&gt; tuple[str, str, str]:\n    \"\"\"\n    Parses a lakeFS URI in the form ``&lt;repo&gt;/&lt;ref&gt;/&lt;resource&gt;``.\n\n    Parameters\n    ----------\n    path: str\n        String path, needs to conform to the lakeFS URI format described above.\n        The ``&lt;resource&gt;`` part can be the empty string.\n\n    Returns\n    -------\n    str\n        A 3-tuple of repository name, reference, and resource name.\n    \"\"\"\n\n    # First regex reflects the lakeFS repository naming rules:\n    # only lowercase letters, digits and dash, no leading dash,\n    # minimum 3, maximum 63 characters\n    # https://docs.lakefs.io/understand/model.html#repository\n    # Second regex is the branch: Only letters, digits, underscores\n    # and dash, no leading dash\n    path_regex = re.compile(r\"(?:lakefs://)?([a-z0-9][a-z0-9\\-]{2,62})/(\\w[\\w\\-]*)/(.*)\")\n    results = path_regex.fullmatch(path)\n    if results is None:\n        raise ValueError(\n            f\"expected path with structure lakefs://&lt;repo&gt;/&lt;ref&gt;/&lt;resource&gt;, got {path!r}\"\n        )\n\n    repo, ref, resource = results.groups()\n    return repo, ref, resource\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Info</p> <p>We aim to provide additional tutorials in the future - contributions are welcome!</p> <ul> <li>Quickstart example: Using <code>lakefs-spec</code> as a file system</li> <li>A fully-worked data science example: Using <code>lakefs-spec</code> together with Pandas to train a classifier based on a public dataset and simulate additional data being collected</li> </ul>"},{"location":"tutorials/demo_data_science_project/","title":"Demo data science project","text":"<pre><code>%pip install lakefs-spec numpy pandas scikit-learn\n</code></pre> <pre>\n<code>Requirement already satisfied: lakefs-spec in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (0.3.1.dev49+g996f21b)\nCollecting numpy\n  Using cached numpy-1.26.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nCollecting pandas\n  Using cached pandas-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting scikit-learn\n  Using cached scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: fsspec&gt;=2023.6.0 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from lakefs-spec) (2023.10.0)\nRequirement already satisfied: lakefs-sdk&gt;=1.0.0 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from lakefs-spec) (1.0.0)\nRequirement already satisfied: pyyaml&gt;=6.0.1 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from lakefs-spec) (6.0.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from pandas) (2023.3.post1)\nCollecting tzdata&gt;=2022.1 (from pandas)\n  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\nCollecting scipy&gt;=1.5.0 (from scikit-learn)\n  Using cached scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nCollecting joblib&gt;=1.1.1 (from scikit-learn)\n  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting threadpoolctl&gt;=2.0.0 (from scikit-learn)\n  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\nRequirement already satisfied: urllib3&lt;2.1.0,&gt;=1.25.3 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from lakefs-sdk&gt;=1.0.0-&gt;lakefs-spec) (2.0.7)\nRequirement already satisfied: pydantic&lt;2,&gt;=1.10.5 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from lakefs-sdk&gt;=1.0.0-&gt;lakefs-spec) (1.10.13)\nRequirement already satisfied: aenum in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from lakefs-sdk&gt;=1.0.0-&gt;lakefs-spec) (3.1.15)\nRequirement already satisfied: six&gt;=1.5 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nRequirement already satisfied: typing-extensions&gt;=4.2.0 in /opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages (from pydantic&lt;2,&gt;=1.10.5-&gt;lakefs-sdk&gt;=1.0.0-&gt;lakefs-spec) (4.8.0)\nUsing cached numpy-1.26.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\nUsing cached pandas-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\nUsing cached scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\nUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\nUsing cached scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\nUsing cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\nInstalling collected packages: tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\nSuccessfully installed joblib-1.3.2 numpy-1.26.2 pandas-2.1.3 scikit-learn-1.3.2 scipy-1.11.4 threadpoolctl-3.2.0 tzdata-2023.3\n\n[notice] A new release of pip is available: 23.2.1 -&gt; 23.3.1\n[notice] To update, run: pip3.11 install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n</code>\n</pre> <p>From a terminal, start a Jupyter notebook server if one is not already running, by executing <code>jupyter notebook</code>. Double click the notebook to autostart a kernel using the created virtual environment.</p> <pre><code>import urllib.request\nimport os\n\ndestination = os.path.expanduser(\"~/.lakectl.yaml\")\nurllib.request.urlretrieve(\n    \"https://raw.githubusercontent.com/aai-institute/lakefs-spec/main/docs/tutorials/.lakectl.yaml\", destination)\n</code></pre> <pre>\n<code>('/home/runner/.lakectl.yaml', &lt;http.client.HTTPMessage at 0x7f145066c790&gt;)</code>\n</pre> <p>Now we can instantiate the <code>LakeFSFileSystem</code> and it will use the credentials we just downloaded for authentication. Alternatively, we could have passed the credentials in the code. It is important, that the credentials are available at the time of filesystem instantiation. </p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\nREPO_NAME = 'weather'\n</code></pre> <p>We will create a repository using a helper function provided by <code>lakefs-spec</code>. If you created one in the UI, make sure to set the <code>REPO_NAME</code> variable in the cell above accordingly. You can re-execute if necessary. Otherwise, execute the next cell.</p> <pre><code>from lakefs_spec.client_helpers import create_repository\n\nrepo = create_repository(client=fs.client, name=REPO_NAME, storage_namespace=f\"local://{REPO_NAME}\")\n</code></pre> <p>Now it's time to get some data. We will use the Open Meteo API, where we can pull weather data from an API for free (as long as we are non-commercial) and without an API-token.</p> <p>First, create the folder <code>data</code> inside a directory when your notebook is located:</p> <pre><code>os.makedirs(\"data\", exist_ok=True)\n</code></pre> <p>Then, for the purpose of training, get the full 2010 weather data from Munich:</p> <pre><code>destination = \"data/weather-2010.json\"\nurllib.request.urlretrieve(\"https://archive-api.open-meteo.com/v1/archive?latitude=52.52&amp;amp;longitude=13.41&amp;amp;start_date=2010-01-01&amp;amp;end_date=2010-12-31&amp;amp;hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m\", destination)\n</code></pre> <pre>\n<code>('data/weather-2010.json', &lt;http.client.HTTPMessage at 0x7f14482fc310&gt;)</code>\n</pre> <pre><code>from lakefs_spec import LakeFSFileSystem\nLakeFSFileSystem.clear_instance_cache()\n\nNEW_BRANCH_NAME = 'transform-raw-data'\n\nfs = LakeFSFileSystem()\nfs.put(destination, f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')\n</code></pre> <p>Going to the LakeFS UI in your browser, you can change the branch view to <code>transform-raw-data</code> and see the saved file. However, the change is not yet committed. While you can do that manually via the uncommitted changes tab in the UI, we will commit the file in a different way.</p> <p>To easily carry out versioning operations while uploading files, you can use a transaction. A transaction is a context manager that keeps track of all files that were uploaded in its scope, as well as all versioning operations happening in between file uploads. All operations are deferred to the end of the transaction, and are executed sequentially on completion.</p> <p>To create a commit after a file upload, you can run the following transaction:</p> <pre><code>with fs.transaction as tx:\n    fs.put(destination, f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json', precheck=True, autocommit=False)\n    tx.commit(repository=REPO_NAME, branch=NEW_BRANCH_NAME, message=\"Add 2010 weather data\")\n</code></pre> <p>This transaction, if successful, will create a commit. Since we already uploaded the file, lakeFS will skip the upload as the checksums of the local and remote file match.</p> <p>If we want to execute the upload even for an unchanged file, we can do so by passing <code>precheck=False</code> to the <code>fs.put()</code> operation.</p> <pre><code>import json\n\nimport pandas as pd\n\n\ndef transform_json_weather_data(filepath):\n    with open(filepath,\"r\") as f:\n        data = json.load(f)\n\n    df = pd.DataFrame.from_dict(data[\"hourly\"])\n    df.time = pd.to_datetime(df.time)\n    df['is_raining'] = df.rain &amp;gt; 0\n    df['is_raining_in_1_day'] = df.is_raining.shift(24)\n    df = df.dropna()\n    return df\n\ndf = transform_json_weather_data('data/weather-2010.json')\ndf.head(5)\n</code></pre> time temperature_2m relativehumidity_2m rain pressure_msl surface_pressure cloudcover cloudcover_low cloudcover_mid cloudcover_high windspeed_10m windspeed_100m winddirection_10m winddirection_100m is_raining is_raining_in_1_day 24 2010-01-02 00:00:00 -3.0 88 0.0 1004.0 999.2 100 54 100 70 8.3 17.3 18 31 False False 25 2010-01-02 01:00:00 -3.2 89 0.0 1004.5 999.7 100 37 98 92 9.1 18.6 9 22 False False 26 2010-01-02 02:00:00 -3.4 89 0.0 1005.2 1000.4 100 24 98 77 10.1 20.3 2 13 False False 27 2010-01-02 03:00:00 -3.5 89 0.0 1005.6 1000.8 93 8 98 89 11.2 21.7 4 12 False False 28 2010-01-02 04:00:00 -3.7 90 0.0 1006.2 1001.4 94 6 100 95 11.2 21.8 358 9 False False <p>Next, we save this data as a CSV file into the main branch. When the transaction commit helper is called, the newly put CSV file is committed. You can verify the saving worked in the LakeFS UI in your browser by switching to the commits tab of the <code>main</code> branch.</p> <pre><code>with fs.transaction as tx:\n    df.to_csv(f'lakefs://{REPO_NAME}/main/weather_2010.csv')\n    tx.commit(repository=REPO_NAME, branch=\"main\", message=\"Update weather data\")\n</code></pre> <pre><code>import sklearn.model_selection\n\nmodel_data = df.drop('time', axis=1)\n\ntrain, test = sklearn.model_selection.train_test_split(model_data, random_state=7)\n</code></pre> <p>We save these train and test datasets into a new <code>training</code> branch. If the branch does not exist yet, as in this case, it is implicitly created by default. You can control this behaviour with the <code>create_branch_ok</code> flag when initializing the <code>LakeFSFileSystem</code>. By default, <code>create_branch_ok</code> is set to <code>True</code>, so we need to only set <code>fs = LakeFSFileSystem()</code> to enable implicit branch creation.</p> <pre><code>TRAINING_BRANCH = 'training'\n\nwith fs.transaction as tx:\n    train.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n    test.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n    tx.commit(repository=REPO_NAME, branch=TRAINING_BRANCH, message=\"Add train-test split of 2010 weather data\")\n</code></pre> <p>Implicit branch creation is a convenient way to create new branches programmatically. However, one drawback is that typos in your code might result in new accidental branch creations. If you want to avoid this implicit behavior and raise errors instead, you can disable implicit branch creation by setting <code>fs.create_branch_ok=False</code>.</p> <p>We can now read train and test files directly from the remote LakeFS instance. (You can verify that neither the train nor the test file are saved in the <code>/data</code> directory).</p> <pre><code>train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\ntest = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n\ntrain.head()\n</code></pre> temperature_2m relativehumidity_2m rain pressure_msl surface_pressure cloudcover cloudcover_low cloudcover_mid cloudcover_high windspeed_10m windspeed_100m winddirection_10m winddirection_100m is_raining is_raining_in_1_day 8594 -3.4 87 0.0 1008.9 1004.1 100 100 99 98 22.4 36.4 341 346 False False 2725 14.6 41 0.0 1023.4 1018.8 18 0 1 57 3.6 4.0 354 350 False False 7096 8.5 65 0.0 1006.9 1002.3 30 1 0 97 15.6 29.2 173 178 False False 6621 12.1 77 0.0 1013.8 1009.2 34 8 0 90 20.1 35.7 134 137 False False 2935 9.4 92 1.5 1008.4 1003.8 100 100 100 95 13.7 23.5 35 40 True False <p>Let's check the shape of train and test data. Later on we will train to get back to this data version and reproduce the results of the experiment.</p> <pre><code>print(f'Initial train data shape: {train.shape}')\nprint(f'Initial test data shape: {test.shape}')\n</code></pre> <pre>\n<code>Initial train data shape: (6552, 15)\nInitial test data shape: (2184, 15)\n</code>\n</pre> <p>We now proceed to train a random forest classifier and evaluate it on the test set:</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\n\ndependent_variable = 'is_raining_in_1_day'\n\nmodel = DecisionTreeClassifier(random_state=7)\nx_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\nx_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n\nmodel.fit(x_train, y_train)\n\ntest_acc = model.score(x_test, y_test)\n\nprint(f\"Test accuracy: {test_acc:.2%}\")\n</code></pre> <pre>\n<code>Test accuracy: 86.86%\n</code>\n</pre> <pre><code>destination = \"data/weather-2020.json\"\nurllib.request.urlretrieve(\"https://archive-api.open-meteo.com/v1/archive?latitude=52.52&amp;amp;longitude=13.41&amp;amp;start_date=2020-01-01&amp;amp;end_date=2020-12-31&amp;amp;hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m\", destination)\n\nnew_data = transform_json_weather_data('data/weather-2020.json')\n\nwith fs.transaction as tx:\n    new_data.to_csv(f'lakefs://{REPO_NAME}/main/weather_2020.csv')\n    tx.commit(repository=REPO_NAME, branch=\"main\", message=\"Add 2020 weather data\")\n</code></pre> <pre><code>new_data = new_data.drop('time', axis=1)\n</code></pre> <p>Let's concatenate the old data and the new data, create a new train-test split, and overwrite the files on lakeFS:</p> <pre><code>df_train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\ndf_test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n\nfull_data = pd.concat([new_data, df_train, df_test])\n\ntrain_df, test_df = sklearn.model_selection.train_test_split(full_data, random_state=7)\n\nwith fs.transaction as tx:\n    train_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n    test_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n    tx.commit(repository=REPO_NAME, branch=TRAINING_BRANCH, message=\"Add train-test split of 2010 and 2020 data\")\n</code></pre> <p>We may now read the updated data directly from lakeFS and check their shape to insure that initial files <code>train_weather.csv</code> and <code>test_weather.csv</code> have been overwritten successfully (number of rows should be significantly higher as 2020 data were added):</p> <pre><code>train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\ntest = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n\nprint(f'Updated train data shape: {train.shape}')\nprint(f'Updated test data shape: {test.shape}')\n</code></pre> <pre>\n<code>Updated train data shape: (13122, 15)\nUpdated test data shape: (4374, 15)\n</code>\n</pre> <p>Now we may train the model based on the new train data and validate based on the new test data:</p> <pre><code>x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\nx_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n\nmodel.fit(x_train, y_train)\n\ntest_acc = model.score(x_test, y_test)\n\nprint(f\"Test accuracy: {test_acc:.2%}\")\n</code></pre> <pre>\n<code>Test accuracy: 84.04%\n</code>\n</pre> <pre><code>from lakefs_spec.client_helpers import rev_parse\n\n# parent is a relative number of a commit when 0 is the latest\nprevious_commit = rev_parse(fs.client, REPO_NAME, TRAINING_BRANCH, parent=1)\nfixed_commit_id = previous_commit.id\nprint(fixed_commit_id)\n</code></pre> <pre>\n<code>803a00bec06cd02a58879c99137beb1f55726548d292efba6943ddca1c86ee42\n</code>\n</pre> <p>With our transaction setup, both <code>DataFrame.to_csv()</code> operations are kept in a single commit. To get other commits with the <code>rev_parse</code> function, you can change the <code>repository</code> and <code>branch</code> parameters. To go back in the chosen branch's commit history, you can increase the <code>parent</code> parameter. In our case the initial data was commited two commits ago - we count the latest commit on a branch as 0, thus <code>parent = 1</code>.</p> <p>Let's check whether we manage to get the initial train and test data with this commit SHA, comparing the shape to the initial data shape:</p> <pre><code>train = pd.read_csv(f\"lakefs://{REPO_NAME}/{fixed_commit_id}/train_weather.csv\", index_col=0)\ntest = pd.read_csv(f\"lakefs://{REPO_NAME}/{fixed_commit_id}/test_weather.csv\", index_col=0)\n\nprint(f'train data shape: {train.shape}')\nprint(f'test data shape: {test.shape}')\n</code></pre> <pre>\n<code>train data shape: (6552, 15)\ntest data shape: (2184, 15)\n</code>\n</pre> <p>Let's train and validate the model based on re-fetched data and see whether we manage to reproduce the initial accuracy ratio:  </p> <pre><code>x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\nx_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n\nmodel.fit(x_train, y_train)\n\ntest_acc = model.score(x_test, y_test)\n\nprint(f\"Test accuracy: {test_acc:.2%}\")\n</code></pre> <pre>\n<code>Test accuracy: 86.86%\n</code>\n</pre> <pre><code>with fs.transaction as tx:\n    # the `tag` result is simply the tag name, in this case 'train-test-split-2010'.\n    tag = tx.tag(repository=REPO_NAME, ref=fixed_commit_id, tag='train-test-split-2010')\n</code></pre> <p>Now we can access the specific files with the semantic tag. Both the <code>fixed_commit_id</code> and <code>tag</code> reference the same version <code>ref</code> in lakeFS, whereas a branch name always points to the latest version on that respective branch.</p> <pre><code>train_from_branch_head = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\ntrain_from_commit_sha = pd.read_csv(f'lakefs://{REPO_NAME}/{fixed_commit_id}/train_weather.csv', index_col=0)\ntrain_from_semantic_tag = pd.read_csv(f'lakefs://{REPO_NAME}/{tag}/train_weather.csv', index_col=0)\n</code></pre> <p>We can verify this by looking at the lengths of the <code>DataFrame</code>s. We see that the <code>train_from_commit_sha</code> and <code>train_from_semantic_tag</code> are equal. </p> <pre><code>print(len(train_from_branch_head))\nprint(len(train_from_commit_sha))\nprint(len(train_from_semantic_tag))\n</code></pre> <pre>\n<code>13122\n6552\n6552\n</code>\n</pre>"},{"location":"tutorials/demo_data_science_project/#introduction","title":"Introduction","text":"<p>In this notebook, we will complete a small end-to-end data science tutorial that employs <code>lakeFS-spec</code> for data versioning. We will use weather data to train a random forest classifier to predict whether a given day from now is a raining day given the current weather.</p> <p>We will do the following:</p> <ul> <li>Environment setup</li> <li>lakeFS setup</li> <li>Data ingestion<ul> <li>Transactions</li> <li>PUT a file</li> </ul> </li> <li>Model training</li> <li>Updating data and retraining a model</li> <li>Accessing data versions and Reproducing Experiments</li> <li>Using a tag instead of a commit SHA for semantic versioning</li> </ul> <p>To execute the code in this tutorial as a Jupyter notebook, download this <code>.ipynb</code> file to a convenient location on your machine. You can also clone the whole <code>lakefs-spec</code> repository. During the execution of this tutorial, in the same directory, a folder <code>data</code> will be created. We will also download a file <code>.lakectl.yaml</code> to the current user's home directory.</p> <p>This tutorial assumes that you have installed <code>lakefs-spec</code> in a virtual environment, and that you have followed the quickstart guide to set up a local lakeFS instance.</p>"},{"location":"tutorials/demo_data_science_project/#environment-setup","title":"Environment Setup","text":"<p>Install the libraries necessary for this notebook on the environment you have just created: </p>"},{"location":"tutorials/demo_data_science_project/#lakefs-setup","title":"lakeFS Setup","text":"<p>Ensure you have Docker Desktop or a similar runtime available and running.</p> <p>Set up LakeFS. You can do this by executing the following <code>docker run</code> command (the lakeFS quickstart) in your console:</p> <p><code>docker run --name lakefs --pull always --rm --publish 8000:8000 treeverse/lakefs:latest run --quickstart</code></p> <p>Open a browser and navigate to the lakeFS instance - by default: http://localhost:8000/. </p> <p>Authenticate with the following credentials:</p> <pre><code>Access Key ID    : AKIAIOSFOLQUICKSTART\nSecret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n</code></pre> <p>As an email, you can enter anything, we won't need it in this example.</p> <p>Before we instantiate the filesystem connector <code>LakeFSFilesystem</code>, we need to configure authentication.</p> <p>There are multiple ways to authenticate to lakeFS from Python code - in this tutorial, we choose the convenient YAML file configuration. Execute the code below to download the YAML file including the lakeFS quickstart credentials and server URL to your user directory.</p>"},{"location":"tutorials/demo_data_science_project/#data-ingestion","title":"Data Ingestion","text":""},{"location":"tutorials/demo_data_science_project/#put-a-file","title":"PUT a file","text":"<p>The data is in JSON format. Therefore, we need to wrangle the data a bit to make it usable. But first we will save it into our lakeFS instance.</p> <p>lakeFS works similar to <code>git</code> as a versioning system. You can create commits that contain specific changes to the data. You can also work with branches to fork your own copy of the data such that you don't interfere with your colleagues. Every commit (on any branch) is identified by a commit SHA. This SHA can be used to programmatically interact with specific states of your data and enables logging of the specific data versions used to create a certain model. We will cover all of this in this demo.</p> <p>For now, we will <code>put</code> the file we have on a new branch, <code>transform-raw-data</code>, specifically created for our data.</p>"},{"location":"tutorials/demo_data_science_project/#transactions","title":"Transactions","text":""},{"location":"tutorials/demo_data_science_project/#data-transformation","title":"Data Transformation","text":"<p>Now let's transform the data for our use case. We put the transformation into a function to be able to reuse it later.</p> <p>In this notebook, we follow a simple toy example to predict whether it is raining at the same time tomorrow given weather data from right now.</p> <p>We will skip a lot of possible feature engineering etc. in order to focus on the application of lakeFS and the <code>LakeFSFileSystem</code>.</p>"},{"location":"tutorials/demo_data_science_project/#model-training","title":"Model Training","text":"<p>First we will do a train-test split:</p>"},{"location":"tutorials/demo_data_science_project/#updating-data-and-model","title":"Updating Data and Model","text":"<p>Until now, we only have used data from 2010. Let's download additional 2020 data, transform it, and save it to LakeFS.</p>"},{"location":"tutorials/demo_data_science_project/#accessing-data-version-and-reproducing-experiment","title":"Accessing Data Version and Reproducing Experiment","text":"<p>Let's assume we need to go to our initial data and reproduce the first experiment (the model trained on the 2010 data with its initial accuracy). This might be tricky as we have overwritten initial train and test data on lakeFS.</p> <p>To enable data versioning we should save the <code>ref</code> of the specific datasets. <code>ref</code> can be a branch we are pulling a file from LakeFS. <code>ref</code> can be also a commit id - then you can access different data versions within the same branch and not only the version from the latest commit. Therefore, we will use explicit versioning and get the actual commit SHA. We have multiple ways to do this. Manually, we could go into the lakeFS UI, select the training branch, and navigate to the \"Commits\" tab. There, we could see the second-latest commit, titled <code>Add train-test split of 2010 weather data</code>, and copy its ID.</p> <p>However, we want to automate as much as possible and therefore use a helper function. You find pre-written helper functions in the <code>lakefs_spec.client_helpers</code> module:</p>"},{"location":"tutorials/demo_data_science_project/#using-a-tag-instead-of-a-commit-sha-for-semantic-versioning","title":"Using a tag instead of a commit SHA for semantic versioning","text":"<p>The above method for data versioning works great when you have experiment tracking tools to store and retrieve the commit SHA in automated pipelines. But it is hard to remember and tedious to retrieve in manual prototyping. We can make selected versions of the dataset more accessible with semantic versioning. We attach a human-interpretable tag that points to a specific commit SHA.</p> <p>Creating a tag is easiest when done inside a transaction, just like the files we already uploaded. To do this, simply call <code>tx.tag</code> on the transaction and supply the repository name, the commit SHA to tag, and the intended tag name.</p>"}]}