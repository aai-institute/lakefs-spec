{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#lakefs-spec-an-fsspec-implementation-for-lakefs","title":"lakefs-spec: An <code>fsspec</code> implementation for lakeFS","text":"<p>This repository contains a filesystem-spec implementation for the lakeFS project. Its main goal is to facilitate versioned data operations in lakeFS directly from Python code, for example using <code>pandas</code>. Data versioning enables reproducibility of experiments - a best practice in machine learning.</p> <p>See the examples below (features, versioning best-practices) below for inspiration.</p> <p>A more detailed example is in the notebook in the <code>/demos</code> directory.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install the package directly from PyPI via <code>pip</code>, run</p> <pre><code>pip install --upgrade pip\npip install lakefs-spec\n</code></pre> <p>or, for the bleeding edge version,</p> <pre><code>pip install git+https://github.com/appliedAI-Initiative/lakefs-spec.git\n</code></pre> <p>To add the project as a dependency using <code>poetry</code>, use</p> <pre><code>poetry add lakefs-spec\n</code></pre> <p>or, for the development version,</p> <pre><code>poetry add git+https://github.com/appliedAI-Initiative/lakefs-spec.git\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>As an example showcase, we use the lakeFS file system to read a Pandas <code>DataFrame</code> directly from a branch. To follow this small tutorial, you should first complete Step 1 in the lakeFS quickstart by launching an instance, and then creating a pre-populated repository by clicking the green button on the login page.</p> <p>Then, run the following code to download the sample dataframe directly from the <code>main</code> branch:</p> <pre><code>import pandas as pd\n\n# change these settings to match your instance's address and credentials\nstorage_options={\n    \"host\": \"localhost:8000\",\n    \"username\": \"username\",\n    \"password\": \"password\",\n}\n\ndf = pd.read_parquet('lakefs://quickstart/main/lakes.parquet', storage_options=storage_options)\n</code></pre> <p>You can then update data in LakeFS like so:</p> <pre><code>df.to_parquet('lakefs://quickstart/main/lakes.parquet', storage_options=storage_options)\n</code></pre> <p>If the target file does not exist, it is created, otherwise, the existing file is updated.</p> <p>If the specified branch does not exist, it is created by default. This behaviour can be set in the filesystem constructor via the <code>create_branch_ok</code> flag.</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\n# create_branch_ok=True (the default setting) enables implicit branch creation.\nfs = LakeFSFileSystem(create_branch_ok=False)\n</code></pre> <p>If set to <code>create_branch_ok = False</code>, adressing non-existing branches causes an error. The flag can also be set in scoped filesystem behaviour changes:</p> <pre><code>with fs.scope(create_branch_ok=False):\n    fs.put(\"lakes.parquet\", 'quickstart/test/lakes.parquet')\n</code></pre> <p>This code throws an error should the <code>test</code> branch not exist.</p>"},{"location":"#paths-and-uris","title":"Paths and URIs","text":"<p>The lakeFS filesystem expects URIs that follow the lakeFS protocol. URIs need to have the form <code>lakefs://&lt;repo&gt;/&lt;ref&gt;/&lt;resource&gt;</code>, with the repository name, the ref name (either a branch name or a commit SHA, depending on the operation), and resource name. The resource can be a single file name, or a directory name for recursive operations.</p>"},{"location":"#client-side-caching","title":"Client-side caching","text":"<p>In order to reduce the number of IO operations, you can enable client-side caching of both uploaded and downloaded files. Caching works by calculating the MD5 checksum of the local file, and comparing it to that of the lakeFS remote file. If they match, the operations are cancelled, and no file up- or downloads happen.</p> <p>Client-side caching can be controlled through the boolean <code>precheck</code> argument in the <code>fs.get</code> and <code>fs_put</code> methods and their more granular single-file counterparts <code>fs.get_file</code> and <code>fs.put_file</code>.</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem()\n\n# The default is precheck=True, you can force the operation by setting precheck=False.\nfs.get_file(\"my-repo/my-ref/file.txt\", \"file.txt\", precheck=True)\n</code></pre>"},{"location":"#creating-lakefs-automations-in-python-with-lakefsfilesystem-hooks","title":"Creating lakeFS automations in Python with <code>LakeFSFileSystem</code> hooks","text":"<p>LakeFS has a variety of administrative APIs available through its Python client library. Within <code>lakefs-spec</code>, you can register hooks to your <code>LakeFSFileSystem</code> to run code after file system operations. A hook needs to have the signature <code>(client, context) -&gt; None</code>, where the <code>client</code> argument holds the file system's lakeFS API client, and the <code>context</code> object contains information about the requested resource (repository, ref/branch, name).</p> <p>As an example, the following snippet installs a lakeFS hook that creates a commit on the lakeFS branch after a file upload:</p> <pre><code>from lakefs_sdk.client import LakeFSClient\n\nfrom lakefs_spec import LakeFSFileSystem\nfrom lakefs_spec.client_helpers import commit\nfrom lakefs_spec.hooks import FSEvent, HookContext\n\ndef create_commit_on_put(client: LakeFSClient, ctx: HookContext) -&gt; None:\n    message = f\"Add file {ctx.resource}\"\n    commit(client, repository=ctx.repository, branch=ctx.ref, message=message)\n\nfs = LakeFSFileSystem()\n\nfs.register_hook(FSEvent.PUT_FILE, create_commit_on_put)\n\n# creates a commit with the message \"Add file my-file.txt\" after the file put.\nfs.put_file(\"my-file.txt\", \"my-repo/my-branch/my-file.txt\")\n</code></pre>"},{"location":"#implicit-initialization-and-instance-caching","title":"Implicit initialization and instance caching","text":"<p>Aside from explicit initialization, you can also use environment variables and a configuration file (by default <code>~/.lakectl.yaml</code>) to initialize a lakeFS file system. The environment variables for the lakeFS client arguments are the names of the constructor arguments prefixed with <code>LAKEFS_</code>:</p> <pre><code>import os\nfrom lakefs_spec import LakeFSFileSystem\n\nos.environ[\"LAKEFS_HOST\"] = \"localhost:8000\"\nos.environ[\"LAKEFS_USERNAME\"] = \"username\"\nos.environ[\"LAKEFS_PASSWORD\"] = \"password\"\n\nfs = LakeFSFileSystem()\n</code></pre> <p>To initialize the lakeFS file system from a <code>lakectl</code> YAML configuration file, you can specify the <code>configfile</code> argument.</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\n# No argument means the default config (~/.lakectl.yaml) will be used.\nfs = LakeFSFileSystem(configfile=\"path/to/my/lakectl.yaml\")\n</code></pre> <p>\u26a0\ufe0f To be able to read settings from a YAML configuration file, <code>pyyaml</code> has to be installed, for example using <code>pip</code>:</p> <pre><code>pip install --upgrade pyyaml\n</code></pre>"},{"location":"#a-note-on-mixing-environment-variables-and-lakectl-configuration-files","title":"A note on mixing environment variables and <code>lakectl</code> configuration files","text":"<p>lakeFS file system instances are cached, and existing lakeFS instances are reused from an instance cache when requested.</p> <p>For implicit initialization from environment variables and configuration files as described above, this means that whichever initialization method is used first populates the cache - thus, when using the other method, a cache hit happens and no new instance is created. This can lead to surprising misconfigurations:</p> <pre><code>import os\nfrom lakefs_spec import LakeFSFileSystem\n\n# set envvars\nos.environ[\"LAKEFS_HOST\"] = \"localhost:8000\"\nos.environ[\"LAKEFS_USERNAME\"] = \"username\"\nos.environ[\"LAKEFS_PASSWORD\"] = \"password\"\n\n# creates a cache entry for the bare instance\nfs = LakeFSFileSystem()\n\n# ~/.lakectl.yaml\n#  server:\n#    endpoint_url: http://example-host\n\n# this time, try to read in the default lakectl config, with http://example-host set as host.\nfs = LakeFSFileSystem()\nprint(fs.client._api.configuration.host) # &lt;- prints localhost:8000!\n</code></pre> <p>The best way to avoid this is to commit to only using either environment variables or <code>lakectl</code> configuration files. If you do have to mix both methods, you can clear the instance cache like so:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nLakeFSFileSystem.clear_instance_cache()\n</code></pre>"},{"location":"#reproducibility-through-data-versioning-with-lakefs-and-lakefs-spec","title":"Reproducibility through data versioning with lakeFS and lakeFS-spec","text":"<p>Here we briefly show an example how data versioning for the reproducibility of machine learning experiments can be achieved using lakeFS-spec. We do this via python-like pseudocode.</p> <p>First, we ingest the data into our versioning system.</p> <pre><code># Data ingestion\nimport pandas as pd\nfrom lakefs_spec.client_helpers import commit\n\nraw = pd.read_csv('local-path-to.csv')\nraw.to_csv('lakefs://&lt;lakeFS-uri')\ncommit('raw data ingestion')\n</code></pre> <p>This commit function creates a commit with a unique SHA which you can get from the lakeFS user interface or withthe <code>lakefs_spec.client_helpers.get_tags</code> function.</p> <p>The SHA points to a specific state of the dataset.</p> <pre><code>raw_df = pd.read_csv('lakefs://&lt;lakeFS-commit-SHA&gt;')\nprep_df = preprocess(raw_df)\ntrained_model = model.fit(prep_df)\nacc = trained_model.eval()\n\nexperiment_tracking.log('Data version','lakefs://&lt;lakeFS-commit-SHA&gt;')\nexperiment_tracking.log('Code version','&lt;git commit SHA of this code&gt;')\nexperiment_tracking.log('Accuracy', acc)\n</code></pre> <p>Now, with the data and code version identified by a specific commit sha you will always reproduce the same experiment outcomes, e.g. <code>acc</code>.</p>"},{"location":"#developing-and-contributing-to-lakefs-spec","title":"Developing and contributing to <code>lakefs-spec</code>","text":"<p>We welcome contributions to the project! For information on the general development workflow, head over to the contribution guide.</p>"},{"location":"CONTRIBUTING/","title":"Developing on <code>lakefs-spec</code>","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#quickstart","title":"Quickstart","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/appliedAI-Initiative/lakefs-spec.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment:</p> <pre><code>cd lakefs-spec\npython3 -m venv venv --system-site-packages\nsource venv/bin/activate\npython -m pip install -r requirements.txt -r requirements-dev.txt\npython -m pip install -e . --no-deps\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests against an ephemeral lakeFS instance, you just run <code>pytest</code>:     <pre><code>pytest\n</code></pre></p> <p>To spin up a local lakeFS instance quickly for testing, you can use the Docker Compose file bundled with this repository:</p> <pre><code>docker-compose -f hack/docker-compose.yml up\n</code></pre> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.optional-dependencies</code> table instead.</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.optional-dependencies</code>.</li> </ol> <p>After adding the dependency in either of these sections, run the helper script <code>hack/lock-deps.sh</code> (which in turn uses <code>pip-compile</code>) to pin all dependencies again:</p> <pre><code>python -m pip install --upgrade pip-tools\nhack/lock-deps.sh\n</code></pre> <p>In addition to these manual steps, we also provide <code>pre-commit</code> hooks that automatically lock the dependencies whenever <code>pyproject.toml</code> is changed.</p> <p>Selective package upgrade for existing dependencies are also handled by the helper script above. If you want to update the <code>lakefs-sdk</code> dependency, for example, simply run:</p> <pre><code>hack/lock-deps.sh lakefs-sdk\n</code></pre> <p>\u26a0\ufe0f Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"CONTRIBUTING/#working-on-documentation","title":"Working on Documentation","text":"<p>Improvements or additions to the project's documentation are highly appreciated.</p> <p>The documentation is based on the <code>mkdocs</code> and <code>mkdocs-material</code> projects, see their homepages for in-depth guides on their features and usage. We use the Numpy documentation style for Python docstrings.</p> <p>To build the documentation locally, you need to first install the optional <code>docs</code> dependencies from <code>requirements-docs.txt</code>, e.g., with <code>pip install -r requirements-docs.txt</code>. You can then start a local documentation server with <code>mkdocs serve</code>, or build the documentation into its output folder in <code>public/</code>.</p> <p>In order to maintain documentation for multiple versions of this library, we use the <code>mike</code> tool, which automatically maintains individual documentation builds per version and publishes them to the <code>gh-pages</code> branch.</p> <p>The GitHub CI pipeline automatically invokes <code>mike</code> as part of the release process with the correct version and updates the GitHub pages branch for the project.</p>"},{"location":"use-cases/","title":"Why should you use <code>lakefs-spec</code>?","text":"<p>There are a few reasons that come to mind:</p>"},{"location":"use-cases/#file-system-operations-client-interactions","title":"File system operations <code>&gt;&gt;</code> client interactions","text":"<p>Before, the only way to interface with a lakeFS instance was to use the Python API client. While it has its merits, and (mostly) works as advertised, it has its issues - the developer experience being the most difficult one, with incomplete typing information, and the woes of reading lots of raw client code, for example when downloading a file to load into a data frame:</p> <pre><code>from lakefs_sdk import Configuration\nfrom lakefs_sdk.client import LakeFSClient\n\n# painful!\nconfiguration = Configuration(host=\"my-host\")\nclient = LakeFSClient(configuration=configuration)\n\nrepository = \"my-repo\"\nref = \"my-ref\"\ndata = \"data.parquet\"\n\nwith open(\"res.parquet\", \"w\") as f:\n    res = client.objects_api.get_object(repository, ref, data)\n    while True:\n        chunk = res.read(2 ** 20)\n        if not chunk:\n            break\n        f.write(chunk)\n</code></pre> <p>When comparing this to <code>lakefs-spec</code>, we see that approaching the problem from a file system point of view makes things much easier:</p> <pre><code>from lakefs_spec import LakeFSFileSystem\n\nfs = LakeFSFileSystem(host=\"my-host\")\n\nfs.get(\"my-repo/my-ref/data.parquet\", \"res.parquet\")\n</code></pre> <p>These abstractions make the code needed to interact with files and specific versions of resources much more straightforward.</p>"},{"location":"use-cases/#integration-with-fsspec-powered-python-data-science-libraries","title":"Integration with <code>fsspec</code>-powered Python data science libraries","text":"<p>Some projects, including <code>pandas</code>, <code>polars</code>, and <code>pyspark</code>, use <code>fsspec</code> to abstract I/O into a file-system paradigm. Registering other file systems is as easy as running a hook in <code>setuptools</code> (or <code>poetry</code>, depending on your preference) at build time.</p> <p><code>lakefs-spec</code> itself is registered as the default file system for the <code>lakefs</code> protocol in the official <code>fsspec</code> registry, so all you have to do is to <code>pip install --upgrade lakefs-spec</code> to start using it.</p> <p>With it, reading and writing data in a versioned manner is supported natively, for example in data frame I/O in <code>pandas</code>:</p> <pre><code>import pandas as pd\n\nstorage_options={\n    \"host\": \"localhost:8000\",\n    \"username\": \"username\",\n    \"password\": \"password\",\n}\n\ndf = pd.read_parquet('lakefs://quickstart/main/lakes.parquet', storage_options=storage_options)\n</code></pre> <p>With more advanced methods in <code>lakefs-spec</code> such as config file instantiation and environment variable setting, you can even instantiate the file system without any arguments and rely on automated client configuration and setup.</p>"},{"location":"use-cases/#setting-up-lakefs-automations-after-file-operations","title":"Setting up lakeFS automations after file operations","text":"<p>As described earlier, dealing with the raw lakeFS API client in interactions with a lakeFS instance can be cumbersome, for reasons like weirdnesses in autogenerated classes and docstrings, API models instead of raw Python types, and unintuitive keyword arguments.</p> <p>Using <code>lakefs-spec</code> you can interact with your lakeFS instance in a file-centric way. Each file operation is mapped to one (or more) client API calls, which are completely abstracted away from the user and hidden behind the standard <code>fsspec</code> APIs, which allow for accurate typing.</p> <p>Since lakeFS is a data versioning tool and not only an object store, you should be able to carry out its versioning operations after file operations. <code>lakefs-spec</code> solves this by exposing a hook mechanism to optionally register and run user code after each completed file system method.</p> <p>A concrete example of this is creating lakeFS commits after a successful file upload. Commits provide a snapshot of a lakeFS repository, similarly to version control systems like git.</p> <pre><code>from lakefs_sdk.client import LakeFSClient\n\nfrom lakefs_spec import LakeFSFileSystem\nfrom lakefs_spec.client_helpers import commit\nfrom lakefs_spec.hooks import FSEvent, HookContext\n\ndef create_commit_on_put(client: LakeFSClient, ctx: HookContext) -&gt; None:\n    message = f\"Add file {ctx.resource}\"\n    commit(client, repository=ctx.repository, branch=ctx.ref, message=message)\n\nfs = LakeFSFileSystem()\n\nfs.register_hook(FSEvent.PUT_FILE, create_commit_on_put)\n\n# creates a commit with the message \"Add file my-file.txt\" after the file put.\nfs.put_file(\"my-file.txt\", \"my-repo/my-branch/my-file.txt\")\n</code></pre> <p>The other major convenience provided by <code>lakefs-spec</code> for creating lakeFS automations is the <code>lakefs_spec.client_helpers</code> module, which contains useful lakeFS operations (creating commits etc.) implemented as Python functions, and can be used as building blocks for hooks.</p>"},{"location":"guides/overview/","title":"Overview","text":"<p>TODO</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>lakefs_spec<ul> <li>client_helpers</li> <li>config</li> <li>errors</li> <li>hooks</li> <li>spec</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/lakefs_spec/","title":"lakefs_spec","text":"<p>lakefs-spec is an fsspec file system integration for the lakeFS data lake.</p>"},{"location":"reference/lakefs_spec/client_helpers/","title":"client_helpers","text":""},{"location":"reference/lakefs_spec/client_helpers/#lakefs_spec.client_helpers.revert","title":"revert","text":"<pre><code>revert(client, repository, branch, parent_number=1)\n</code></pre> <p>Reverts the commit on the specified branch to the parent specified by parent_number.</p> PARAMETER  DESCRIPTION <code>client</code> <p>The client to interact with.</p> <p> TYPE: <code>LakeFSClient</code> </p> <code>repository</code> <p>Repository in which the specified branch is located.</p> <p> TYPE: <code>str</code> </p> <code>branch</code> <p>Branch on which the commit should be reverted.</p> <p> TYPE: <code>str</code> </p> <code>parent_number</code> <p>If there are multiple parents to a commit, specify to which parent the commit should be reverted. Index starts at 1. Defaults to 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/lakefs_spec/client_helpers.py</code> <pre><code>def revert(client: LakeFSClient, repository: str, branch: str, parent_number: int = 1) -&gt; None:\n    \"\"\"Reverts the commit on the specified branch to the parent specified by parent_number.\n\n    Parameters\n    ----------\n    client: LakeFSClient\n        The client to interact with.\n    repository: str\n        Repository in which the specified branch is located.\n    branch: str\n        Branch on which the commit should be reverted.\n    parent_number: int, optional\n        If there are multiple parents to a commit, specify to which parent the commit should be reverted. Index starts at 1. Defaults to 1.\n    \"\"\"\n    revert_creation = RevertCreation(ref=branch, parent_number=parent_number)\n    client.branches_api.revert_branch(\n        repository=repository, branch=branch, revert_creation=revert_creation\n    )\n</code></pre>"},{"location":"reference/lakefs_spec/config/","title":"config","text":""},{"location":"reference/lakefs_spec/errors/","title":"errors","text":""},{"location":"reference/lakefs_spec/errors/#lakefs_spec.errors.translate_lakefs_error","title":"translate_lakefs_error","text":"<pre><code>translate_lakefs_error(error, message=None, set_cause=True, *args)\n</code></pre> <p>Convert a lakeFS client ApiException into a Python exception.</p> PARAMETER  DESCRIPTION <code>error</code> <p>The exception returned by the lakeFS API.</p> <p> TYPE: <code>ApiException</code> </p> <code>message</code> <p>An error message to use for the returned exception. If not given, the error message returned by the lakeFS server is used instead.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>set_cause</code> <p>Whether to set the <code>__cause__</code> attribute to the previous exception if the exception is translated.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>*args</code> <p>Additional arguments to pass to the exception constructor, after the error message. Useful for passing the filename arguments to <code>IOError</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>OSError</code> <p>An instantiated exception ready to be thrown. If the error code isn't recognized, an <code>IOError</code> with the original error message is returned.</p> Source code in <code>src/lakefs_spec/errors.py</code> <pre><code>def translate_lakefs_error(\n    error: ApiException,\n    message: str | None = None,\n    set_cause: bool = True,\n    *args: Any,\n) -&gt; OSError:\n    \"\"\"Convert a lakeFS client ApiException into a Python exception.\n\n    Parameters\n    ----------\n\n    error : lakefs_client.ApiException\n        The exception returned by the lakeFS API.\n    message : str\n        An error message to use for the returned exception. If not given, the\n        error message returned by the lakeFS server is used instead.\n    set_cause : bool\n        Whether to set the ``__cause__`` attribute to the previous exception if the\n        exception is translated.\n    *args:\n        Additional arguments to pass to the exception constructor, after the\n        error message. Useful for passing the filename arguments to ``IOError``.\n\n    Returns\n    -------\n    OSError\n        An instantiated exception ready to be thrown. If the error code isn't\n        recognized, an ``IOError`` with the original error message is returned.\n    \"\"\"\n    status, reason, body = error.status, error.reason, error.body\n\n    emsg = f\"HTTP{status} ({reason})\"\n    try:\n        lakefs_msg = json.loads(body)[\"message\"]\n        emsg += f\": {lakefs_msg}\"\n    except json.JSONDecodeError:\n        pass\n\n    constructor = HTTP_CODE_TO_ERROR.get(status, functools.partial(IOError, errno.EIO))\n    custom_exc = constructor(message or emsg, *args)\n    if set_cause:\n        custom_exc.__cause__ = error\n    return custom_exc\n</code></pre>"},{"location":"reference/lakefs_spec/hooks/","title":"hooks","text":""},{"location":"reference/lakefs_spec/hooks/#lakefs_spec.hooks.LakeFSHook","title":"LakeFSHook  <code>module-attribute</code>","text":"<pre><code>LakeFSHook = Callable[[LakeFSClient, HookContext], None]\n</code></pre> <p>A hook to execute after a completed file operation. Input arguments are the lakeFS file system's client, and a context object containing repository, branch name, and (remote) resource name.</p>"},{"location":"reference/lakefs_spec/spec/","title":"spec","text":""},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFileSystem","title":"LakeFSFileSystem","text":"<p>             Bases: <code>AbstractFileSystem</code></p> <p>lakeFS file system implementation.</p> <p>The client is immutable in this implementation, so different users need different file systems.</p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>class LakeFSFileSystem(AbstractFileSystem):\n    \"\"\"\n    lakeFS file system implementation.\n\n    The client is immutable in this implementation, so different users need different\n    file systems.\n    \"\"\"\n\n    protocol = \"lakefs\"\n\n    def __init__(\n        self,\n        host: str | None = None,\n        username: str | None = None,\n        password: str | None = None,\n        api_key: str | None = None,\n        api_key_prefix: str | None = None,\n        access_token: str | None = None,\n        verify_ssl: bool = True,\n        ssl_ca_cert: str | None = None,\n        proxy: str | None = None,\n        configfile: str = \"~/.lakectl.yaml\",\n        create_branch_ok: bool = True,\n        source_branch: str = \"main\",\n        **storage_options: Any,\n    ):\n        \"\"\"\n        The LakeFS file system constructor.\n\n        Parameters\n        ----------\n        host: str or None\n            The address of your lakeFS instance.\n        username: str or None\n            The access key name to use in case of access key authentication.\n        password: str or None\n            The access key secret to use in case of access key authentication.\n        api_key: str or None\n            The API key to use in case of authentication with an API key.\n        api_key_prefix: str or None\n            A string prefix to use for the API key in authentication.\n        access_token: str or None\n            An access token to use in case of access token authentication.\n        verify_ssl: bool\n            Whether to verify SSL certificates in API interactions. Do not disable in production.\n        ssl_ca_cert: str or None\n            A custom certificate PEM file to use to verify the peer in SSL connections.\n        proxy: str or None\n            Proxy address to use when connecting to a lakeFS server.\n        create_branch_ok: bool\n            Whether to create branches implicitly when not-existing branches are referenced on file uploads.\n        source_branch: str\n            Source branch set as origin when a new branch is implicitly created.\n        storage_options: Any\n            Configuration options to pass to the file system's directory cache.\n        \"\"\"\n        super().__init__(**storage_options)\n\n        if (p := Path(configfile).expanduser()).exists():\n            lakectl_config = LakectlConfig.read(p)\n        else:\n            # empty config.\n            lakectl_config = LakectlConfig()\n\n        configuration = Configuration(\n            host=host or os.getenv(\"LAKEFS_HOST\") or lakectl_config.host,\n            api_key=api_key or os.getenv(\"LAKEFS_API_KEY\"),\n            api_key_prefix=api_key_prefix or os.getenv(\"LAKEFS_API_KEY_PREFIX\"),\n            access_token=access_token or os.getenv(\"LAKEFS_ACCESS_TOKEN\"),\n            username=username or os.getenv(\"LAKEFS_USERNAME\") or lakectl_config.username,\n            password=password or os.getenv(\"LAKEFS_PASSWORD\") or lakectl_config.password,\n            ssl_ca_cert=ssl_ca_cert or os.getenv(\"LAKEFS_SSL_CA_CERT\"),\n        )\n        # proxy address, not part of the constructor\n        configuration.proxy = proxy\n        # whether to verify SSL certs, not part of the constructor\n        configuration.verify_ssl = verify_ssl\n\n        self.client = LakeFSClient(configuration=configuration)\n        self.create_branch_ok = create_branch_ok\n        self.source_branch = source_branch\n\n        self._hooks: dict[FSEvent, LakeFSHook] = {}\n\n    def register_hook(self, fsevent: str, hook: LakeFSHook, clobber: bool = False) -&gt; None:\n        fsevent = FSEvent.canonicalize(fsevent)\n        if not clobber and fsevent in self._hooks:\n            raise RuntimeError(\n                f\"hook already registered for file system event '{str(fsevent)}'. \"\n                f\"To force registration, rerun with `clobber=True`.\"\n            )\n        self._hooks[fsevent] = hook\n\n    def deregister_hook(self, fsevent: str) -&gt; None:\n        self._hooks.pop(FSEvent.canonicalize(fsevent), None)\n\n    def run_hook(self, fsevent: str, ctx: HookContext) -&gt; None:\n        hook = self._hooks.get(FSEvent.canonicalize(fsevent), noop)\n        hook(self.client, ctx)\n\n    @classmethod\n    @overload\n    def _strip_protocol(cls, path: str | os.PathLike[str] | Path) -&gt; str:\n        ...\n\n    @classmethod\n    @overload\n    def _strip_protocol(cls, path: list[str | os.PathLike[str] | Path]) -&gt; list[str]:\n        ...\n\n    @classmethod\n    def _strip_protocol(cls, path):\n        \"\"\"Copied verbatim from the base class, save for the slash rstrip.\"\"\"\n        if isinstance(path, list):\n            return [cls._strip_protocol(p) for p in path]\n        spath = super()._strip_protocol(path)\n        if stringify_path(path).endswith(\"/\"):\n            return spath + \"/\"\n        return spath\n\n    @contextmanager\n    def wrapped_api_call(self, message: str | None = None, set_cause: bool = True) -&gt; EmptyYield:\n        try:\n            yield\n        except ApiException as e:\n            raise translate_lakefs_error(e, message=message, set_cause=set_cause)\n\n    @contextmanager\n    def scope(\n        self,\n        create_branch_ok: bool | None = None,\n        source_branch: str | None = None,\n        disable_hooks: bool = False,\n    ) -&gt; EmptyYield:\n        \"\"\"\n        A context manager yielding scope in which the lakeFS file system behavior\n        is changed from defaults.\n        \"\"\"\n        curr_create_branch_ok, curr_source_branch, curr_hooks = (\n            self.create_branch_ok,\n            self.source_branch,\n            self._hooks,\n        )\n        try:\n            if create_branch_ok is not None:\n                self.create_branch_ok = create_branch_ok\n            if source_branch is not None:\n                self.source_branch = source_branch\n            if disable_hooks:\n                self._hooks = {}\n            yield\n        finally:\n            self.create_branch_ok = curr_create_branch_ok\n            self.source_branch = curr_source_branch\n            self._hooks = curr_hooks\n\n    def checksum(self, path: str) -&gt; str | None:\n        try:\n            checksum = self.info(path).get(\"checksum\", None)\n        except FileNotFoundError:\n            checksum = None\n\n        self.run_hook(FSEvent.CHECKSUM, HookContext.new(path))\n        return checksum\n\n    def exists(self, path: str, **kwargs: Any) -&gt; bool:\n        repository, ref, resource = parse(path)\n\n        exists = False\n        with self.wrapped_api_call():\n            try:\n                self.client.objects_api.head_object(repository, ref, resource, **kwargs)\n                exists = True\n            except NotFoundException:\n                pass\n            except ApiException as e:\n                # in case of an error other than \"not found\", existence cannot be\n                # decided, so raise the translated error.\n                raise translate_lakefs_error(e)\n            finally:\n                ctx = HookContext(repository=repository, ref=ref, resource=resource)\n                self.run_hook(FSEvent.EXISTS, ctx)\n                return exists\n\n    def cp_file(self, path1: str, path2: str, **kwargs: Any) -&gt; None:\n        if path1 == path2:\n            return\n\n        orig_repo, orig_ref, orig_path = parse(path1)\n        dest_repo, dest_ref, dest_path = parse(path2)\n\n        if orig_repo != dest_repo:\n            raise ValueError(\n                \"can only copy objects within a repository, but got source \"\n                f\"repository {orig_repo!r} and destination repository \"\n                f\"{dest_repo!r}\"\n            )\n\n        with self.wrapped_api_call():\n            object_copy_creation = ObjectCopyCreation(src_path=orig_path, src_ref=orig_ref)\n            self.client.objects_api.copy_object(\n                repository=dest_repo,\n                branch=dest_ref,\n                dest_path=dest_path,\n                object_copy_creation=object_copy_creation,\n                **kwargs,\n            )\n\n        self.run_hook(FSEvent.CP_FILE, HookContext.new(path1))\n\n    def get(\n        self,\n        rpath: str,\n        lpath: str,\n        recursive: bool = False,\n        callback: Callback = _DEFAULT_CALLBACK,\n        maxdepth: int = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().get(\n            rpath, lpath, recursive=recursive, callback=callback, maxdepth=maxdepth, **kwargs\n        )\n        self.run_hook(FSEvent.GET, HookContext.new(rpath))\n\n    def get_file(\n        self,\n        rpath: str,\n        lpath: str,\n        callback: Callback = _DEFAULT_CALLBACK,\n        outfile: Any = None,\n        precheck: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        repository, ref, resource = parse(rpath)\n\n        def run_get_file_hook():\n            ctx = HookContext(repository=repository, ref=ref, resource=resource)\n            self.run_hook(FSEvent.GET_FILE, ctx)\n\n        info = self.info(rpath)\n\n        if precheck and Path(lpath).exists():\n            local_checksum = md5_checksum(lpath, blocksize=self.blocksize)\n            remote_checksum = info.get(\"checksum\")\n            if local_checksum == remote_checksum:\n                logger.info(\n                    f\"Skipping download of resource {rpath!r} to local path {lpath!r}: \"\n                    f\"Resource {lpath!r} exists and checksums match.\"\n                )\n                run_get_file_hook()\n                return\n\n        filesize = info[\"size\"]\n        callback.set_size(filesize)\n\n        if isfilelike(lpath):\n            outfile = lpath\n        else:\n            outfile = open(lpath, \"wb\")\n\n        try:\n            offset = 0\n            while True:\n                next_offset = max(offset + self.blocksize, filesize)\n                byterange = f\"bytes={offset}-{next_offset - 1}\"\n\n                chunk = self.client.objects_api.get_object(\n                    repository, ref, resource, range=byterange, **kwargs\n                )\n                res = outfile.write(chunk)\n                callback.relative_update(res)\n\n                offset += self.blocksize\n                if next_offset &gt;= filesize:\n                    break\n        except ApiException as e:\n            from fsspec.implementations.local import LocalFileSystem\n\n            LocalFileSystem().rm_file(lpath)\n            raise translate_lakefs_error(e)\n        finally:\n            if not isfilelike(lpath):\n                outfile.close()\n            run_get_file_hook()\n\n    def info(self, path: str, **kwargs: Any) -&gt; dict[str, Any]:\n        path = self._strip_protocol(path)\n\n        # input path is a directory name\n        if path.endswith(\"/\"):\n            out = self.ls(path, detail=True, **kwargs)\n            if not out:\n                raise FileNotFoundError(path)\n\n            resource = path.split(\"/\", maxsplit=2)[-1]\n            statobj = {\n                \"name\": resource,\n                \"size\": sum(o.get(\"size\", 0) for o in out),\n                \"type\": \"directory\",\n            }\n        # input path is a file name\n        else:\n            with self.wrapped_api_call():\n                repository, ref, resource = parse(path)\n                res = self.client.objects_api.stat_object(\n                    repository=repository, ref=ref, path=resource, **kwargs\n                )\n\n            statobj = {\n                \"checksum\": res.checksum,\n                \"content-type\": res.content_type,\n                \"mtime\": res.mtime,\n                \"name\": res.path,\n                \"size\": res.size_bytes,\n                \"type\": \"file\",\n            }\n\n        self.run_hook(FSEvent.INFO, HookContext.new(path))\n        return statobj\n\n    def ls(self, path: str, detail: bool = True, **kwargs: Any) -&gt; list:\n        path = self._strip_protocol(path)\n        repository, ref, prefix = parse(path)\n\n        try:\n            cache_entry: list[Any] | None = self._ls_from_cache(prefix)\n        except FileNotFoundError:\n            # we patch files missing from an ls call in the cache entry below,\n            # so this should not be an error.\n            cache_entry = None\n\n        if cache_entry is not None:\n            if not detail:\n                return [e[\"name\"] for e in cache_entry]\n            return cache_entry\n\n        has_more, after = True, \"\"\n        # stat infos are either the path only (`detail=False`) or a dict full of metadata\n        info: list[Any] = []\n\n        with self.wrapped_api_call():\n            while has_more:\n                res: ObjectStatsList = self.client.objects_api.list_objects(\n                    repository, ref, after=after, prefix=prefix, **kwargs\n                )\n                has_more, after = res.pagination.has_more, res.pagination.next_offset\n                for obj in res.results:\n                    info.append(\n                        {\n                            \"checksum\": obj.checksum,\n                            \"content-type\": obj.content_type,\n                            \"mtime\": obj.mtime,\n                            \"name\": obj.path,\n                            \"size\": obj.size_bytes,\n                            \"type\": \"file\",\n                        }\n                    )\n\n        # cache the info if not empty.\n        if info:\n            # assumes that the returned info is name-sorted.\n            pp = self._parent(info[0][\"name\"])\n            if pp in self.dircache:\n                # ls info has files not in cache, so we update them in the cache entry.\n                cache_entry = self.dircache[pp]\n                # extend the entry by the new ls results\n                cache_entry.extend(info)\n                self.dircache[pp] = sorted(cache_entry, key=operator.itemgetter(\"name\"))\n            else:\n                self.dircache[pp] = info\n\n        if not detail:\n            info = [o[\"name\"] for o in info]\n\n        ctx = HookContext(repository=repository, ref=ref, resource=prefix)\n        self.run_hook(FSEvent.LS, ctx)\n\n        return info\n\n    def _open(\n        self,\n        path: str,\n        mode: Literal[\"rb\", \"wb\"] = \"rb\",\n        block_size: int | None = None,\n        autocommit: bool = True,\n        cache_options: dict[str, str] | None = None,\n        **kwargs: Any,\n    ) -&gt; LakeFSFile:\n        if mode not in {\"rb\", \"wb\"}:\n            raise NotImplementedError(f\"unsupported mode {mode!r}\")\n\n        return LakeFSFile(\n            self,\n            path=path,\n            mode=mode,\n            block_size=block_size or self.blocksize,\n            autocommit=autocommit,\n            cache_options=cache_options,\n            **kwargs,\n        )\n\n    def put_file_to_blockstore(\n        self,\n        lpath: str,\n        repository: str,\n        branch: str,\n        resource: str,\n        callback: Callback = _DEFAULT_CALLBACK,\n        presign: bool = False,\n        storage_options: dict[str, Any] | None = None,\n    ) -&gt; None:\n        staging_location = self.client.staging_api.get_physical_address(\n            repository, branch, resource, presign=presign\n        )\n\n        if presign:\n            remote_url = staging_location.presigned_url\n            content_type, _ = mimetypes.guess_type(lpath)\n            if content_type is None:\n                content_type = \"application/octet-stream\"\n            with open(lpath, \"rb\") as f:\n                headers = {\n                    \"Content-Type\": content_type,\n                }\n                request = urllib.request.Request(\n                    url=remote_url, data=f, headers=headers, method=\"PUT\"\n                )\n                try:\n                    if not remote_url.lower().startswith(\"http\"):\n                        raise ValueError(\"Wrong protocol for remote connection\")\n                    else:\n                        logger.info(f\"Begin upload of {lpath}\")\n                        with urllib.request.urlopen(\n                            request\n                        ):  # nosec [B310:blacklist] # We catch faulty protocols above.\n                            logger.info(f\"Successfully uploaded {lpath}\")\n                except urllib.error.HTTPError as e:\n                    urllib_http_error_as_lakefs_api_exception = ApiException(\n                        status=e.code, reason=e.reason\n                    )\n                    translate_lakefs_error(error=urllib_http_error_as_lakefs_api_exception)\n        else:\n            blockstore_type = self.client.config_api.get_config().storage_config.blockstore_type\n            # lakeFS blockstore name is \"azure\", but Azure's fsspec registry entry is \"az\".\n            if blockstore_type == \"azure\":\n                blockstore_type = \"az\"\n\n            if blockstore_type not in [\"s3\", \"gs\", \"az\"]:\n                raise ValueError(\n                    f\"Blockstore writes are not implemented for blockstore type {blockstore_type!r}\"\n                )\n\n            remote_url = staging_location.physical_address\n            remote = filesystem(blockstore_type, **(storage_options or {}))\n            remote.put_file(lpath, remote_url, callback=callback)\n\n        staging_metadata = StagingMetadata(\n            staging=staging_location,\n            checksum=md5_checksum(lpath, blocksize=self.blocksize),\n            size_bytes=os.path.getsize(lpath),\n        )\n        self.client.staging_api.link_physical_address(\n            repository, branch, resource, staging_metadata\n        )\n\n    def put_file(\n        self,\n        lpath: str,\n        rpath: str,\n        callback: Callback = _DEFAULT_CALLBACK,\n        precheck: bool = True,\n        use_blockstore: bool = False,\n        presign: bool = False,\n        storage_options: dict[str, Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        repository, branch, resource = parse(rpath)\n\n        def run_put_file_hook():\n            ctx = HookContext(repository=repository, ref=branch, resource=resource)\n            self.run_hook(FSEvent.PUT_FILE, ctx)\n\n        if precheck:\n            remote_checksum = self.checksum(rpath)\n            local_checksum = md5_checksum(lpath, blocksize=self.blocksize)\n            if local_checksum == remote_checksum:\n                logger.info(\n                    f\"Skipping upload of resource {lpath!r} to remote path {rpath!r}: \"\n                    f\"Resource {rpath!r} exists and checksums match.\"\n                )\n                run_put_file_hook()\n                return\n\n        if use_blockstore:\n            self.put_file_to_blockstore(\n                lpath,\n                repository,\n                branch,\n                resource,\n                presign=presign,\n                callback=callback,\n                storage_options=storage_options,\n            )\n        else:\n            size = Path(lpath).stat().st_size\n            callback.set_size(size)\n\n            with self.wrapped_api_call():\n                self.client.objects_api.upload_object(\n                    repository=repository, branch=branch, path=resource, content=lpath, **kwargs\n                )\n\n            # this is stupid, but the best we can do without multipart uploads.\n            callback.relative_update(size)\n\n        run_put_file_hook()\n\n    def put(\n        self,\n        lpath: str,\n        rpath: str,\n        recursive: bool = False,\n        callback: Callback = _DEFAULT_CALLBACK,\n        maxdepth: int | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        repository, branch, resource = parse(rpath)\n        if self.create_branch_ok:\n            ensure_branch(self.client, repository, branch, self.source_branch)\n\n        super().put(\n            lpath, rpath, recursive=recursive, callback=callback, maxdepth=maxdepth, **kwargs\n        )\n\n        ctx = HookContext(repository=repository, ref=branch, resource=resource)\n        self.run_hook(FSEvent.PUT, ctx)\n\n    def rm_file(self, path: str) -&gt; None:\n        repository, branch, resource = parse(path)\n\n        with self.wrapped_api_call():\n            self.client.objects_api.delete_object(\n                repository=repository, branch=branch, path=resource\n            )\n\n        ctx = HookContext(repository=repository, ref=branch, resource=resource)\n        self.run_hook(FSEvent.RM_FILE, ctx)\n\n    def rm(self, path: str, recursive: bool = False, maxdepth: int | None = None) -&gt; None:\n        super().rm(path, recursive=recursive, maxdepth=maxdepth)\n\n        self.run_hook(FSEvent.RM, HookContext.new(path))\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFileSystem.__init__","title":"__init__","text":"<pre><code>__init__(\n    host=None,\n    username=None,\n    password=None,\n    api_key=None,\n    api_key_prefix=None,\n    access_token=None,\n    verify_ssl=True,\n    ssl_ca_cert=None,\n    proxy=None,\n    configfile=\"~/.lakectl.yaml\",\n    create_branch_ok=True,\n    source_branch=\"main\",\n    **storage_options\n)\n</code></pre> <p>The LakeFS file system constructor.</p> PARAMETER  DESCRIPTION <code>host</code> <p>The address of your lakeFS instance.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>username</code> <p>The access key name to use in case of access key authentication.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>password</code> <p>The access key secret to use in case of access key authentication.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>api_key</code> <p>The API key to use in case of authentication with an API key.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>api_key_prefix</code> <p>A string prefix to use for the API key in authentication.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>access_token</code> <p>An access token to use in case of access token authentication.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>verify_ssl</code> <p>Whether to verify SSL certificates in API interactions. Do not disable in production.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ssl_ca_cert</code> <p>A custom certificate PEM file to use to verify the peer in SSL connections.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>proxy</code> <p>Proxy address to use when connecting to a lakeFS server.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>create_branch_ok</code> <p>Whether to create branches implicitly when not-existing branches are referenced on file uploads.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>source_branch</code> <p>Source branch set as origin when a new branch is implicitly created.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'main'</code> </p> <code>storage_options</code> <p>Configuration options to pass to the file system's directory cache.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>def __init__(\n    self,\n    host: str | None = None,\n    username: str | None = None,\n    password: str | None = None,\n    api_key: str | None = None,\n    api_key_prefix: str | None = None,\n    access_token: str | None = None,\n    verify_ssl: bool = True,\n    ssl_ca_cert: str | None = None,\n    proxy: str | None = None,\n    configfile: str = \"~/.lakectl.yaml\",\n    create_branch_ok: bool = True,\n    source_branch: str = \"main\",\n    **storage_options: Any,\n):\n    \"\"\"\n    The LakeFS file system constructor.\n\n    Parameters\n    ----------\n    host: str or None\n        The address of your lakeFS instance.\n    username: str or None\n        The access key name to use in case of access key authentication.\n    password: str or None\n        The access key secret to use in case of access key authentication.\n    api_key: str or None\n        The API key to use in case of authentication with an API key.\n    api_key_prefix: str or None\n        A string prefix to use for the API key in authentication.\n    access_token: str or None\n        An access token to use in case of access token authentication.\n    verify_ssl: bool\n        Whether to verify SSL certificates in API interactions. Do not disable in production.\n    ssl_ca_cert: str or None\n        A custom certificate PEM file to use to verify the peer in SSL connections.\n    proxy: str or None\n        Proxy address to use when connecting to a lakeFS server.\n    create_branch_ok: bool\n        Whether to create branches implicitly when not-existing branches are referenced on file uploads.\n    source_branch: str\n        Source branch set as origin when a new branch is implicitly created.\n    storage_options: Any\n        Configuration options to pass to the file system's directory cache.\n    \"\"\"\n    super().__init__(**storage_options)\n\n    if (p := Path(configfile).expanduser()).exists():\n        lakectl_config = LakectlConfig.read(p)\n    else:\n        # empty config.\n        lakectl_config = LakectlConfig()\n\n    configuration = Configuration(\n        host=host or os.getenv(\"LAKEFS_HOST\") or lakectl_config.host,\n        api_key=api_key or os.getenv(\"LAKEFS_API_KEY\"),\n        api_key_prefix=api_key_prefix or os.getenv(\"LAKEFS_API_KEY_PREFIX\"),\n        access_token=access_token or os.getenv(\"LAKEFS_ACCESS_TOKEN\"),\n        username=username or os.getenv(\"LAKEFS_USERNAME\") or lakectl_config.username,\n        password=password or os.getenv(\"LAKEFS_PASSWORD\") or lakectl_config.password,\n        ssl_ca_cert=ssl_ca_cert or os.getenv(\"LAKEFS_SSL_CA_CERT\"),\n    )\n    # proxy address, not part of the constructor\n    configuration.proxy = proxy\n    # whether to verify SSL certs, not part of the constructor\n    configuration.verify_ssl = verify_ssl\n\n    self.client = LakeFSClient(configuration=configuration)\n    self.create_branch_ok = create_branch_ok\n    self.source_branch = source_branch\n\n    self._hooks: dict[FSEvent, LakeFSHook] = {}\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFileSystem.scope","title":"scope","text":"<pre><code>scope(create_branch_ok=None, source_branch=None, disable_hooks=False)\n</code></pre> <p>A context manager yielding scope in which the lakeFS file system behavior is changed from defaults.</p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>@contextmanager\ndef scope(\n    self,\n    create_branch_ok: bool | None = None,\n    source_branch: str | None = None,\n    disable_hooks: bool = False,\n) -&gt; EmptyYield:\n    \"\"\"\n    A context manager yielding scope in which the lakeFS file system behavior\n    is changed from defaults.\n    \"\"\"\n    curr_create_branch_ok, curr_source_branch, curr_hooks = (\n        self.create_branch_ok,\n        self.source_branch,\n        self._hooks,\n    )\n    try:\n        if create_branch_ok is not None:\n            self.create_branch_ok = create_branch_ok\n        if source_branch is not None:\n            self.source_branch = source_branch\n        if disable_hooks:\n            self._hooks = {}\n        yield\n    finally:\n        self.create_branch_ok = curr_create_branch_ok\n        self.source_branch = curr_source_branch\n        self._hooks = curr_hooks\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFile","title":"LakeFSFile","text":"<p>             Bases: <code>AbstractBufferedFile</code></p> <p>lakeFS file implementation. Buffered in reads, unbuffered in writes.</p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>class LakeFSFile(AbstractBufferedFile):\n    \"\"\"lakeFS file implementation. Buffered in reads, unbuffered in writes.\"\"\"\n\n    def __init__(\n        self,\n        fs: LakeFSFileSystem,\n        path: str,\n        mode: Literal[\"rb\", \"wb\"] = \"rb\",\n        block_size: int | str = \"default\",\n        autocommit: bool = True,\n        cache_type: str = \"readahead\",\n        cache_options: dict[str, Any] | None = None,\n        size: int | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(\n            fs,\n            path,\n            mode=mode,\n            block_size=block_size,\n            autocommit=autocommit,\n            cache_type=cache_type,\n            cache_options=cache_options,\n            size=size,\n            **kwargs,\n        )\n\n        if mode == \"wb\":\n            global _warn_on_fileupload\n            if _warn_on_fileupload:\n                warnings.warn(\n                    f\"Calling `{self.__class__.__name__}.open()` in write mode results in unbuffered \"\n                    \"file uploads, because the lakeFS Python client does not support multipart \"\n                    \"uploads. Uploading large files unbuffered can have performance implications.\",\n                    UserWarning,\n                )\n                _warn_on_fileupload = False\n            repository, branch, resource = parse(path)\n            ensure_branch(self.fs.client, repository, branch, self.fs.source_branch)\n\n    def _upload_chunk(self, final: bool = False) -&gt; bool:\n        \"\"\"Single-chunk (unbuffered) upload, on final (i.e. during file.close()).\"\"\"\n        if final:\n            repository, branch, resource = parse(self.path)\n\n            with self.fs.wrapped_api_call():\n                # single-shot upload.\n                # empty buffer is equivalent to a touch()\n                self.buffer.seek(0)\n                self.fs.client.objects_api.upload_object(\n                    repository=repository,\n                    branch=branch,\n                    path=resource,\n                    content=self.buffer.read(),\n                )\n\n        return not final\n\n    def flush(self, force: bool = False) -&gt; None:\n        \"\"\"\n        Write buffered data to backend store.\n\n        Writes the current buffer, if it is larger than the block-size, or if\n        the file is being closed.\n\n        In contrast to the abstract class, this implementation does NOT unload the buffer\n        if it is larger than the block size, because the lakeFS server does not support\n        multipart uploads.\n\n        Parameters\n        ----------\n        force: bool\n            When closing, write the last block even if it is smaller than\n            blocks are allowed to be. Disallows further writing to this file.\n        \"\"\"\n\n        if self.closed:\n            raise ValueError(\"Flush on closed file\")\n        self.forced: bool\n        if force and self.forced:\n            raise ValueError(\"Force flush cannot be called more than once\")\n        if force:\n            self.forced = True\n\n        if self.mode != \"wb\":\n            # no-op to flush on read-mode\n            return\n\n        if not force and self.buffer.tell() &lt; self.blocksize:\n            # Defer write on small block\n            return\n\n        self.offset: int\n        if self.offset is None:\n            # Initialize an upload\n            self.offset = 0\n\n        if self._upload_chunk(final=force) is not False:\n            self.offset += self.buffer.seek(0, 2)\n\n    def _fetch_range(self, start: int, end: int) -&gt; bytes:\n        repository, ref, resource = parse(self.path)\n        with self.fs.wrapped_api_call():\n            return self.fs.client.objects_api.get_object(\n                repository, ref, resource, range=f\"bytes={start}-{end - 1}\"\n            )\n\n    def close(self) -&gt; None:\n        super().close()\n        if self.mode == \"wb\":\n            self.fs.run_hook(FSEvent.FILEUPLOAD, HookContext.new(self.path))\n        elif self.mode == \"rb\":\n            self.fs.run_hook(FSEvent.FILEDOWNLOAD, HookContext.new(self.path))\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.LakeFSFile.flush","title":"flush","text":"<pre><code>flush(force=False)\n</code></pre> <p>Write buffered data to backend store.</p> <p>Writes the current buffer, if it is larger than the block-size, or if the file is being closed.</p> <p>In contrast to the abstract class, this implementation does NOT unload the buffer if it is larger than the block size, because the lakeFS server does not support multipart uploads.</p> PARAMETER  DESCRIPTION <code>force</code> <p>When closing, write the last block even if it is smaller than blocks are allowed to be. Disallows further writing to this file.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>def flush(self, force: bool = False) -&gt; None:\n    \"\"\"\n    Write buffered data to backend store.\n\n    Writes the current buffer, if it is larger than the block-size, or if\n    the file is being closed.\n\n    In contrast to the abstract class, this implementation does NOT unload the buffer\n    if it is larger than the block size, because the lakeFS server does not support\n    multipart uploads.\n\n    Parameters\n    ----------\n    force: bool\n        When closing, write the last block even if it is smaller than\n        blocks are allowed to be. Disallows further writing to this file.\n    \"\"\"\n\n    if self.closed:\n        raise ValueError(\"Flush on closed file\")\n    self.forced: bool\n    if force and self.forced:\n        raise ValueError(\"Force flush cannot be called more than once\")\n    if force:\n        self.forced = True\n\n    if self.mode != \"wb\":\n        # no-op to flush on read-mode\n        return\n\n    if not force and self.buffer.tell() &lt; self.blocksize:\n        # Defer write on small block\n        return\n\n    self.offset: int\n    if self.offset is None:\n        # Initialize an upload\n        self.offset = 0\n\n    if self._upload_chunk(final=force) is not False:\n        self.offset += self.buffer.seek(0, 2)\n</code></pre>"},{"location":"reference/lakefs_spec/spec/#lakefs_spec.spec.ensure_branch","title":"ensure_branch","text":"<pre><code>ensure_branch(client, repository, branch, source_branch)\n</code></pre> <p>Checks if a branch exists. If not, it is created. This implementation depends on server-side error handling.</p> PARAMETER  DESCRIPTION <code>client</code> <p>The lakeFS client configured for (and authenticated with) the target instance.</p> <p> TYPE: <code>LakeFSClient</code> </p> <code>repository</code> <p>Repository name.</p> <p> TYPE: <code>str</code> </p> <code>branch</code> <p>Name of the branch.</p> <p> TYPE: <code>str</code> </p> <code>source_branch</code> <p>Name of the source branch the new branch is created from.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>src/lakefs_spec/spec.py</code> <pre><code>def ensure_branch(client: LakeFSClient, repository: str, branch: str, source_branch: str) -&gt; None:\n    \"\"\"\n    Checks if a branch exists. If not, it is created.\n    This implementation depends on server-side error handling.\n\n    Parameters\n    ----------\n    client: LakeFSClient\n        The lakeFS client configured for (and authenticated with) the target instance.\n    repository: str\n        Repository name.\n    branch: str\n        Name of the branch.\n    source_branch: str\n        Name of the source branch the new branch is created from.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    try:\n        new_branch = BranchCreation(name=branch, source=source_branch)\n        # client.branches_api.create_branch throws ApiException when branch exists\n        client.branches_api.create_branch(repository=repository, branch_creation=new_branch)\n        logger.info(f\"Created new branch {branch!r} from branch {source_branch!r}.\")\n    except ApiException:\n        pass\n</code></pre>"},{"location":"reference/lakefs_spec/util/","title":"util","text":""},{"location":"reference/lakefs_spec/util/#lakefs_spec.util.parse","title":"parse","text":"<pre><code>parse(path)\n</code></pre> <p>Parses a lakeFS URI in the form <code>&lt;repo&gt;/&lt;ref&gt;/&lt;resource&gt;</code>.</p> PARAMETER  DESCRIPTION <code>path</code> <p>String path, needs to conform to the lakeFS URI format described above. The <code>&lt;resource&gt;</code> part can be the empty string.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A 3-tuple of repository name, reference, and resource name.</p> Source code in <code>src/lakefs_spec/util.py</code> <pre><code>def parse(path: str) -&gt; tuple[str, str, str]:\n    \"\"\"\n    Parses a lakeFS URI in the form ``&lt;repo&gt;/&lt;ref&gt;/&lt;resource&gt;``.\n\n    Parameters\n    ----------\n    path: str\n        String path, needs to conform to the lakeFS URI format described above.\n        The ``&lt;resource&gt;`` part can be the empty string.\n\n    Returns\n    -------\n    str\n        A 3-tuple of repository name, reference, and resource name.\n    \"\"\"\n\n    # First regex reflects the lakeFS repository naming rules:\n    # only lowercase letters, digits and dash, no leading dash,\n    # minimum 3, maximum 63 characters\n    # https://docs.lakefs.io/understand/model.html#repository\n    # Second regex is the branch: Only letters, digits, underscores\n    # and dash, no leading dash\n    path_regex = re.compile(r\"([a-z0-9][a-z0-9\\-]{2,62})/(\\w[\\w\\-]*)/(.*)\")\n    results = path_regex.fullmatch(path)\n    if results is None:\n        raise ValueError(f\"expected path with structure &lt;repo&gt;/&lt;ref&gt;/&lt;resource&gt;, got {path!r}\")\n\n    repo, ref, resource = results.groups()\n    return repo, ref, resource\n</code></pre>"}]}