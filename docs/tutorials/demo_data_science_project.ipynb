{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "67fa0d7b",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we will complete a small end-to-end data science tutorial that employs `lakeFS-spec` for data versioning. We will use weather data to train a random forest classifier to predict whether a given day from now is a raining day given the current weather.\n",
        "\n",
        "We will do the following:\n",
        "* Environment setup\n",
        "* lakeFS setup\n",
        "* Data ingestion\n",
        "    * Transactions\n",
        "    * PUT a file\n",
        "* Model training\n",
        "* Updating data and retraining a model\n",
        "* Accessing data versions and Reproducing Experiments\n",
        "* Using a tag instead of a commit SHA for semantic versioning\n",
        "\n",
        "To execute the code in this tutorial as a Jupyter notebook, download this `.ipynb` file to a convenient location on your machine. You can also clone the whole `lakefs-spec` repository. During the execution of this tutorial, in the same directory, a folder 'data' will be created. We will also download a file `.lakectl.yaml` to the current user's home directory.\n",
        "\n",
        "Prerequisites before we start:\n",
        "* Python 3.9 or higher\n",
        "* Docker desktop installed - [see guidance](https://www.docker.com/get-started/)\n",
        "* git installed\n",
        "\n",
        "# Environment Setup\n",
        "To set up the environment, run the following commands in your console:\n",
        "\n",
        "Create a virtual environment:\n",
        "\n",
        "`python -m venv .venv`\n",
        "\n",
        "Activate the environment.\n",
        "\n",
        "On macOS and Linux:\n",
        "\n",
        "`source .venv/bin/activate`\n",
        "\n",
        "On Windows:\n",
        "\n",
        "`.venv\\Scripts\\activate`\n",
        "\n",
        "Install the libraries necessary for this notebook on the environment you have just created: \n",
        "\n",
        "`pip install lakefs-spec numpy pandas scikit-learn`.\n",
        "\n",
        "From a terminal activate this Jupyter notebook (if not running already).\n",
        "\n",
        "In the notebook \"Kernel\" menu select the environment you just created as a Jupyter kernel.\n",
        "\n",
        "# lakeFS Setup\n",
        "\n",
        "Ensure you have docker desktop is running.\n",
        "\n",
        "Set up LakeFS. You can do this by executing the `docker run` command given here (the lakeFS quickstart) in your console:\n",
        "\n",
        "`docker run --name lakefs --pull always --rm --publish 8000:8000 treeverse/lakefs:latest run --quickstart`\n",
        "\n",
        "Open a browser and navigate to the lakeFS instance - by default: http://localhost:8000/. \n",
        "\n",
        "Authenticate with the credentials provided https://docs.lakefs.io/quickstart/launch.html :\n",
        "\n",
        "    Access Key ID    : AKIAIOSFOLQUICKSTART\n",
        "    Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n",
        "\n",
        "As an email, you can enter anything, we won't need it in this example.\n",
        "\n",
        "We begin by instantiating the filesystem connector `LakeFSFilesystem` and fixing the name of the repository we are about to create. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "349e6b87",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:23.906274Z",
          "iopub.status.busy": "2023-11-14T16:32:23.905962Z",
          "iopub.status.idle": "2023-11-14T16:32:24.173083Z",
          "shell.execute_reply": "2023-11-14T16:32:24.172822Z"
        }
      },
      "outputs": [],
      "source": [
        "from lakefs_spec import LakeFSFileSystem\n",
        "\n",
        "fs = LakeFSFileSystem()\n",
        "\n",
        "REPO_NAME = 'weather'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53992dd8",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "There many ways to authenticate to lakeFS while executing Python code - in this tutorial, we choose a convenient YAML file configuration. Execute the code below to download the YAML file including the lakeFS quickstart credentials and server URL. This file will be downloaded to your user directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "12e3138c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:24.207059Z",
          "iopub.status.busy": "2023-11-14T16:32:24.206972Z",
          "iopub.status.idle": "2023-11-14T16:32:24.679249Z",
          "shell.execute_reply": "2023-11-14T16:32:24.678186Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/Users/maxmynter/.lakectl.yaml', <http.client.HTTPMessage at 0x10bbd9910>)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "destination = os.path.expanduser(\"~/.lakectl.yaml\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/appliedAI-Initiative/lakefs-spec/main/demos/.lakectl.yaml\", destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create a repository using a helper function provided by `lakefs-spec`. If you created one in the UI, make sure to set the `REPO_NAME` variable in the cell above accordingly. You can re-execute if necessary.\n",
        "Otherwise, execute the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Repository(id='weather', creation_date=1700048699, default_branch='main', storage_namespace='local://weather')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from lakefs_spec.client_helpers import create_repository\n",
        "\n",
        "create_repository(client=fs.client, name=REPO_NAME,\n",
        "                  storage_namespace=f\"local://{REPO_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb9d3502",
      "metadata": {},
      "source": [
        "# Data Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d37daf2",
      "metadata": {},
      "source": [
        "Now it's time to get some data. We will use the [Open Meteo api](https://open-meteo.com/), where we can pull weather data from an API for free (as long as we are non-commercial) and without an API-token.\n",
        "\n",
        "First, create the folder 'data' inside a directory when your notebook is located:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c20f56ba",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:24.683566Z",
          "iopub.status.busy": "2023-11-14T16:32:24.683250Z",
          "iopub.status.idle": "2023-11-14T16:32:24.806288Z",
          "shell.execute_reply": "2023-11-14T16:32:24.804739Z"
        }
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"data\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f35215",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 0
      },
      "source": [
        "Then, for the purpose of training, get the full data of the 2010s from Munich:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "331bb8bb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:24.811039Z",
          "iopub.status.busy": "2023-11-14T16:32:24.810707Z",
          "iopub.status.idle": "2023-11-14T16:32:31.863888Z",
          "shell.execute_reply": "2023-11-14T16:32:31.862399Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('data/weather-2010s.json', <http.client.HTTPMessage at 0x10bbdfa10>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "destination = \"data/weather-2010s.json\"\n",
        "urllib.request.urlretrieve(\"https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2010-01-01&end_date=2019-12-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m\", destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e651374",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "# PUT a file\n",
        "\n",
        "The data is in JSON format. Therefore, we need to wrangle the data a bit to make it usable. But first we will save it into our lakeFS instance.\n",
        "\n",
        "lakeFS works similar to `git` as a versioning system. You can create *commits* that contain specific changes to the data. You can also work with *branches* to fork your own copy of the data such that you don't interfere with your colleagues. Every commit (on any branch) is identified by a commit SHA. This SHA can be used to programmatically interact with specific states of your data and enables logging of the specific data versions used to create a certain model. We will cover all of this in this demo.\n",
        "\n",
        "For now, we will `put` the file we have on a new branch, `transform-raw-data`, specifically created for our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6b94db09",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:31.869472Z",
          "iopub.status.busy": "2023-11-14T16:32:31.869023Z",
          "iopub.status.idle": "2023-11-14T16:32:32.024635Z",
          "shell.execute_reply": "2023-11-14T16:32:32.023893Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from lakefs_spec import LakeFSFileSystem\n",
        "LakeFSFileSystem.clear_instance_cache()\n",
        "\n",
        "NEW_BRANCH_NAME = 'transform-raw-data'\n",
        "\n",
        "fs = LakeFSFileSystem()\n",
        "fs.put('./data/weather-2010s.json', f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f86a01a",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 0
      },
      "source": [
        "Going to the LakeFS UI in your browser, you can change the branch view to `transform-raw-data` and see the saved file. However, the change is not yet committed. While you can do that manually via the uncommitted changes tab in the UI, we will commit the file in a different way."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bd285eb",
      "metadata": {},
      "source": [
        "## Transactions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729cadd9",
      "metadata": {},
      "source": [
        "To easily carry out versioning operations while uploading files, you can use a *transaction*. A transaction is a context manager that keeps track of all files that were uploaded in its scope, as well as all versioning operations happening in between file uploads. All operations are deferred to the end of the transaction, and are executed sequentially on completion.\n",
        "\n",
        "To create a commit after a file upload, you can run the following transaction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9b579e5d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:32.028144Z",
          "iopub.status.busy": "2023-11-14T16:32:32.027892Z",
          "iopub.status.idle": "2023-11-14T16:32:32.032983Z",
          "shell.execute_reply": "2023-11-14T16:32:32.032470Z"
        }
      },
      "outputs": [],
      "source": [
        "with fs.transaction as tx:\n",
        "    fs.put('data/weather-2010s.json', f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json', precheck=True, autocommit=False)\n",
        "    tx.commit(repository=REPO_NAME, branch=NEW_BRANCH_NAME, message=\"Add weather data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a84d00",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "This transaction, if successful, will create a commit. Since we already uploaded the file, lakeFS will skip the upload as the checksums of the local and remote file match.\n",
        "\n",
        "If we want to execute the upload even for an unchanged file, we can do so by passing `precheck=False` to the `fs.put()` operation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be7debf4",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "# Data Transformation\n",
        "Now let's transform the data for our use case. We put the transformation into a function to be able to reuse it later.\n",
        "\n",
        "In this notebook, we follow a simple toy example to predict whether it is raining at the same time tomorrow given weather data from right now.\n",
        "\n",
        "We will skip a lot of possible feature engineering etc. in order to focus on the application of lakeFS and the `LakeFSFileSystem`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7dd5cde7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:32.105568Z",
          "iopub.status.busy": "2023-11-14T16:32:32.105482Z",
          "iopub.status.idle": "2023-11-14T16:32:33.656925Z",
          "shell.execute_reply": "2023-11-14T16:32:33.656684Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>temperature_2m</th>\n",
              "      <th>relativehumidity_2m</th>\n",
              "      <th>rain</th>\n",
              "      <th>pressure_msl</th>\n",
              "      <th>surface_pressure</th>\n",
              "      <th>cloudcover</th>\n",
              "      <th>cloudcover_low</th>\n",
              "      <th>cloudcover_mid</th>\n",
              "      <th>cloudcover_high</th>\n",
              "      <th>windspeed_10m</th>\n",
              "      <th>windspeed_100m</th>\n",
              "      <th>winddirection_10m</th>\n",
              "      <th>winddirection_100m</th>\n",
              "      <th>is_raining</th>\n",
              "      <th>is_raining_in_1_day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2010-01-02 00:00:00</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1004.0</td>\n",
              "      <td>999.2</td>\n",
              "      <td>100</td>\n",
              "      <td>54</td>\n",
              "      <td>100</td>\n",
              "      <td>70</td>\n",
              "      <td>8.3</td>\n",
              "      <td>17.3</td>\n",
              "      <td>18</td>\n",
              "      <td>31</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2010-01-02 01:00:00</td>\n",
              "      <td>-3.2</td>\n",
              "      <td>89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1004.5</td>\n",
              "      <td>999.7</td>\n",
              "      <td>100</td>\n",
              "      <td>37</td>\n",
              "      <td>98</td>\n",
              "      <td>92</td>\n",
              "      <td>9.1</td>\n",
              "      <td>18.6</td>\n",
              "      <td>9</td>\n",
              "      <td>22</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2010-01-02 02:00:00</td>\n",
              "      <td>-3.4</td>\n",
              "      <td>89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1005.2</td>\n",
              "      <td>1000.4</td>\n",
              "      <td>100</td>\n",
              "      <td>24</td>\n",
              "      <td>98</td>\n",
              "      <td>77</td>\n",
              "      <td>10.1</td>\n",
              "      <td>20.3</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2010-01-02 03:00:00</td>\n",
              "      <td>-3.5</td>\n",
              "      <td>89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1005.6</td>\n",
              "      <td>1000.8</td>\n",
              "      <td>93</td>\n",
              "      <td>8</td>\n",
              "      <td>98</td>\n",
              "      <td>89</td>\n",
              "      <td>11.2</td>\n",
              "      <td>21.7</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2010-01-02 04:00:00</td>\n",
              "      <td>-3.7</td>\n",
              "      <td>90</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1006.2</td>\n",
              "      <td>1001.4</td>\n",
              "      <td>94</td>\n",
              "      <td>6</td>\n",
              "      <td>100</td>\n",
              "      <td>95</td>\n",
              "      <td>11.2</td>\n",
              "      <td>21.8</td>\n",
              "      <td>358</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  time  temperature_2m  relativehumidity_2m  rain  \\\n",
              "24 2010-01-02 00:00:00            -3.0                   88   0.0   \n",
              "25 2010-01-02 01:00:00            -3.2                   89   0.0   \n",
              "26 2010-01-02 02:00:00            -3.4                   89   0.0   \n",
              "27 2010-01-02 03:00:00            -3.5                   89   0.0   \n",
              "28 2010-01-02 04:00:00            -3.7                   90   0.0   \n",
              "\n",
              "    pressure_msl  surface_pressure  cloudcover  cloudcover_low  \\\n",
              "24        1004.0             999.2         100              54   \n",
              "25        1004.5             999.7         100              37   \n",
              "26        1005.2            1000.4         100              24   \n",
              "27        1005.6            1000.8          93               8   \n",
              "28        1006.2            1001.4          94               6   \n",
              "\n",
              "    cloudcover_mid  cloudcover_high  windspeed_10m  windspeed_100m  \\\n",
              "24             100               70            8.3            17.3   \n",
              "25              98               92            9.1            18.6   \n",
              "26              98               77           10.1            20.3   \n",
              "27              98               89           11.2            21.7   \n",
              "28             100               95           11.2            21.8   \n",
              "\n",
              "    winddirection_10m  winddirection_100m  is_raining is_raining_in_1_day  \n",
              "24                 18                  31       False               False  \n",
              "25                  9                  22       False               False  \n",
              "26                  2                  13       False               False  \n",
              "27                  4                  12       False               False  \n",
              "28                358                   9       False               False  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def transform_json_weather_data(filepath):\n",
        "    with open(filepath,\"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data[\"hourly\"])\n",
        "    df.time = pd.to_datetime(df.time)\n",
        "    df['is_raining'] = df.rain > 0\n",
        "    df['is_raining_in_1_day'] = df.is_raining.shift(24)\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "    \n",
        "df = transform_json_weather_data('data/weather-2010s.json')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dfd927d",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "Next, we save this data as a CSV file into the main branch. When the transaction commit helper is called, the newly put CSV file is committed. You can verify the saving worked in the LakeFS UI in your browser by switching to the commits tab of the `main` branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "01c6c9a5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:33.658288Z",
          "iopub.status.busy": "2023-11-14T16:32:33.658176Z",
          "iopub.status.idle": "2023-11-14T16:32:34.082584Z",
          "shell.execute_reply": "2023-11-14T16:32:34.082288Z"
        }
      },
      "outputs": [],
      "source": [
        "with fs.transaction as tx:\n",
        "    df.to_csv(f'lakefs://{REPO_NAME}/main/weather_2010s.csv')\n",
        "    tx.commit(repository=REPO_NAME, branch=\"main\", message=\"Update weather data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c69526",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "# Model Training\n",
        "First we will do a train-test split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "845c5012",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:34.083999Z",
          "iopub.status.busy": "2023-11-14T16:32:34.083902Z",
          "iopub.status.idle": "2023-11-14T16:32:36.205631Z",
          "shell.execute_reply": "2023-11-14T16:32:36.205271Z"
        }
      },
      "outputs": [],
      "source": [
        "import sklearn.model_selection\n",
        "\n",
        "model_data = df.drop('time', axis=1)\n",
        "\n",
        "train, test = sklearn.model_selection.train_test_split(model_data, random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f494332",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "We save these train and test datasets into a new `training` branch. If the branch does not exist yet, as in this case, it is implicitly created by default. You can control this behaviour with the `create_branch_ok` flag when initializing the `LakeFSFileSystem`. By default, `create_branch_ok` is set to `True`, so we need to only set `fs = LakeFSFileSystem()` to enable implicit branch creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "204048af",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:36.207403Z",
          "iopub.status.busy": "2023-11-14T16:32:36.207227Z",
          "iopub.status.idle": "2023-11-14T16:32:36.618294Z",
          "shell.execute_reply": "2023-11-14T16:32:36.618038Z"
        }
      },
      "outputs": [],
      "source": [
        "TRAINING_BRANCH = 'training'\n",
        "\n",
        "with fs.transaction as tx:\n",
        "    train.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
        "    test.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n",
        "    tx.commit(repository=REPO_NAME, branch=TRAINING_BRANCH, message=\"Add train-test split of 2010s weather data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a79efb76",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "Implicit branch creation is a convenient way to create new branches programmatically. However, one drawback is that typos in your code might result in new accidental branch creations. If you want to avoid this implicit behavior and raise errors instead, you can disable implicit branch creation by setting `fs.create_branch_ok=False`.\n",
        "\n",
        "We can now read train and test files directly from the remote LakeFS instance. (You can verify that neither the train nor the test file are saved in the `/data` directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7b3f6367",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:36.619671Z",
          "iopub.status.busy": "2023-11-14T16:32:36.619594Z",
          "iopub.status.idle": "2023-11-14T16:32:36.725657Z",
          "shell.execute_reply": "2023-11-14T16:32:36.725386Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>temperature_2m</th>\n",
              "      <th>relativehumidity_2m</th>\n",
              "      <th>rain</th>\n",
              "      <th>pressure_msl</th>\n",
              "      <th>surface_pressure</th>\n",
              "      <th>cloudcover</th>\n",
              "      <th>cloudcover_low</th>\n",
              "      <th>cloudcover_mid</th>\n",
              "      <th>cloudcover_high</th>\n",
              "      <th>windspeed_10m</th>\n",
              "      <th>windspeed_100m</th>\n",
              "      <th>winddirection_10m</th>\n",
              "      <th>winddirection_100m</th>\n",
              "      <th>is_raining</th>\n",
              "      <th>is_raining_in_1_day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>59272</th>\n",
              "      <td>10.4</td>\n",
              "      <td>79</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1026.8</td>\n",
              "      <td>1022.1</td>\n",
              "      <td>100</td>\n",
              "      <td>58</td>\n",
              "      <td>93</td>\n",
              "      <td>98</td>\n",
              "      <td>21.7</td>\n",
              "      <td>35.9</td>\n",
              "      <td>11</td>\n",
              "      <td>13</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73375</th>\n",
              "      <td>16.5</td>\n",
              "      <td>79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1011.9</td>\n",
              "      <td>1007.4</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>12.4</td>\n",
              "      <td>16.7</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26941</th>\n",
              "      <td>0.2</td>\n",
              "      <td>82</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1005.9</td>\n",
              "      <td>1001.1</td>\n",
              "      <td>87</td>\n",
              "      <td>2</td>\n",
              "      <td>93</td>\n",
              "      <td>98</td>\n",
              "      <td>14.4</td>\n",
              "      <td>24.9</td>\n",
              "      <td>176</td>\n",
              "      <td>183</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65930</th>\n",
              "      <td>16.2</td>\n",
              "      <td>78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1010.9</td>\n",
              "      <td>1006.4</td>\n",
              "      <td>42</td>\n",
              "      <td>10</td>\n",
              "      <td>37</td>\n",
              "      <td>36</td>\n",
              "      <td>13.4</td>\n",
              "      <td>25.8</td>\n",
              "      <td>54</td>\n",
              "      <td>63</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55318</th>\n",
              "      <td>4.3</td>\n",
              "      <td>68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1008.0</td>\n",
              "      <td>1003.3</td>\n",
              "      <td>46</td>\n",
              "      <td>12</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>21.8</td>\n",
              "      <td>302</td>\n",
              "      <td>308</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       temperature_2m  relativehumidity_2m  rain  pressure_msl  \\\n",
              "59272            10.4                   79   0.3        1026.8   \n",
              "73375            16.5                   79   0.0        1011.9   \n",
              "26941             0.2                   82   0.0        1005.9   \n",
              "65930            16.2                   78   0.0        1010.9   \n",
              "55318             4.3                   68   0.0        1008.0   \n",
              "\n",
              "       surface_pressure  cloudcover  cloudcover_low  cloudcover_mid  \\\n",
              "59272            1022.1         100              58              93   \n",
              "73375            1007.4          28               0               3   \n",
              "26941            1001.1          87               2              93   \n",
              "65930            1006.4          42              10              37   \n",
              "55318            1003.3          46              12              58   \n",
              "\n",
              "       cloudcover_high  windspeed_10m  windspeed_100m  winddirection_10m  \\\n",
              "59272               98           21.7            35.9                 11   \n",
              "73375               88           12.4            16.7                 17   \n",
              "26941               98           14.4            24.9                176   \n",
              "65930               36           13.4            25.8                 54   \n",
              "55318                0           11.0            21.8                302   \n",
              "\n",
              "       winddirection_100m  is_raining  is_raining_in_1_day  \n",
              "59272                  13        True                False  \n",
              "73375                  19       False                False  \n",
              "26941                 183       False                False  \n",
              "65930                  63       False                False  \n",
              "55318                 308       False                False  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
        "test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c9c4dc",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "Let's check the shape of train and test data. Later on we will train to get back to this data version and reproduce the results of the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "39562daa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:36.727061Z",
          "iopub.status.busy": "2023-11-14T16:32:36.726974Z",
          "iopub.status.idle": "2023-11-14T16:32:36.728790Z",
          "shell.execute_reply": "2023-11-14T16:32:36.728538Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial train data shape: (65718, 15)\n",
            "Initial test data shape: (21906, 15)\n"
          ]
        }
      ],
      "source": [
        "print(f'Initial train data shape: {train.shape}')\n",
        "print(f'Initial test data shape: {test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e97ae35",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "We now proceed to train a random forest classifier and evaluate it on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "10ac980e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:36.730033Z",
          "iopub.status.busy": "2023-11-14T16:32:36.729954Z",
          "iopub.status.idle": "2023-11-14T16:32:45.462087Z",
          "shell.execute_reply": "2023-11-14T16:32:45.461789Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 87.99%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "dependent_variable = 'is_raining_in_1_day'\n",
        "\n",
        "model = RandomForestClassifier(random_state=7)\n",
        "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
        "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "test_acc = model.score(x_test, y_test)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e67b34",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "# Updating Data and Model\n",
        "Until now, we only have used data from the 2010s. Let's download additional 2020s data, transform it, and save it to LakeFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "43fa0216",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:45.463640Z",
          "iopub.status.busy": "2023-11-14T16:32:45.463556Z",
          "iopub.status.idle": "2023-11-14T16:32:47.057867Z",
          "shell.execute_reply": "2023-11-14T16:32:47.057561Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "destination = \"data/weather-2020s.json\"\n",
        "urllib.request.urlretrieve(\"https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2020-01-01&end_date=2023-08-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m\", destination)\n",
        "\n",
        "new_data = transform_json_weather_data('data/weather-2020s.json')\n",
        "\n",
        "with fs.transaction as tx:\n",
        "    new_data.to_csv(f'lakefs://{REPO_NAME}/main/weather_2020s.csv')\n",
        "    tx.commit(repository=REPO_NAME, branch=\"main\", message=\"Add 2020s weather data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "11242a6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_data = new_data.drop('time', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b68e8c2",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "Let's concatenate the old data and the new data, create a new train-test split, and overwrite the files on lakeFS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5313be48",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:47.059348Z",
          "iopub.status.busy": "2023-11-14T16:32:47.059263Z",
          "iopub.status.idle": "2023-11-14T16:32:47.662850Z",
          "shell.execute_reply": "2023-11-14T16:32:47.662603Z"
        }
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
        "df_test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
        "\n",
        "full_data = pd.concat([new_data, df_train, df_test])\n",
        "\n",
        "train_df, test_df = sklearn.model_selection.train_test_split(full_data, random_state=7)\n",
        "\n",
        "with fs.transaction as tx:\n",
        "    train_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
        "    test_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n",
        "    tx.commit(repository=REPO_NAME, branch=TRAINING_BRANCH, message=\"Add train-test split of full 2010-2020s data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1b84823",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 0
      },
      "source": [
        "We may now read the updated data directly from lakeFS and check their shape to insure that initial files `train_weather.csv` and `test_weather.csv` have been overwritten successfully (number of rows should be significantly higher as 2020 data were added):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d92a3639",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:47.664288Z",
          "iopub.status.busy": "2023-11-14T16:32:47.664203Z",
          "iopub.status.idle": "2023-11-14T16:32:47.782392Z",
          "shell.execute_reply": "2023-11-14T16:32:47.782145Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated train data shape: (89802, 15)\n",
            "Updated test data shape: (29934, 15)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
        "test = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv', index_col=0)\n",
        "\n",
        "print(f'Updated train data shape: {train.shape}')\n",
        "print(f'Updated test data shape: {test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b862924e",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "Now we may train the model based on the new train data and validate based on the new test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "655b96df",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:47.783763Z",
          "iopub.status.busy": "2023-11-14T16:32:47.783662Z",
          "iopub.status.idle": "2023-11-14T16:32:59.059329Z",
          "shell.execute_reply": "2023-11-14T16:32:59.058989Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 87.27%\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
        "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "test_acc = model.score(x_test, y_test)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c70b39",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "# Accessing Data Version and Reproducing Experiment\n",
        "\n",
        "Let's assume we need to go to our initial data and reproduce the first experiment (the model trained on the 2010s data with its initial accuracy). This might be tricky as we have overwritten initial train and test data on lakeFS.\n",
        "\n",
        "To enable data versioning we should save the `ref` of the specific datasets. `ref` can be a branch we are pulling a file from LakeFS. `ref` can be also a commit id - then you can access different data versions within the same branch and not only the version from the latest commit. Therefore, we will use explicit versioning and get the actual commit SHA. We have multiple ways to do this. Manually, we could go into the lakeFS UI, select the training branch, and navigate to the \"Commits\" tab. There, we could see the second-latest commit, titled `Add train-test split of 2010s weather data`, and copy its ID.\n",
        "\n",
        "However, we want to automate as much as possible and therefore use a helper function. You find pre-written helper functions in the `lakefs_spec.client_helpers` module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1c06cb94",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:59.060814Z",
          "iopub.status.busy": "2023-11-14T16:32:59.060731Z",
          "iopub.status.idle": "2023-11-14T16:32:59.064447Z",
          "shell.execute_reply": "2023-11-14T16:32:59.064197Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id='bb9d29ecebb34ead638db374d9f23856dbb209418555a9e39e19eb42fcb321df' parents=['53e4284f2b8f01dcc0dd869f0c38ba8c39ba09aeb76178b2497b9c97efe3af98'] committer='quickstart' message='Add train-test split of 2010s weather data' creation_date=1700048706 meta_range_id='6df70c09db877711b6ba8a6c53dec014be19308ce14c2436a38697dfd4929d49' metadata={}\n"
          ]
        }
      ],
      "source": [
        "from lakefs_spec.client_helpers import rev_parse\n",
        "\n",
        "# parent is a relative number of a commit when 0 is the latest\n",
        "fixed_commit_id = rev_parse(fs.client, REPO_NAME, TRAINING_BRANCH, parent=1)\n",
        "print(fixed_commit_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e2558b",
      "metadata": {},
      "source": [
        "With our transaction setup, both `DataFrame.to_csv()` operations are kept in a single commit. To get other commits with the `rev_parse` function, you can change the `repository` and `branch` parameters. To go back in the chosen branch's commit history, you can increase the `parent` parameter. In our case the initial data was commited two commits ago - we count the latest commit on a branch as 0, thus `parent = 1`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9adf24fc",
      "metadata": {},
      "source": [
        "Let's check whether we manage to get the initial train and test data with this commit SHA, comparing the shape to the initial data shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f908ab5f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:59.065775Z",
          "iopub.status.busy": "2023-11-14T16:32:59.065698Z",
          "iopub.status.idle": "2023-11-14T16:32:59.146981Z",
          "shell.execute_reply": "2023-11-14T16:32:59.146691Z"
        }
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "expected path with structure <repo>/<ref>/<resource>, got 'weather/'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/maxmynter/Desktop/appliedAI/lakefs/lakefs-spec/docs/tutorials/demo_data_science_project.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maxmynter/Desktop/appliedAI/lakefs/lakefs-spec/docs/tutorials/demo_data_science_project.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlakefs://\u001b[39;49m\u001b[39m{\u001b[39;49;00mREPO_NAME\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mfixed_commit_id\u001b[39m}\u001b[39;49;00m\u001b[39m/train_weather.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxmynter/Desktop/appliedAI/lakefs/lakefs-spec/docs/tutorials/demo_data_science_project.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlakefs://\u001b[39m\u001b[39m{\u001b[39;00mREPO_NAME\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfixed_commit_id\u001b[39m}\u001b[39;00m\u001b[39m/test_weather.csv\u001b[39m\u001b[39m\"\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxmynter/Desktop/appliedAI/lakefs/lakefs-spec/docs/tutorials/demo_data_science_project.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain data shape: \u001b[39m\u001b[39m{\u001b[39;00mtrain\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    719\u001b[0m     path_or_buf,\n\u001b[1;32m    720\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    721\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    722\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    723\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    724\u001b[0m )\n\u001b[1;32m    726\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/pandas/io/common.py:418\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m     file_obj \u001b[39m=\u001b[39m fsspec\u001b[39m.\u001b[39;49mopen(\n\u001b[1;32m    419\u001b[0m         filepath_or_buffer, mode\u001b[39m=\u001b[39;49mfsspec_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(storage_options \u001b[39mor\u001b[39;49;00m {})\n\u001b[1;32m    420\u001b[0m     )\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    421\u001b[0m \u001b[39m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mtuple\u001b[39m(err_types_to_retry_with_anon):\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/fsspec/core.py:452\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(urlpath, mode, compression, encoding, errors, protocol, newline, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    393\u001b[0m     urlpath,\n\u001b[1;32m    394\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    401\u001b[0m ):\n\u001b[1;32m    402\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a path or paths, return one ``OpenFile`` object.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \n\u001b[1;32m    404\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39m      https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m     out \u001b[39m=\u001b[39m open_files(\n\u001b[1;32m    453\u001b[0m         urlpath\u001b[39m=\u001b[39;49m[urlpath],\n\u001b[1;32m    454\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    455\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    456\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    457\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    458\u001b[0m         protocol\u001b[39m=\u001b[39;49mprotocol,\n\u001b[1;32m    459\u001b[0m         newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    460\u001b[0m         expand\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    461\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    462\u001b[0m     )\n\u001b[1;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out:\n\u001b[1;32m    464\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(urlpath)\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/fsspec/core.py:280\u001b[0m, in \u001b[0;36mopen_files\u001b[0;34m(urlpath, mode, compression, encoding, errors, name_function, num, protocol, newline, auto_mkdir, expand, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_files\u001b[39m(\n\u001b[1;32m    202\u001b[0m     urlpath,\n\u001b[1;32m    203\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    214\u001b[0m ):\n\u001b[1;32m    215\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a path or paths, return a list of ``OpenFile`` objects.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \n\u001b[1;32m    217\u001b[0m \u001b[39m    For writing, a str path must contain the \"*\" character, which will be filled\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39m      https://filesystem-spec.readthedocs.io/en/latest/api.html#other-known-implementations\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     fs, fs_token, paths \u001b[39m=\u001b[39m get_fs_token_paths(\n\u001b[1;32m    281\u001b[0m         urlpath,\n\u001b[1;32m    282\u001b[0m         mode,\n\u001b[1;32m    283\u001b[0m         num\u001b[39m=\u001b[39;49mnum,\n\u001b[1;32m    284\u001b[0m         name_function\u001b[39m=\u001b[39;49mname_function,\n\u001b[1;32m    285\u001b[0m         storage_options\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m    286\u001b[0m         protocol\u001b[39m=\u001b[39;49mprotocol,\n\u001b[1;32m    287\u001b[0m         expand\u001b[39m=\u001b[39;49mexpand,\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    289\u001b[0m     \u001b[39mif\u001b[39;00m fs\u001b[39m.\u001b[39mprotocol \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    290\u001b[0m         fs\u001b[39m.\u001b[39mauto_mkdir \u001b[39m=\u001b[39m auto_mkdir\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/fsspec/core.py:633\u001b[0m, in \u001b[0;36mget_fs_token_paths\u001b[0;34m(urlpath, mode, num, name_function, storage_options, protocol, expand)\u001b[0m\n\u001b[1;32m    631\u001b[0m     paths \u001b[39m=\u001b[39m fs\u001b[39m.\u001b[39m_strip_protocol(paths)\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(paths, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mset\u001b[39m)):\n\u001b[0;32m--> 633\u001b[0m     paths \u001b[39m=\u001b[39m expand_paths_if_needed(paths, mode, num, fs, name_function)\n\u001b[1;32m    634\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m expand:\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/fsspec/core.py:563\u001b[0m, in \u001b[0;36mexpand_paths_if_needed\u001b[0;34m(paths, mode, num, fs, name_function)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mfor\u001b[39;00m curr_path \u001b[39min\u001b[39;00m paths:\n\u001b[1;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m has_magic(curr_path):\n\u001b[1;32m    562\u001b[0m         \u001b[39m# expand using glob\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m         expanded_paths\u001b[39m.\u001b[39mextend(fs\u001b[39m.\u001b[39;49mglob(curr_path))\n\u001b[1;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m         expanded_paths\u001b[39m.\u001b[39mappend(curr_path)\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/fsspec/spec.py:602\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m         depth \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m allpaths \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind(root, maxdepth\u001b[39m=\u001b[39;49mdepth, withdirs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, detail\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    603\u001b[0m \u001b[39m# Escape characters special to python regex, leaving our supported\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[39m# special characters in place.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[39m# See https://www.gnu.org/software/bash/manual/html_node/Pattern-Matching.html\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[39m# for shell globbing details.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m pattern \u001b[39m=\u001b[39m (\n\u001b[1;32m    608\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m^\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[39m+\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m )\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/fsspec/spec.py:492\u001b[0m, in \u001b[0;36mAbstractFileSystem.find\u001b[0;34m(self, path, maxdepth, withdirs, detail, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m out \u001b[39m=\u001b[39m {}\n\u001b[1;32m    490\u001b[0m \u001b[39m# Add the root directory if withdirs is requested\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[39m# This is needed for posix glob compliance\u001b[39;00m\n\u001b[0;32m--> 492\u001b[0m \u001b[39mif\u001b[39;00m withdirs \u001b[39mand\u001b[39;00m path \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49misdir(path):\n\u001b[1;32m    493\u001b[0m     out[path] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo(path)\n\u001b[1;32m    495\u001b[0m \u001b[39mfor\u001b[39;00m _, dirs, files \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwalk(path, maxdepth, detail\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/.demovenv/lib/python3.11/site-packages/fsspec/spec.py:721\u001b[0m, in \u001b[0;36mAbstractFileSystem.isdir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Is this entry directory-like?\"\"\"\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo(path)[\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdirectory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/src/lakefs_spec/spec.py:255\u001b[0m, in \u001b[0;36mLakeFSFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m# input path is a directory name\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 255\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mls(path, detail\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out:\n\u001b[1;32m    257\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(path)\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/src/lakefs_spec/spec.py:286\u001b[0m, in \u001b[0;36mLakeFSFileSystem.ls\u001b[0;34m(self, path, detail, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mls\u001b[39m(\u001b[39mself\u001b[39m, path: \u001b[39mstr\u001b[39m, detail: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    285\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strip_protocol(path)\n\u001b[0;32m--> 286\u001b[0m     repository, ref, prefix \u001b[39m=\u001b[39m parse(path)\n\u001b[1;32m    288\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m         cache_entry: \u001b[39mlist\u001b[39m[Any] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ls_from_cache(prefix)\n",
            "File \u001b[0;32m~/Desktop/appliedAI/lakefs/lakefs-spec/src/lakefs_spec/util.py:45\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     43\u001b[0m results \u001b[39m=\u001b[39m path_regex\u001b[39m.\u001b[39mfullmatch(path)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m results \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected path with structure <repo>/<ref>/<resource>, got \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m repo, ref, resource \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mgroups()\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m repo, ref, resource\n",
            "\u001b[0;31mValueError\u001b[0m: expected path with structure <repo>/<ref>/<resource>, got 'weather/'"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv(f\"lakefs://{REPO_NAME}/{fixed_commit_id}/train_weather.csv\", index_col=0)\n",
        "test = pd.read_csv(f\"lakefs://{REPO_NAME}/{fixed_commit_id}/test_weather.csv\", index_col=0)\n",
        "\n",
        "print(f'train data shape: {train.shape}')\n",
        "print(f'test data shape: {test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09528632",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 0
      },
      "source": [
        "Let's train and validate the model based on re-fetched data and see whether we manage to reproduce the initial accuracy ratio:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cca9112",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:32:59.148389Z",
          "iopub.status.busy": "2023-11-14T16:32:59.148309Z",
          "iopub.status.idle": "2023-11-14T16:33:07.239976Z",
          "shell.execute_reply": "2023-11-14T16:33:07.239715Z"
        }
      },
      "outputs": [],
      "source": [
        "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
        "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "test_acc = model.score(x_test, y_test)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be45c4d",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "# Using a tag instead of a commit SHA for semantic versioning\n",
        "The above method for data versioning works great when you have experiment tracking tools to store and retrieve the commit SHA in automated pipelines. But it is hard to remember and tedious to retrieve in manual prototyping. We can make selected versions of the dataset more accessible with semantic versioning. We attach a human-interpretable tag that points to a specific commit SHA.\n",
        "\n",
        "Creating a tag is easiest when done inside a transaction, just like the files we already uploaded. To do this, simply call `tx.tag` on the transaction and supply the repository name, the commit SHA to tag, and the intended tag name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b36971",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:33:07.241427Z",
          "iopub.status.busy": "2023-11-14T16:33:07.241343Z",
          "iopub.status.idle": "2023-11-14T16:33:07.262325Z",
          "shell.execute_reply": "2023-11-14T16:33:07.262080Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "with fs.transaction as tx:\n",
        "    # the `tag` result is simply the tag name, in this case 'train-test-split-2010'.\n",
        "    tag = tx.tag(repository=REPO_NAME, ref=fixed_commit_id, tag='train-test-split-2010')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "157749da",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "Now we can access the specific files with the semantic tag. Both the `fixed_commit_id` and `tag` reference the same version `ref` in lakeFS, whereas a branch name always points to the latest version on that respective branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0e7d2a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:33:07.263686Z",
          "iopub.status.busy": "2023-11-14T16:33:07.263602Z",
          "iopub.status.idle": "2023-11-14T16:33:07.484873Z",
          "shell.execute_reply": "2023-11-14T16:33:07.484567Z"
        }
      },
      "outputs": [],
      "source": [
        "train_from_branch_head = pd.read_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv', index_col=0)\n",
        "train_from_commit_sha = pd.read_csv(f'lakefs://{REPO_NAME}/{fixed_commit_id}/train_weather.csv', index_col=0)\n",
        "train_from_semantic_tag = pd.read_csv(f'lakefs://{REPO_NAME}/{tag}/train_weather.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1ef6a1",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "We can verify this by looking at the lengths of the `DataFrame`s. We see that the `train_from_commit_sha` and `train_from_semantic_tag` are equal. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85da3511",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-14T16:33:07.486360Z",
          "iopub.status.busy": "2023-11-14T16:33:07.486282Z",
          "iopub.status.idle": "2023-11-14T16:33:07.488150Z",
          "shell.execute_reply": "2023-11-14T16:33:07.487899Z"
        }
      },
      "outputs": [],
      "source": [
        "print(len(train_from_branch_head))\n",
        "print(len(train_from_commit_sha))\n",
        "print(len(train_from_semantic_tag))"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent"
      }
    },
    "kernelspec": {
      "display_name": ".demovenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
