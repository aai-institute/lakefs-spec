{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93f1762-0aef-4527-808b-0d9344e09bb4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will complete a small end-to-end data science tutorial that employs lakeFS-spec for data versioning. We will use versioned weather data to train a decision tree classifier to predict whether it is raining tomorrow given the current weather.\n",
    "\n",
    "We will do the following:\n",
    "\n",
    "* Environment setup\n",
    "* LakeFS setup\n",
    "* Authenticating with the lakeFS server\n",
    "* Data ingestion via transactions\n",
    "* Model training\n",
    "* Updating data and retraining a model\n",
    "* Accessing data versions and reproducing experiments\n",
    "* Using tags for semantic versioning\n",
    "\n",
    "To execute the code in this tutorial as a Jupyter notebook, download the `.ipynb` file from the lakeFS-spec repository. \n",
    "\n",
    "This tutorial assumes that you have installed lakeFS-spec in a virtual environment, and that you have followed the [quickstart guide](../quickstart.md) to set up a local lakeFS instance.\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "Install the necessary libraries for this notebook on the environment you have just created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0b372",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5fbb9",
   "metadata": {},
   "source": [
    "Also install an appropriate lakeFS-spec version, which can be either the latest release from PyPI via `pip install --upgrade lakefs-spec`, or the development version from GitHub via `pip install git+https://github.com/aai-institute/lakefs-spec.git`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35daf1-e3a4-4b5b-91c9-5f365b8e13fe",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## lakeFS Setup\n",
    "\n",
    "With Docker Desktop or a similar runtime running set up lakeFS by executing the following `docker run` command (from the [lakeFS quickstart](https://docs.lakefs.io/quickstart/launch.html)) in your console:\n",
    "\n",
    "`docker run --name lakefs --pull always --rm --publish 8000:8000 treeverse/lakefs:latest run --quickstart`\n",
    "\n",
    "You find the authentication credentials in the terminal output. The default address for the local lakeFS GUI is http://localhost:8000/. \n",
    "\n",
    "## Authenticating with the lakeFS server\n",
    "\n",
    "There are multiple ways to authenticate with lakeFS from Python code. In this tutorial, we choose the YAML file configuration. By executing the cell below, you will download a YAML file containing the default lakeFS quickstart credentials and server URL to your user directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/aai-institute/lakefs-spec/main/docs/tutorials/.lakectl.yaml\", os.path.expanduser(\"~/.lakectl.yaml\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4127bf1",
   "metadata": {},
   "source": [
    "We can now instantiate the `LakeFSFileSystem` with the credentials we just downloaded. Alternatively, we could have passed the credentials directly in the code. It is important that the credentials are available at the time of filesystem instantiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_spec import LakeFSFileSystem\n",
    "\n",
    "fs = LakeFSFileSystem()\n",
    "\n",
    "REPO_NAME = 'weather'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f8cd8",
   "metadata": {},
   "source": [
    "We will create a repository using a helper function provided by lakeFS-spec. If you have already created one in the UI, make sure to set the `REPO_NAME` variable accordingly in the cell directly above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_spec.client_helpers import create_repository\n",
    "\n",
    "repo = create_repository(client=fs.client, name=REPO_NAME, storage_namespace=f\"local://{REPO_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d3502",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Now it's time to get some data. We will use the [Open Meteo API](https://open-meteo.com/), where we can pull weather data from an API for free (as long as we are non-commercial) and without an API-token.\n",
    "\n",
    "For training our toy model, we download the full weather data of Munich for the year 2010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bb8bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "outfile, _ = urllib.request.urlretrieve(\"https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2010-01-01&end_date=2010-12-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e651374",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The data is in JSON format. Therefore, we need to wrangle the data a bit to make it usable. But first, we will upload it to our lakeFS instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd285eb",
   "metadata": {},
   "source": [
    "## Upload a file using transactions\n",
    "\n",
    "lakeFS works similar to `git` as a versioning system. You can create *commits* that contain specific changes to the data. You can also work with *branches* to create your own isolated view of the data independently of your colleagues. Every commit (on any branch) is identified by a commit SHA. This SHA can be used to programmatically interact with specific states of your data and enables logging of the specific data versions used to create a certain model.\n",
    "\n",
    "To easily carry out versioning operations while uploading files, you can use **transactions**. A transaction is a context manager that keeps track of all files that were uploaded in its scope, as well as all versioning operations happening between file uploads. All operations are deferred to the end of the transaction, and are executed sequentially on completion.\n",
    "\n",
    "To create a commit after a file upload, you can run the following transaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b579e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_BRANCH_NAME = 'transform-raw-data'\n",
    "\n",
    "with fs.transaction as tx:\n",
    "    fs.put(outfile, f'{REPO_NAME}/{NEW_BRANCH_NAME}/weather-2010.json')\n",
    "    tx.commit(repository=REPO_NAME, branch=NEW_BRANCH_NAME, message=\"Add 2010 weather data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a84d00",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "You can inspect this commit by selecting the `transform-raw-data` branch, and navigating to the **Commits** tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7debf4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Data Transformation\n",
    "\n",
    "Now let's transform the data for our use case. We put the transformation into a function to be able to reuse it later.\n",
    "\n",
    "In this notebook, we use a simple toy model to predict whether it is raining at the same time tomorrow given weather data from right now.\n",
    "\n",
    "We will skip a lot of possible feature engineering and other data science aspects in order to focus more on the application of the `LakeFSFileSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def transform_json_weather_data(filepath):\n",
    "    if hasattr(filepath, \"close\") and hasattr(filepath, \"tell\"):\n",
    "        data = json.load(filepath)\n",
    "    else:\n",
    "        with open(filepath,\"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data[\"hourly\"])\n",
    "    df.time = pd.to_datetime(df.time)\n",
    "    df['is_raining'] = df.rain > 0\n",
    "    df['is_raining_in_1_day'] = df.is_raining.shift(24).astype(bool)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "    \n",
    "df = transform_json_weather_data(outfile)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd927d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Next, we save this data as a CSV file into the main branch. When the transaction commit helper is called, the newly put CSV file is committed. You can verify the saving worked in the lakeFS UI in your browser by switching to the commits tab of the `main` branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.transaction as tx:\n",
    "    df.to_csv(f'lakefs://{REPO_NAME}/main/weather_2010.csv')\n",
    "    tx.commit(repository=REPO_NAME, branch=\"main\", message=\"Update weather data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c69526",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Training the initial weather model\n",
    "\n",
    "First we will do a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "model_data = df.drop('time', axis=1)\n",
    "\n",
    "train, test = sklearn.model_selection.train_test_split(model_data, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f494332",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We save these train and test datasets into a new `training` branch. If the branch does not exist yet, as in this case, it is implicitly created by default. You can control this behaviour with the `create_branch_ok` flag when initializing the `LakeFSFileSystem`. By default, `create_branch_ok` is set to `True`, so we need to only set `fs = LakeFSFileSystem()` to enable implicit branch creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204048af",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_BRANCH = 'training'\n",
    "\n",
    "with fs.transaction as tx:\n",
    "    train.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "    test.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n",
    "    tx.commit(repository=REPO_NAME, branch=TRAINING_BRANCH, message=\"Add train-test split of 2010 weather data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9c4dc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's check the shape of train and test data. Later on, we will get back to this data version and reproduce the results of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39562daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Initial train data shape: {train.shape}')\n",
    "print(f'Initial test data shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97ae35",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We now proceed to train a decision tree classifier and evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dependent_variable = 'is_raining_in_1_day'\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=7)\n",
    "\n",
    "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
    "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e67b34",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Updating data and retraining the model\n",
    "\n",
    "Until now, we only have used data from 2010. Let's download additional 2020 data, transform it, and save it to LakeFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa0216",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "outfile, _ = urllib.request.urlretrieve(\"https://archive-api.open-meteo.com/v1/archive?latitude=52.52&longitude=13.41&start_date=2020-01-01&end_date=2020-12-31&hourly=temperature_2m,relativehumidity_2m,rain,pressure_msl,surface_pressure,cloudcover,cloudcover_low,cloudcover_mid,cloudcover_high,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m\")\n",
    "\n",
    "new_data = transform_json_weather_data(outfile)\n",
    "\n",
    "with fs.transaction as tx:\n",
    "    new_data.to_csv(f'lakefs://{REPO_NAME}/main/weather_2020.csv')\n",
    "    tx.commit(repository=REPO_NAME, branch=\"main\", message=\"Add 2020 weather data\")\n",
    "\n",
    "# Remove leftover temporary files from previous `urlretrieve` calls\n",
    "urllib.request.urlcleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68e8c2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Let's concatenate the old data and the new data, create a new train-test split, and push the updated files to lakeFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.drop('time', axis=1)\n",
    "full_data = pd.concat([new_data, train, test])\n",
    "\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(full_data, random_state=7)\n",
    "\n",
    "print(f'Updated train data shape: {train_df.shape}')\n",
    "print(f'Updated test data shape: {test_df.shape}')\n",
    "\n",
    "with fs.transaction as tx:\n",
    "    train_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/train_weather.csv')\n",
    "    test_df.to_csv(f'lakefs://{REPO_NAME}/{TRAINING_BRANCH}/test_weather.csv')\n",
    "    tx.commit(repository=REPO_NAME, branch=TRAINING_BRANCH, message=\"Add train-test split of 2010 and 2020 data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b862924e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now, we train the model on the new data and validate on the new test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_df.drop(dependent_variable, axis=1), train_df[dependent_variable].astype(bool)\n",
    "x_test, y_test = test_df.drop(dependent_variable, axis=1), test_df[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c70b39",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Accessing data versions through commits and reproducing experiments\n",
    "\n",
    "If we need to go to our initial data and reproduce the first experiment (the model trained on the 2010 data with its initial accuracy), we can go back in the commit history of the `training` branch and select the appropriate commit data snapshot. Since we have created multiple commits on the same branch already, we will address different data versions by their commit SHAs. \n",
    "\n",
    "To obtain the actual commit SHA from a branch, we have multiple options. Manually, we could go into the lakeFS UI, select the training branch, and navigate to the **Commits** tab. There, we take the parent of the previous commit, titled `Add train-test split of 2010 weather data`, and copy its revision SHA (also called `ID`).\n",
    "\n",
    "In code, we can use a versioning helper called `rev_parse` to obtain commit SHAs for different revisions on the `training` branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_spec.client_helpers import rev_parse\n",
    "\n",
    "# parent is the parent number of a commit relative to HEAD (the latest commit, for which parent = 0).\n",
    "previous_commit = rev_parse(fs.client, REPO_NAME, TRAINING_BRANCH, parent=1)\n",
    "fixed_commit_id = previous_commit.id\n",
    "print(fixed_commit_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf24fc",
   "metadata": {},
   "source": [
    "Let's check whether we managed to get the initial train and test data with this commit SHA, checking equality to the initial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train = pd.read_csv(f\"lakefs://{REPO_NAME}/{fixed_commit_id}/train_weather.csv\", index_col=0)\n",
    "orig_test = pd.read_csv(f\"lakefs://{REPO_NAME}/{fixed_commit_id}/test_weather.csv\", index_col=0)\n",
    "\n",
    "print(f'Is the pulled training data equal to the local training data? {train.equals(orig_train)}')\n",
    "print(f'Is the pulled test data equal to the local test data? {test.equals(orig_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09528632",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's train and validate the model again based on the redownloaded data and see if we manage to reproduce the initial accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train.drop(dependent_variable, axis=1), train[dependent_variable].astype(bool)\n",
    "x_test, y_test = test.drop(dependent_variable, axis=1), test[dependent_variable].astype(bool)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "test_acc = model.score(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be45c4d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Using tags instead of commit SHAs for semantic versioning\n",
    "\n",
    "The above method for data versioning works great when you have experiment tracking tools to store and retrieve the commit SHA in automated pipelines. But it can be tedious to retrieve in manual prototyping. We can make selected versions of the dataset more accessible with semantic versioning by attaching a human-interpretable tag to a specific commit SHA.\n",
    "\n",
    "Creating a tag is easiest when done inside a transaction, just like the files we already uploaded. To do this, simply call `tx.tag` on the transaction and supply the repository name, the commit SHA to tag, and the intended tag name. Tags are immutable once created, so attempting to tag two different commits with the same name will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b36971",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with fs.transaction as tx:\n",
    "    # the `tag` result is simply the tag name, in this case 'train-test-split-2010'.\n",
    "    tag = tx.tag(repository=REPO_NAME, ref=fixed_commit_id, tag='train-test-split-2010')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157749da",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we can access the specific files with the semantic tag. Both the `fixed_commit_id` and `tag` reference the same version `ref` in lakeFS, whereas a branch name always points to the latest version on that respective branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_commit = pd.read_csv(f'lakefs://{REPO_NAME}/{fixed_commit_id}/train_weather.csv', index_col=0)\n",
    "train_from_tag = pd.read_csv(f'lakefs://{REPO_NAME}/{tag}/train_weather.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ef6a1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "We can verify this by comparing the `DataFrame`s. We see that the `train_from_commit` and `train_from_tag` are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is the data tagged {tag!r} equal to the data in commit {fixed_commit_id[:8]}? {train_from_commit.equals(train_from_tag)}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
